<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="神经网络、深度学习">
    
    <meta name="author" content="宣胤">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://xuanyin02.github.io/2023/061444686.html"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="记录了对神经网络与深度学习的基础知识的学习，不仅是对知识的复习，也是知识的分享">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络与深度学习基础">
<meta property="og:url" content="http://xuanyin02.github.io/2023/061444686.html">
<meta property="og:site_name" content="宣胤星球">
<meta property="og:description" content="记录了对神经网络与深度学习的基础知识的学习，不仅是对知识的复习，也是知识的分享">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614110838619.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614131156046.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614132108765.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614120024102.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614120127480.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614201025232.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614202430527.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614202500173.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203106650.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203119416.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203129561.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203204431.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614204713506.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614205832741.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614205944157.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210036585.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210057728.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210345947.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614212133554.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614214409036.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614214454965.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616195315180.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616195401429.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210054860.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210208650.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210612271.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210657743.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616211205201.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616211213065.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212642583.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212659347.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212735211.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212816520.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212922251.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213351479.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213514535.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213652658.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213730036.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213813803.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215330990.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215430254.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215507162.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618115758487.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618115842373.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132418413.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618130140090.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618130358201.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132025481.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132003364.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618133428149.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618133525290.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618134254873.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155119379.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155610329.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155902249.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160039086.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160922908.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160609079.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161615395.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161734473.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161839234.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162536909.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162714881.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162843338.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162910807.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163218221.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163226605.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163234326.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163814108.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163917974.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163928435.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163937133.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164416777.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164812981.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164821840.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164836618.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164846632.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165841310.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165930224.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165814453.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170017607.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170209900.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170227381.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170242557.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170058279.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170728825.png">
<meta property="article:published_time" content="2023-06-14T02:49:42.000Z">
<meta property="article:modified_time" content="2023-06-18T09:33:44.360Z">
<meta property="article:author" content="宣胤">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614110838619.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/Hopstarter.png" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/Hopstarter.png">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/Hopstarter.png">
    <!--- Page Info-->
    
    <title>
        
            神经网络与深度学习基础 -
        
        宣胤星球
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/assets/fonts.css">

    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"xuanyin02.github.io","root":"/","language":"zh-CN","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":true,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[""]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":6,"number":true,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":true},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/wallhaven-wqery6-light.webp","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"欢迎你的到来","subtitle":{"text":["Loading..."],"hitokoto":{"enable":true,"api":"https://v1.hitokoto.cn/?c=f"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":"Noto Sans SC","url":"https://fonts.googleapis.com/css2?family=Noto+Sans+SC:wght@400;500;700&display=swap"},"social_links":{"enable":false,"links":{"github":null,"instagram":null,"zhihu":null,"twitter":null,"email":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":true,"type":"fixed","audios":[{"name":"Something Just Like This","artist":"Coldplay","url":"https://evan.beee.top/music/Something%20Just%20Like%20This%20-%20The%20Chainsmokers%E3%80%81Coldplay.mp3","cover":"https://evan.beee.top/music/covers/Something_Just_Like_This.png"},{"name":"BABYDOLL (Speed)","artist":"Ari Abdul","url":"/musics/BABYDOLL (Speed).mp3","cover":"http://p2.music.126.net/jhjtPJ9HfOgX-G7MYl-yaQ==/109951167210282652.jpg?param=130y130"},{"name":"One Day","artist":"Matisyahu","url":"/musics/One Day.mp3","cover":"http://p1.music.126.net/S6knilCPQYxOqRFNJ9DQXA==/109951165610749659.jpg?param=130y130"},{"name":"Empty Love","artist":"Lulleaux / Kid Princess","url":"/musics/Empty Love.mp3","cover":"http://p1.music.126.net/xrWSChs7pIOWFjOz5eQIzw==/109951164855840145.jpg?param=130y130"},{"name":"年少有为","artist":"李荣浩","url":"/musics/年少有为.mp3","cover":"http://p1.music.126.net/tt8xwK-ASC2iqXNUXYKoDQ==/109951163606377163.jpg?param=130y130"},{"name":"不将就","artist":"李荣浩","url":"/musics/不将就.mp3","cover":"http://p2.music.126.net/e-Uc6W3Kug-HFHJ5nvCUPg==/109951166562828988.jpg?param=130y130"},{"name":"起风了","artist":"买辣椒也用券","url":"/musics/起风了.mp3","cover":"http://p2.music.126.net/diGAyEmpymX8G7JcnElncQ==/109951163699673355.jpg?param=130y130"},{"name":"像我这样的人","artist":"毛不易","url":"/musics/像我这样的人.mp3","cover":"http://p2.music.126.net/vmCcDvD1H04e9gm97xsCqg==/109951163350929740.jpg?param=130y130"},{"name":"凄美地","artist":"郭顶","url":"/musics/凄美地.mp3","cover":"http://p2.music.126.net/wSMfGvFzOAYRU_yVIfquAA==/2946691248081599.jpg?param=130y130"},{"name":"飞云之下","artist":"林俊杰/韩红","url":"/musics/飞云之下.mp3","cover":"http://p1.music.126.net/YsQrEZ_M6kduwN2zh6Q6kg==/109951163311406661.jpg?param=130y130"}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.1.4","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"},"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"About":{"icon":"fa-regular fa-user","submenus":{"Me":"/about","Github":"https://github.com/xuanyin02","Blog":"https://xuanyin02.github.io/"}}},"search":{"enable":true,"preload":true},"tags":{"tags":{"icon":"fa-solid fa-tags","path":"/tags/"}},"categories":{"categories":{"icon":"fa-solid fa-folder","path":"/categories/"}}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":{"Archives":{"path":"/archives","icon":"fa-regular fa-archive"},"Tags":{"path":"/tags","icon":"fa-regular fa-tags"},"Categories":{"path":"/categories","icon":"fa-regular fa-folder"}}},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s 秒前","minute":"%s 分钟前","hour":"%s 小时前","day":"%s 天前","week":"%s 周前","month":"%s 个月前","year":"%s 年前"};
    Global.data_config = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/fontawesome/regular.min.css">

    
    
    
    
<meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/rss2.xml" title="宣胤星球" type="application/rss+xml">
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
                <a class="logo-image" href="/">
                    <img src="/images/Hopstarter.png">
                </a>
            
            <a class="logo-title" href="/">
                
                宣胤星球
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        首页
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/archives"  >
                                    
                                        
                                            <i class="fa-regular fa-archive"></i>
                                        
                                        归档
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="has-dropdown" 
                                    href="#" onClick="return false;">
                                    
                                        
                                            <i class="fa-regular fa-user"></i>
                                        
                                        关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                                    
                                </a>
                                <!-- Submenu -->
                                
                                    <ul class="sub-menu">
                                    
                                        <li>
                                        <a href="/about">ME
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a target="_blank" rel="noopener" href="https://github.com/xuanyin02">GITHUB
                                        </a>
                                        </li>
                                    
                                        <li>
                                        <a href="https://xuanyin02.github.io/">BLOG
                                        </a>
                                        </li>
                                    
                                    </ul>
                                
                            </li>
                    
                    
                        <li class="navbar-item search search-popup-trigger">
                            <i class="fa-solid fa-magnifying-glass"></i>
                        </li>
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                    <div class="icon-item search search-popup-trigger"><i class="fa-solid fa-magnifying-glass"></i></div>
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                首页
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/archives"  >
                             
                                
                                    <i class="fa-regular fa-archive"></i>
                                
                                归档
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="has-dropdown" 
                        href="#" onClick="return false;">
                            
                                
                                    <i class="fa-regular fa-user"></i>
                                
                                关于&nbsp;<i class="fa-solid fa-chevron-down"></i>
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                              
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="/about">ME</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" target="_blank" rel="noopener" href="https://github.com/xuanyin02">GITHUB</a>
                            </li>
                        
                            <li class="dropdown-item flex-center">
                                <a class="dropdown-item" href="https://xuanyin02.github.io/">BLOG</a>
                            </li>
                        
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">神经网络与深度学习基础</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/avatar.jpg">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">宣胤</span>
                            
                                <span class="author-label"></span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2023-06-14 10:49:42</span>
        <span class="mobile">2023-06-14 10:49</span>
        <span class="hover-info">创建</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2023-06-18 17:33:44</span>
            <span class="mobile">2023-06-18 17:33</span>
            <span class="hover-info">更新</span>
        </span>
    

    
        <span class="article-categories article-meta-item">
            <i class="fa-regular fa-folders"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
        <span class="article-wordcount article-meta-item">
            <i class="fa-regular fa-typewriter"></i>&nbsp;<span>5.3k 字</span>
        </span>
    
    
        <span class="article-min2read article-meta-item">
            <i class="fa-regular fa-clock"></i>&nbsp;<span>18 分钟</span>
        </span>
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h2 id="神经网络与深度学习导论"><a href="#神经网络与深度学习导论" class="headerlink" title="神经网络与深度学习导论"></a>神经网络与深度学习导论</h2><h3 id="人工神经网络发展史"><a href="#人工神经网络发展史" class="headerlink" title="人工神经网络发展史"></a>人工神经网络发展史</h3><h4 id="人工智能、机器学习、深度学习之间的关系"><a href="#人工智能、机器学习、深度学习之间的关系" class="headerlink" title="人工智能、机器学习、深度学习之间的关系"></a>人工智能、机器学习、深度学习之间的关系<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614110838619.png" alt="image-20230614110838619" style="zoom:50%;"><figcaption>image-20230614110838619</figcaption></figure></h4><p>人工智能：计算机科学的一个分支<br>它是研究、开发用于<strong>模拟、延伸和扩展人的智能的理论、方法、技术及应用系统</strong>的一门新的技术科学</p>
<p>机器学习：一种<strong>实现人工智能</strong>的方法<br>机器学习是一门<strong>多领域交叉学科</strong>，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门<strong>研究计算机怎样模拟或实现人类的学习行为</strong>，以<strong>获取</strong>新的知识或技能，重新组织已有的知识结构使之<strong>不断改善自身</strong>的性能</p>
<p>深度学习：一种<strong>实现机器学习</strong>的技术<br>深度学习是机器学习领域中一个新的研究方向，<strong>学习样本数据的内在规律和表示层次</strong>。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据</p>
<h4 id="人工智能门派"><a href="#人工智能门派" class="headerlink" title="人工智能门派"></a>人工智能门派</h4><p>“<strong>符号主义</strong>”，又称逻辑主义、计算机学派，主张用公理和逻辑体系搭建一套人工智能系统</p>
<p>“<strong>连接主义</strong>”，又称仿生学派，主张模仿人类的神经元，用神经网络的连接机制实现人工智能</p>
<p>在<strong>符号主义者</strong>的眼里，人工智能应该模仿人类的逻辑方式获取知识，而<strong>连接主义者</strong>奉行大数据和训练学习知识</p>
<h4 id="人工神经网络发展的不同时期"><a href="#人工神经网络发展的不同时期" class="headerlink" title="人工神经网络发展的不同时期"></a>人工神经网络发展的不同时期</h4><p>萌芽期：19世纪，众多生理学家和心理学家已开始进行动物实验，尝试通过观察动物的行为模式，分析智能行为实现的原理<br>通过实验得出，动物会在每次尝试中，构建一种刺激-反应联系，当它们成功时，就会记下这些刺激-反应，那些不正确的就会慢慢被排除掉，也就是说<strong>学习的实质是不断的形成刺激-反应的过程</strong></p>
<p>孕育期：<br><strong>1943年</strong>，心理学家<strong>Warren McCulloch</strong>和数理逻辑学家<strong>Walter Pitts</strong>在合作的论文中<strong>提出了人工神经网络的概念及人工神经元的数学模型（M-P模型）</strong>，从而开创了人工神经网络研究的时代</p>
<p><strong>1949年</strong>，心理学家<strong>Donald Olding Hebb</strong>根据神经可塑性的机制在论文中描述了<strong>神经元学习法则</strong>，并提出了一种学习假说，Hebb的假说后来被成为<strong>赫布型学习</strong>，被认为是一种典型的<strong>无监督学习规则</strong></p>
<p><strong>1957年</strong>，<strong>Frank Rosenblatt</strong>教授提出了<strong>感知机模型（Perceptron）</strong>。感知机使用<strong>特征向量</strong>来表示的<strong>前馈式人工神经网络</strong>，它是一种<strong>二元分类器</strong>，在人工神经网络领域中，感知机也被指为<strong>单层的人工神经网络</strong></p>
<p>达特茅斯会议：<strong>1956年</strong>，一个崭新的学科–<strong>人工智能</strong>，从达特茅斯会议开启</p>
<p>寒冬：<br><strong>1969年</strong>，<strong>Marvin Minsky</strong> 和 <strong>Seymour Papert</strong>在《Perceptrons》一书中，仔细分析了以感知机为代表的单层神经网络系统的功能及局限。证明了感知机的两大缺陷：其一，无法处理<strong>异或问题</strong>；其二，当时计算机的<strong>计算能力不足</strong>以处理大型神经网络。<strong>Marvin Minsky</strong>教授甚至做出了“基于感知机的研究注定失败”的结论</p>
<p>由于<strong>Rosenblatt</strong>教授等人没能及时推广感知机学习算法到多层神经网络上，又由于《Perceptrons》在研究领域中的巨大影响，及人们对书中论点的误解，造成了人工神经领域发展的长年停滞及低潮，<strong>之后的十多年内，基于神经网络的研究几乎处于停滞状态</strong></p>
<p>误差反向传播算法：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614131156046.png" alt="image-20230614131156046"><figcaption>image-20230614131156046</figcaption></figure><br><strong>1974年</strong>，BP算法首次提出：<strong>Paul Werbos</strong>在博士论文中提出了用**误差反向传导（BP)**来训练人工神经网络，有效解决了异或回路问题，使得训练多层神经网络成为可能。但是当时正值神经网络低潮期，并未受到应有的重视</p>
<p><strong>1983年</strong>，神经网络的应用：物理学家<strong>John Hopfield</strong>利用<strong>神经网络</strong>，在旅行商这个<strong>NP</strong>问题的求解上获得当时最好成绩，引起了轰动</p>
<p><strong>1986年</strong>，BP算法改进与应用：<strong>Hinton</strong>等人将重新改进的<strong>反向传播算法</strong>引入<strong>多层感知机</strong>，神经网络重新成为热点。反向传播算法是神经网络中极为重要的学习算法，<strong>直到现在仍然占据着重要地位</strong></p>
<p>寒冬中的寒冬：<strong>1995年</strong>，<strong>万普尼克</strong>正式提出了统计学学习理论，并将该方法应用到了<strong>SVM</strong>。支持向量机可以免去神经网络需要调节参数的不足，还避免了神经网络中局部最优问题，一举击败神经网络，称为当时人工智能领域的主流算法，使得<strong>神经网络再受沉重打击</strong></p>
<p>LeNet-5模型：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614132108765.png" alt="image-20230614132108765"><figcaption>image-20230614132108765</figcaption></figure><br><strong>1998年</strong>，计算机科学家<strong>Yann LeCun</strong>等提出的<strong>LeNet-5</strong>采用了<strong>基于梯度的反向传播算法</strong>对网络进行<strong>有监督</strong>的训练，主要进行手写数字识别和英文字母识别。LeNet-5网络虽然很小，但是它<strong>包含了后来被称为“深度学习”的基本模块：卷积层，池化层，全连接层</strong>，是其他深度学习模型的基础</p>
<p>春天：随着<strong>GPU、FPGA</strong>等器件被用于<strong>高性能计算</strong>，使得人们可以通过单纯地增加器件数量来提升神经网络训练学习的速度。<strong>大数据</strong>的出现，使得训练深层网络模型所需要的数据难题得以大幅度缓解。从此，<strong>神经网络成为人工智能领域最热门的研究方向</strong></p>
<p>深度学习：<strong>2006年</strong>，<strong>Hinton</strong>提出了<strong>深度置信网络（DBN）</strong>，一种深层网络模型。使用一种贪心无监督训练方法来解决问题并取得良好结果。DBN的训练方法降低了学习隐藏层参数的难度，并且该算法的训练时间和网络的大小和深度近乎线性关系</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614120024102.png" alt="image-20230614120024102"><figcaption>image-20230614120024102</figcaption></figure></p>
<p>当前常见深度学习框架</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614120127480.png" alt="image-20230614120127480"><figcaption>image-20230614120127480</figcaption></figure></p>
<h3 id="神经网络结构基础"><a href="#神经网络结构基础" class="headerlink" title="神经网络结构基础"></a>神经网络结构基础</h3><h4 id="生物神经元"><a href="#生物神经元" class="headerlink" title="生物神经元"></a>生物神经元</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614201025232.png" alt="image-20230614201025232"><figcaption>image-20230614201025232</figcaption></figure></p>
<h4 id="神经网络如何学习"><a href="#神经网络如何学习" class="headerlink" title="神经网络如何学习"></a>神经网络如何学习</h4><p>赫布法则：当神经元A的一个轴突和神经元B很近，足以对它产生影响，并且持续地、重复地参与了神经元B的兴奋，那么在这两个神经元或其中之一会发生某种生长过程或新城代谢变化，以致于神经元A作为能使神经元B兴奋的细胞之一，它的效能加强了–加拿大心理学家Donald Hebb</p>
<h4 id="生物神经网络"><a href="#生物神经网络" class="headerlink" title="生物神经网络"></a>生物神经网络</h4><p>在机器学习和相关领域，<strong>人工神经网络</strong>的计算模型灵感正是来自<strong>生物神经网络</strong>：每个神经元与其他神经元<strong>相连</strong>，当它<strong>兴奋</strong>时，就会向相邻的神经元<strong>发送</strong>化学物质，从而改变这些神经元内的电位。<br>如果某神经元的电位超过了一个阈值，那么它就会被<strong>激活（兴奋）</strong>，向其他神经元发送化学物质。</p>
<p><strong>人工神经网络</strong>通常呈现为按照一定的层次结构连接起来的“神经元”，它可以从输入的计算值，进行<strong>分布式并行</strong>信息处理的算法数学模型。这种网络依靠系统的<strong>复杂程度</strong>，通过<strong>调整</strong>内部大量节点之间相互<strong>连接的关系</strong>，从而达到处理信息的目的。</p>
<p>常被用于估计或可以依赖于<strong>大量的输入</strong>和一般的<strong>未知近似函数</strong>，来最大化的<strong>拟合</strong>现实中的实际数据，提高<strong>机器学习预测的精度</strong>。</p>
<h4 id="从生物神经元到人工神经元模型（M-P模型）"><a href="#从生物神经元到人工神经元模型（M-P模型）" class="headerlink" title="从生物神经元到人工神经元模型（M-P模型）"></a>从生物神经元到人工神经元模型（M-P模型）</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614202430527.png" alt="image-20230614202430527"><figcaption>image-20230614202430527</figcaption></figure></p>
<p>人工神经元：</p>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614202500173.png" alt="image-20230614202500173" style="zoom: 25%;"><figcaption>image-20230614202500173</figcaption></figure>

<p>激活函数的性质：</p>
<ul>
<li>连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数</li>
<li>激活函数及其导函数要尽可能的简单，有利于提高网络计算效率</li>
<li>激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性</li>
</ul>
<p>常见激活函数及其导数：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203106650.png" alt="image-20230614203106650"><figcaption>image-20230614203106650</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203119416.png" alt="image-20230614203119416"><figcaption>image-20230614203119416</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203129561.png" alt="image-20230614203129561"><figcaption>image-20230614203129561</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614203204431.png" alt="image-20230614203204431"><figcaption>image-20230614203204431</figcaption></figure></p>
<h4 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h4><p>人工神经网络主要由大量的神经元以及它们之间的有向连接构成。因此考虑三方面：</p>
<ol>
<li>神经元的激活规则：主要指神经元输入到输出之间的映射关系，一般为非线性函数</li>
<li>网络的拓扑结构：不同神经元之间的连接关系</li>
<li>学习算法：通过训练数据来学习神经网络的参数</li>
</ol>
<p>人工神经网络由神经元模型构成，这种由许多神经元组成的信息处理网络具有并行分布结构：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614204713506.png" alt="image-20230614204713506"><figcaption>image-20230614204713506</figcaption></figure></p>
<h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>前馈神经网络（全连接神经网络、多层感知器）：</p>
<ul>
<li>各神经元分别属于不同的<strong>层</strong>，层内无连接</li>
<li>相邻两层之间的神经元全部<strong>两两连接</strong></li>
<li>整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示</li>
</ul>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614205832741.png" alt="image-20230614205832741" style="zoom:67%;"><figcaption>image-20230614205832741</figcaption></figure>

<h3 id="前馈网络"><a href="#前馈网络" class="headerlink" title="前馈网络"></a>前馈网络</h3><p>给定一个前馈神经网络，我们用下面的记号来描述这个网络：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614205944157.png" alt="image-20230614205944157"><figcaption>image-20230614205944157</figcaption></figure></p>
<p>前馈神经网络通过下面的公式进行信息传播：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210036585.png" alt="image-20230614210036585" style="zoom:50%;"><figcaption>image-20230614210036585</figcaption></figure></p>
<p>前馈计算：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210057728.png" alt="image-20230614210057728"><figcaption>image-20230614210057728</figcaption></figure></p>
<h3 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h3><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614210345947.png" alt="image-20230614210345947" style="zoom: 50%;"><figcaption>image-20230614210345947</figcaption></figure>

<blockquote>
<p>根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数</p>
</blockquote>
<h3 id="应用到机器学习"><a href="#应用到机器学习" class="headerlink" title="应用到机器学习"></a>应用到机器学习</h3><p>神经网络可以作为一个<strong>“万能”</strong>函数来使用，可以用来进行复杂的特征转换，或逼近一个复杂的条件分布<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614212133554.png" alt="image-20230614212133554"><figcaption>image-20230614212133554</figcaption></figure></p>
<p>如果g(.)为logistic回归，那么logistic回归分类器可以看成神经网络的最后一层</p>
<p>对于N类分类问题：<br>如果使用softmax回归分类器，相当于网络最后一层设置N个神经元，其输出经过softmax函数进行归一化后可以作为每个类的后验概率：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614214409036.png" alt="image-20230614214409036"><figcaption>image-20230614214409036</figcaption></figure><br>采用交叉熵损失函数，对于样本(x,y)，其损失函数为：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230614214454965.png" alt="image-20230614214454965"><figcaption>image-20230614214454965</figcaption></figure></p>
<h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616195315180.png" alt="image-20230616195315180"><figcaption>image-20230616195315180</figcaption></figure></p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616195401429.png" alt="image-20230616195401429"><figcaption>image-20230616195401429</figcaption></figure></p>
<p>如何计算梯度<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210054860.png" alt="image-20230616210054860"><figcaption>image-20230616210054860</figcaption></figure></p>
<h3 id="深度学习的步骤"><a href="#深度学习的步骤" class="headerlink" title="深度学习的步骤"></a>深度学习的步骤</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210208650.png" alt="image-20230616210208650"><figcaption>image-20230616210208650</figcaption></figure></p>
<h3 id="优化问题"><a href="#优化问题" class="headerlink" title="优化问题"></a>优化问题</h3><p>难点：</p>
<ul>
<li><p>参数过多，影响训练</p>
</li>
<li><p>非凸优化问题：即存在局部最优而非全局最优解，影响迭代</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210612271.png" alt="image-20230616210612271"><figcaption>image-20230616210612271</figcaption></figure></p>
</li>
<li><p>下层参数比较难调</p>
</li>
<li><p>参数解释起来比较困难</p>
</li>
</ul>
<p>需求：</p>
<ul>
<li>计算资源要大</li>
<li>数据要多</li>
<li>算法效率要好：即收敛快</li>
</ul>
<p>梯度消失问题<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616210657743.png" alt="image-20230616210657743"><figcaption>image-20230616210657743</figcaption></figure></p>
<h2 id="BP算法"><a href="#BP算法" class="headerlink" title="BP算法"></a>BP算法</h2><h3 id="神经元的训练"><a href="#神经元的训练" class="headerlink" title="神经元的训练"></a>神经元的训练</h3><p>训练的过程就是<strong>不断更新权重w和偏置b</strong>的过程，直到找到稳定的w和b使得模型的<strong>整体误差最小</strong>：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616211205201.png" alt="image-20230616211205201"><figcaption>image-20230616211205201</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616211213065.png" alt="image-20230616211213065"><figcaption>image-20230616211213065</figcaption></figure></p>
<h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p>前向传播</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212642583.png" alt="image-20230616212642583"><figcaption>image-20230616212642583</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212659347.png" alt="image-20230616212659347"><figcaption>image-20230616212659347</figcaption></figure></p>
<p>损失函数</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212735211.png" alt="image-20230616212735211"><figcaption>image-20230616212735211</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212816520.png" alt="image-20230616212816520"><figcaption>image-20230616212816520</figcaption></figure></p>
<h3 id="BP算法实例"><a href="#BP算法实例" class="headerlink" title="BP算法实例"></a>BP算法实例</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616212922251.png" alt="image-20230616212922251"><figcaption>image-20230616212922251</figcaption></figure></p>
<p>设定了输入，和神经元之间的权重</p>
<p>训练步骤：</p>
<p>正向计算-1：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213351479.png" alt="image-20230616213351479"><figcaption>image-20230616213351479</figcaption></figure></p>
<p>进行了隐藏层的相关计算</p>
<p>正向计算-2：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213514535.png" alt="image-20230616213514535"><figcaption>image-20230616213514535</figcaption></figure></p>
<p>进行了输出层的相关计算</p>
<p>损失Cost计算：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213652658.png" alt="image-20230616213652658"><figcaption>image-20230616213652658</figcaption></figure></p>
<p>计算了损失</p>
<p>反向传播-1：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213730036.png" alt="image-20230616213730036"><figcaption>image-20230616213730036</figcaption></figure></p>
<p>反向传播-2：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616213813803.png" alt="image-20230616213813803"><figcaption>image-20230616213813803</figcaption></figure></p>
<p>反向传播-3：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215330990.png" alt="image-20230616215330990"><figcaption>image-20230616215330990</figcaption></figure></p>
<p>对W32、W41、W42同理计算</p>
<p>更新权值：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215430254.png" alt="image-20230616215430254"><figcaption>image-20230616215430254</figcaption></figure></p>
<p>迭代更新：</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230616215507162.png" alt="image-20230616215507162"><figcaption>image-20230616215507162</figcaption></figure></p>
<h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>全连接前馈神经网络缺点：<br>权重矩阵的参数非常多<br>局部不变性特征</p>
<ul>
<li>自然图像中的物体都具有局部不变性特征，比如尺度缩放、平移、旋转等操作不影响其语义信息</li>
<li>而全连接前馈神经网络很难提取这些局部不变特征</li>
</ul>
<h3 id="卷积神经网络的由来"><a href="#卷积神经网络的由来" class="headerlink" title="卷积神经网络的由来"></a>卷积神经网络的由来</h3><h3 id="卷积神经网络概念及特性"><a href="#卷积神经网络概念及特性" class="headerlink" title="卷积神经网络概念及特性"></a>卷积神经网络概念及特性</h3><p>卷积神经网络是一种前馈神经网络：</p>
<ul>
<li>卷积神经网络是受生物学学上“<strong>感受野</strong>”的机制提出而提出的</li>
<li>在视觉神经系统中，一个神经元的“<strong>感受野</strong>”是指视网膜上的<strong>特定区域</strong>，只有这个区域内的刺激才能够激活该神经元</li>
</ul>
<p>卷积神经网络有三个结构上的特性：</p>
<ul>
<li>局部连接</li>
<li>权重共享</li>
<li>空间或时间上的次采样</li>
</ul>
<h3 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h3><p>卷积经常用在信号处理中，用于计算信号的延迟累积。<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618115758487.png" alt="image-20230618115758487"><figcaption>image-20230618115758487</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618115842373.png" alt="image-20230618115842373"><figcaption>image-20230618115842373</figcaption></figure></p>
<p>两维卷积<br>在图像处理中，图像是以二维矩阵的形式输入到神经网络中，因此我们需要二维卷积（下面图像好像有问题）<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132418413.png" alt="image-20230618132418413"><figcaption>image-20230618132418413</figcaption></figure></p>
<h4 id="卷积的运算过程"><a href="#卷积的运算过程" class="headerlink" title="卷积的运算过程"></a>卷积的运算过程</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618130140090.png" alt="image-20230618130140090"><figcaption>image-20230618130140090</figcaption></figure></p>
<h4 id="卷积扩展"><a href="#卷积扩展" class="headerlink" title="卷积扩展"></a>卷积扩展</h4><p>引入滤波器的 <strong>滑动步长s</strong> 和 <strong>零填充p</strong></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618130358201.png" alt="image-20230618130358201"><figcaption>image-20230618130358201</figcaption></figure></p>
<h4 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h4><p>在进行卷积运算时，输入矩阵的边缘会比矩阵内部的元素计算次数少，且输出的矩阵的大小会在卷积运算中相比较于输入的尺寸变小。因此，可在输入的矩阵的四周补零，称为padding，其大小为P。比如当P&#x3D;1时，原输入3*3的矩阵如下，实线框中为原矩阵，周围使用0作为padding：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132025481.png" alt="image-20230618132025481"><figcaption>image-20230618132025481</figcaption></figure></p>
<p>一般来说，如果在高的两侧一共填充Ph行，在宽的两侧一共填充Pw列，那么输出形状将会是：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618132003364.png" alt="image-20230618132003364"><figcaption>image-20230618132003364</figcaption></figure></p>
<h4 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h4><p>在先前的例子里，卷积窗口从输入数组的<strong>最左上方开始，按从左往右、从上往下</strong>的顺序，依次在输入数组上滑动。每次滑动的行数和列数称为<strong>步幅</strong>，先前的例子里，在高和宽两个方向上步幅均为1.当然，根据实际情况也可以使用更大的步幅。如下图所示（实际输入为3*3，高、宽各以0值填充1行&#x2F;列），在高上步幅为3、在宽上步幅为2的卷积运算。可以看到，输出第一列第二个元素时，卷积窗口向下滑动了3行，而在输出第一行第二个元素时卷积窗口向右滑动了2列。当卷积窗口在输入数组上再向右滑动2列时，由于输入元素无法填满窗口，无结果输出：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618133428149.png" alt="image-20230618133428149"><figcaption>image-20230618133428149</figcaption></figure></p>
<p>一般来说，当高上步幅为Sh，宽上步幅为Sw时，输出形状为：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618133525290.png" alt="image-20230618133525290"><figcaption>image-20230618133525290</figcaption></figure></p>
<h4 id="多输入通道"><a href="#多输入通道" class="headerlink" title="多输入通道"></a>多输入通道</h4><p>前面例子中用到的输入和输出都是二维数组，<strong>但真实数据的维度经常更高</strong>。例如，彩色图像在高和宽两个维度外还有RGB（红、绿、蓝）3个颜色通道。假设彩色图像的高和宽分别是h和w（像素），那么它可以表示为一个3hw的多维数组，<strong>将大小为3的这一维称为通道（channel）维</strong>。当输入数据含多个通道时，一般需要构造一个<strong>输入通道数与输入数据的通道数相同的卷积核</strong>，从而能够与含多通道的输入数据做卷积运算。</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618134254873.png" alt="image-20230618134254873"><figcaption>image-20230618134254873</figcaption></figure></p>
<h3 id="卷积类型"><a href="#卷积类型" class="headerlink" title="卷积类型"></a>卷积类型</h3><p>卷积的结果按输出长度不同可以分为三类：</p>
<ul>
<li>窄卷积：步长s&#x3D;1，两端不补零p&#x3D;0，卷积后输出长度为n-m+1</li>
<li>宽卷积：步长s&#x3D;1，两端补零p&#x3D;m-1，卷积后输出长度为n+m-1</li>
<li>等长卷积：步长s&#x3D;1，两端补零p&#x3D;(m-1)&#x2F;2，卷积后输出长度n</li>
</ul>
<blockquote>
<p>在早期的文献中，卷积一般默认为窄卷积，而在目前的文献中，卷积一般默认为等宽卷积</p>
</blockquote>
<h5 id="转置卷积"><a href="#转置卷积" class="headerlink" title="转置卷积"></a>转置卷积</h5><p>通过卷积操作来实现高维特征到低维特征的转换。比如在一维卷积中，一个5维的输入特征，经过一个大小为3的卷积核，其输出为3维特征，如果设置步长大于1，可以进一步降低输出特征的维数。但在一些任务中，需要通过将低维特征映射到高维特征，并且依然希望通过卷积操作来实现，这种卷积操作称为<strong>转置卷积。</strong></p>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155119379.png" alt="image-20230618155119379" style="zoom:50%;"><figcaption>image-20230618155119379</figcaption></figure>

<h5 id="微步卷积"><a href="#微步卷积" class="headerlink" title="微步卷积"></a>微步卷积</h5><p>可以通过增加卷积操作的步长s&gt;1来实现对输入特征的降采样操作，大幅降低特征维数。同样，也可以通过减少转置卷积的步长s&lt;1来实现上采样操作，大幅提高特征维数。<br>步长s&lt;1的转置卷积也称为<strong>微步卷积</strong>。为了实现微步卷积，可以在输入特征之间插入0来间接地使得步长变小。</p>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155610329.png" alt="image-20230618155610329" style="zoom:50%;"><figcaption>image-20230618155610329</figcaption></figure>

<h5 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h5><p>如何增加输出单元的感受野？可以通过以下方法：</p>
<ul>
<li>增加卷积核的大小</li>
<li>增加层数来实现</li>
<li>在卷积之前进行汇聚操作</li>
</ul>
<p><strong>空洞卷积</strong>：通过卷积核插入“空洞”来变相地增加其大小</p>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618155902249.png" alt="image-20230618155902249" style="zoom: 50%;"><figcaption>image-20230618155902249</figcaption></figure>

<h3 id="卷积神经网络的基本组成"><a href="#卷积神经网络的基本组成" class="headerlink" title="卷积神经网络的基本组成"></a>卷积神经网络的基本组成</h3><p>用卷积层代替全连接层</p>
<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160039086.png" alt="image-20230618160039086" style="zoom:50%;"><figcaption>image-20230618160039086</figcaption></figure>

<p>输入：D个特征映射 M x N x D<br>输出：P个特征映射 M’ x N’ x P</p>
<p>特征映射：一幅图像经过卷积后得到的特征<br>卷积核看成一个特征提取器</p>
<p>典型的卷积层可以表示成3维结构</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160922908.png" alt="image-20230618160922908"><figcaption>image-20230618160922908</figcaption></figure></p>
<h4 id="卷积层的映射关系"><a href="#卷积层的映射关系" class="headerlink" title="卷积层的映射关系"></a>卷积层的映射关系</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618160609079.png" alt="image-20230618160609079"><figcaption>image-20230618160609079</figcaption></figure></p>
<h4 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h4><p>池化层（Pooling Layer）也叫子采样层，其作用是进行<strong>特征选择，降低特征数量</strong>，并从而减少参数数量。<br>卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少。如果后面接一个分类器，每个分类器的输入维数依然很高，很容易出现过拟合。为了解决这个问题，可以<strong>在卷积层之后加上一个池化层</strong>，从而降低特征维数，<strong>避免过拟合</strong>。</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161615395.png" alt="image-20230618161615395"><figcaption>image-20230618161615395</figcaption></figure></p>
<p>最大池化层（max pooling）：<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161734473.png" alt="image-20230618161734473" style="zoom:67%;"><figcaption>image-20230618161734473</figcaption></figure></p>
<blockquote>
<p>特点：能很好的保留纹理特征</p>
</blockquote>
<p>平均池化层（mean&#x2F;average pooling）:<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618161839234.png" alt="image-20230618161839234" style="zoom:67%;"><figcaption>image-20230618161839234</figcaption></figure></p>
<blockquote>
<p>特点：能很好的保留背景，但容易使得图片变模糊</p>
</blockquote>
<h4 id="卷积网络结构"><a href="#卷积网络结构" class="headerlink" title="卷积网络结构"></a>卷积网络结构</h4><p>卷积网络是由卷积层、池化层、全连接层交叉堆叠而成<br>趋向于小卷积、大深度<br>趋向于全卷积</p>
<p>典型结构</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162536909.png" alt="image-20230618162536909"><figcaption>image-20230618162536909</figcaption></figure></p>
<p>一个卷积块为连续M个卷积层和b个池化层（M通常设置为2<del>5，b为0或1）.一个卷积网络中可以堆叠N个连续的卷积块，然后在接着K个全连接层（N的取值区间比较大，比如1</del>100或者更大；K一般为0~2）。</p>
<h3 id="卷积神经网络的学习能力"><a href="#卷积神经网络的学习能力" class="headerlink" title="卷积神经网络的学习能力"></a>卷积神经网络的学习能力</h3><h4 id="卷积的作用"><a href="#卷积的作用" class="headerlink" title="卷积的作用"></a>卷积的作用</h4><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162714881.png" alt="image-20230618162714881" style="zoom: 67%;"><figcaption>image-20230618162714881</figcaption></figure>

<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162843338.png" alt="image-20230618162843338" style="zoom: 50%;"><figcaption>image-20230618162843338</figcaption></figure>

<figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618162910807.png" alt="image-20230618162910807" style="zoom:50%;"><figcaption>image-20230618162910807</figcaption></figure>

<h4 id="表示学习"><a href="#表示学习" class="headerlink" title="表示学习"></a>表示学习</h4><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163218221.png" alt="image-20230618163218221" style="zoom: 50%;"><figcaption>image-20230618163218221</figcaption></figure>

<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163226605.png" alt="image-20230618163226605"><figcaption>image-20230618163226605</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163234326.png" alt="image-20230618163234326"><figcaption>image-20230618163234326</figcaption></figure></p>
<h3 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet-5"></a>LeNet-5</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163814108.png" alt="image-20230618163814108"><figcaption>image-20230618163814108</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163917974.png" alt="image-20230618163917974"><figcaption>image-20230618163917974</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163928435.png" alt="image-20230618163928435"><figcaption>image-20230618163928435</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618163937133.png" alt="image-20230618163937133"><figcaption>image-20230618163937133</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164416777.png" alt="image-20230618164416777"><figcaption>image-20230618164416777</figcaption></figure></p>
<blockquote>
<p>注意：C3层是一个卷积层16核卷积，C3并不是与S2全连接而是部分连接。C3层的第0-5个特征映射依赖于S2层的特征映射组的每3个连续子集，第6-11个特征映射依赖于S2层的特征映射组的每4个连续子集，第15个特征映射依赖于S2层的所有特征映射。</p>
</blockquote>
<p>论文中提及了如此组合的原因：</p>
<ul>
<li>减少参数</li>
<li>这种不对称的组合连接的方式有利于提取多种组合特征</li>
</ul>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164812981.png" alt="image-20230618164812981"><figcaption>image-20230618164812981</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164821840.png" alt="image-20230618164821840"><figcaption>image-20230618164821840</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164836618.png" alt="image-20230618164836618"><figcaption>image-20230618164836618</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618164846632.png" alt="image-20230618164846632"><figcaption>image-20230618164846632</figcaption></figure></p>
<p>LeNet-5是一种用于手写体字符识别的非常高效的卷积神经网络。卷积神经网络能够很好的利用图像的结构信息。卷积层的参数较少，这也是由卷积层的<strong>主要特性即局部连接和共享权重</strong>所决定。</p>
<h3 id="典型的卷积网络"><a href="#典型的卷积网络" class="headerlink" title="典型的卷积网络"></a>典型的卷积网络</h3><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165841310.png" alt="image-20230618165841310"><figcaption>image-20230618165841310</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165930224.png" alt="image-20230618165930224"><figcaption>image-20230618165930224</figcaption></figure></p>
<h4 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618165814453.png" alt="image-20230618165814453"><figcaption>image-20230618165814453</figcaption></figure></p>
<h4 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170017607.png" alt="image-20230618170017607"><figcaption>image-20230618170017607</figcaption></figure></p>
<h4 id="Inception网络"><a href="#Inception网络" class="headerlink" title="Inception网络"></a>Inception网络</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170209900.png" alt="image-20230618170209900"><figcaption>image-20230618170209900</figcaption></figure></p>
<p>Inception-V1</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170227381.png" alt="image-20230618170227381"><figcaption>image-20230618170227381</figcaption></figure></p>
<p>Inception-V3</p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170242557.png" alt="image-20230618170242557"><figcaption>image-20230618170242557</figcaption></figure></p>
<h4 id="残差网络"><a href="#残差网络" class="headerlink" title="残差网络"></a>残差网络</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170058279.png" alt="image-20230618170058279"><figcaption>image-20230618170058279</figcaption></figure></p>
<p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230618170728825.png" alt="image-20230618170728825"><figcaption>image-20230618170728825</figcaption></figure></p>
<h3 id="卷积神经网络在NLP中的应用"><a href="#卷积神经网络在NLP中的应用" class="headerlink" title="卷积神经网络在NLP中的应用"></a>卷积神经网络在NLP中的应用</h3><ol>
<li>Ngram特征与卷积</li>
<li>文本序列的卷积</li>
<li>基于卷积的句子表示</li>
<li>AlphaGo</li>
<li>Mask RCNN</li>
<li>OCR</li>
<li>图像生成</li>
<li>Deep Dream</li>
<li>画风迁移</li>
<li>对抗样本</li>
</ol>

            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>标题:</strong> 神经网络与深度学习基础</li>
        <li><strong>作者:</strong> 宣胤</li>
        <li><strong>创建于:</strong> 2023-06-14 10:49:42</li>
        
            <li>
                <strong>更新于:</strong> 2023-06-18 17:33:44
            </li>
        
        <li>
            <strong>链接:</strong> http://xuanyin02.github.io/2023/061444686.html
        </li>
        <li>
            <strong>版权声明:</strong> 本文章采用 <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a> 进行许可。
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2023/060851507.html"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">算法与数据结构</span>
                                    <span class="post-nav-item">下一篇</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;评论
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">此页目录</div>
        <div class="page-title">神经网络与深度学习基础</div>
        <ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA"><span class="nav-number">1.</span> <span class="nav-text">神经网络与深度学习导论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E5%8F%B2"><span class="nav-number">1.1.</span> <span class="nav-text">人工神经网络发展史</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="nav-number">1.1.1.</span> <span class="nav-text">人工智能、机器学习、深度学习之间的关系image-20230614110838619</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%97%A8%E6%B4%BE"><span class="nav-number">1.1.2.</span> <span class="nav-text">人工智能门派</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%91%E5%B1%95%E7%9A%84%E4%B8%8D%E5%90%8C%E6%97%B6%E6%9C%9F"><span class="nav-number">1.1.3.</span> <span class="nav-text">人工神经网络发展的不同时期</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9F%BA%E7%A1%80"><span class="nav-number">1.2.</span> <span class="nav-text">神经网络结构基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E5%85%83"><span class="nav-number">1.2.1.</span> <span class="nav-text">生物神经元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0"><span class="nav-number">1.2.2.</span> <span class="nav-text">神经网络如何学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.3.</span> <span class="nav-text">生物神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%8E%E7%94%9F%E7%89%A9%E7%A5%9E%E7%BB%8F%E5%85%83%E5%88%B0%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B%EF%BC%88M-P%E6%A8%A1%E5%9E%8B%EF%BC%89"><span class="nav-number">1.2.4.</span> <span class="nav-text">从生物神经元到人工神经元模型（M-P模型）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%BA%E5%B7%A5%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">1.2.5.</span> <span class="nav-text">人工神经网络</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">2.</span> <span class="nav-text">前馈神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">2.1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E9%A6%88%E7%BD%91%E7%BB%9C"><span class="nav-number">2.2.</span> <span class="nav-text">前馈网络</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E7%94%A8%E8%BF%91%E4%BC%BC%E5%AE%9A%E7%90%86"><span class="nav-number">2.3.</span> <span class="nav-text">通用近似定理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%BA%94%E7%94%A8%E5%88%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.4.</span> <span class="nav-text">应用到机器学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.5.</span> <span class="nav-text">参数学习</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">2.6.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%AD%A5%E9%AA%A4"><span class="nav-number">2.7.</span> <span class="nav-text">深度学习的步骤</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98"><span class="nav-number">2.8.</span> <span class="nav-text">优化问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#BP%E7%AE%97%E6%B3%95"><span class="nav-number">3.</span> <span class="nav-text">BP算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E8%AE%AD%E7%BB%83"><span class="nav-number">3.1.</span> <span class="nav-text">神经元的训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="nav-number">3.2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BP%E7%AE%97%E6%B3%95%E5%AE%9E%E4%BE%8B"><span class="nav-number">3.3.</span> <span class="nav-text">BP算法实例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-number">4.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%94%B1%E6%9D%A5"><span class="nav-number">4.1.</span> <span class="nav-text">卷积神经网络的由来</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A6%82%E5%BF%B5%E5%8F%8A%E7%89%B9%E6%80%A7"><span class="nav-number">4.2.</span> <span class="nav-text">卷积神经网络概念及特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.3.</span> <span class="nav-text">卷积</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E8%BF%90%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="nav-number">4.3.1.</span> <span class="nav-text">卷积的运算过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E6%89%A9%E5%B1%95"><span class="nav-number">4.3.2.</span> <span class="nav-text">卷积扩展</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A1%AB%E5%85%85"><span class="nav-number">4.3.3.</span> <span class="nav-text">填充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A5%E5%B9%85"><span class="nav-number">4.3.4.</span> <span class="nav-text">步幅</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%A4%9A%E8%BE%93%E5%85%A5%E9%80%9A%E9%81%93"><span class="nav-number">4.3.5.</span> <span class="nav-text">多输入通道</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%B1%BB%E5%9E%8B"><span class="nav-number">4.4.</span> <span class="nav-text">卷积类型</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BD%AC%E7%BD%AE%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.4.0.1.</span> <span class="nav-text">转置卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BE%AE%E6%AD%A5%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.4.0.2.</span> <span class="nav-text">微步卷积</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%A9%BA%E6%B4%9E%E5%8D%B7%E7%A7%AF"><span class="nav-number">4.4.0.3.</span> <span class="nav-text">空洞卷积</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90"><span class="nav-number">4.5.</span> <span class="nav-text">卷积神经网络的基本组成</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E7%9A%84%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB"><span class="nav-number">4.5.1.</span> <span class="nav-text">卷积层的映射关系</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B1%A0%E5%8C%96%E5%B1%82"><span class="nav-number">4.5.2.</span> <span class="nav-text">池化层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-number">4.5.3.</span> <span class="nav-text">卷积网络结构</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0%E8%83%BD%E5%8A%9B"><span class="nav-number">4.6.</span> <span class="nav-text">卷积神经网络的学习能力</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="nav-number">4.6.1.</span> <span class="nav-text">卷积的作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="nav-number">4.6.2.</span> <span class="nav-text">表示学习</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LeNet-5"><span class="nav-number">4.7.</span> <span class="nav-text">LeNet-5</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B8%E5%9E%8B%E7%9A%84%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C"><span class="nav-number">4.8.</span> <span class="nav-text">典型的卷积网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#AlexNet"><span class="nav-number">4.8.1.</span> <span class="nav-text">AlexNet</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#VGG"><span class="nav-number">4.8.2.</span> <span class="nav-text">VGG</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Inception%E7%BD%91%E7%BB%9C"><span class="nav-number">4.8.3.</span> <span class="nav-text">Inception网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="nav-number">4.8.4.</span> <span class="nav-text">残差网络</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9C%A8NLP%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="nav-number">4.9.</span> <span class="nav-text">卷积神经网络在NLP中的应用</span></a></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2023&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">宣胤</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        访问人数&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        总访问量&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">由 <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a> 驱动</span>
                <br>
            <span class="theme-version-container">主题&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.4</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/4/13 00:00:00
            </div>
            <div>
                博客已运行 <span class="odometer" id="runtime_days" ></span> 天 <span class="odometer" id="runtime_hours"></span> 小时 <span class="odometer" id="runtime_minutes"></span> 分钟 <span class="odometer" id="runtime_seconds"></span> 秒
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    
        <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
          <span class="search-input-field-pre">
            <i class="fa-solid fa-keyboard"></i>
          </span>
            <div class="search-input-container">
                <input autocomplete="off"
                       autocorrect="off"
                       autocapitalize="off"
                       placeholder="搜索..."
                       spellcheck="false"
                       type="search"
                       class="search-input"
                >
            </div>
            <span class="popup-btn-close">
                <i class="fa-solid fa-times"></i>
            </span>
        </div>
        <div id="search-result">
            <div id="no-result">
                <i class="fa-solid fa-spinner fa-spin-pulse fa-5x fa-fw"></i>
            </div>
        </div>
    </div>
</div>

    


</main>




<script src="/js/utils.js"></script>

<script src="/js/main.js"></script>

<script src="/js/layouts/navbarShrink.js"></script>

<script src="/js/tools/scrollTopBottom.js"></script>

<script src="/js/tools/lightDarkSwitch.js"></script>



    
<script src="/js/tools/localSearch.js"></script>




    
<script src="/js/tools/codeBlock.js"></script>




    
<script src="/js/layouts/lazyload.js"></script>




    
<script src="/js/tools/runtime.js"></script>

    
<script src="/js/layouts/odometer.min.js"></script>

    
<link rel="stylesheet" href="/assets/odometer-theme-minimal.css">




  
<script src="/js/libs/Typed.min.js"></script>

  
<script src="/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
        
<script src="/js/tools/tocToggle.js"></script>

<script src="/js/libs/anime.min.js"></script>

<script src="/js/layouts/toc.js"></script>

<script src="/js/plugins/tabs.js"></script>

    
</div>


    
<script src="/js/libs/pjax.min.js"></script>

<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




    <div id="aplayer"></div>

<script src="/js/libs/APlayer.min.js"></script>


<script src="/js/plugins/aplayer.js"></script>


</body>
</html>
