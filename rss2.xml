<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>宣胤星球</title>
    <link>http://xuanyin02.github.io/</link>
    
    <atom:link href="http://xuanyin02.github.io/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>人生总是充满遗憾，学会在遗憾中成长</description>
    <pubDate>Fri, 19 May 2023 16:23:02 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>关于大数据处理引擎Spark知识的学习</title>
      <link>http://xuanyin02.github.io/2023/051959676.html</link>
      <guid>http://xuanyin02.github.io/2023/051959676.html</guid>
      <pubDate>Fri, 19 May 2023 14:23:10 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;第一章：RDD详解&quot;&gt;&lt;a href=&quot;#第一章：RDD详解&quot; class=&quot;headerlink&quot; title=&quot;第一章：RDD详解&quot;&gt;&lt;/a&gt;第一章：RDD详解&lt;/h3&gt;&lt;h3 id=&quot;1-1-为什么需要RDD&quot;&gt;&lt;a href=&quot;#1-1-为什么需要RDD&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="第一章：RDD详解"><a href="#第一章：RDD详解" class="headerlink" title="第一章：RDD详解"></a>第一章：RDD详解</h3><h3 id="1-1-为什么需要RDD"><a href="#1-1-为什么需要RDD" class="headerlink" title="1.1 为什么需要RDD"></a>1.1 为什么需要RDD</h3><p>分布式计算需要：</p><ul><li>​    分区控制</li><li>​    Shuffle控制</li><li>​    数据存储\序列化\发送</li><li>​    数据计算API  等一系列功能</li></ul><p>这些功能，不能简单的通过Python内置的本地集合对象（如List\字典等）去完成。我们在分布式框架中，需要有一个统一的数据抽象对象，来实现上述分布式计算所需功能。<strong>这个抽象对象，就是RDD</strong>。</p><h3 id="1-2-什么是RDD"><a href="#1-2-什么是RDD" class="headerlink" title="1.2 什么是RDD"></a>1.2 什么是RDD</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，代表一个<strong>不可变、可分区、里面的元素可并行计算</strong>的集合。</p><p><strong>Resilient</strong>：Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换<br>在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。<br>Spark把这个Job执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败<br>Task如果失败会自动进行特定次数的重试，默认重试次数为4<br>Stage如果失败会自动进行特定次数的重试，默认重试次数为4<br>数据分片的高度弹性：可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率</p><p><strong>Distributed</strong>：RDD中的数据是分布式存储的，可用于分布式计算。RDD的数据是跨机器存储的（跨进程）</p><p><strong>Dataset</strong>：一个数据集合，用于存放数据的。List、Dict、Array本地集合（数据全部在一个进程内部）</p><p>不可变：不可变集合，变量的声明使用val</p><p>分区的：集合的数据被划分为很多部分，每部分称为分区Partition</p><p>并行计算：集合中的数据可以被并行的计算处理，每个分区数据被一个task处理</p><h3 id="1-3-RDD的五大特性"><a href="#1-3-RDD的五大特性" class="headerlink" title="1.3 RDD的五大特性"></a>1.3 RDD的五大特性</h3><p>前三个特性每个RDD都具备的，后两个特征是可选的：</p><ol><li>RDD是有分区的</li><li>计算方法都会作用到每一个分片（分区）之上</li><li>RDD之间是有相互依赖的关系的</li><li>KV型RDD可以有分区器</li><li>RDD分区数据的读取会尽量靠近数据所在地</li></ol><h3 id="第二章-RDD编程入门"><a href="#第二章-RDD编程入门" class="headerlink" title="第二章 RDD编程入门"></a>第二章 RDD编程入门</h3><h3 id="第三章-RDD的持久化"><a href="#第三章-RDD的持久化" class="headerlink" title="第三章 RDD的持久化"></a>第三章 RDD的持久化</h3><h4 id="3-1-RDD的数据的过程数据"><a href="#3-1-RDD的数据的过程数据" class="headerlink" title="3.1 RDD的数据的过程数据"></a>3.1 RDD的数据的过程数据</h4><p>RDD之间相互迭代计算（Transformation的转换），当执行开启后，新的RDD的生成，代表老RDD的消失</p><p>RDD的数据是过程数据，只在数据的处理过程中存在，一旦处理完成，就不见了。这个特性可以最大化的利用资源，老旧RDD没用了，就从内存中清理，给后续的计算腾出内存空间</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519230343524.png" alt="image-20230519230343524"><figcaption>image-20230519230343524</figcaption></figure></p><p>为了不重新从rdd1开始构建rdd3，所以要使用rdd的持久化技术</p><h4 id="3-2-RDD-Cache"><a href="#3-2-RDD-Cache" class="headerlink" title="3.2 RDD Cache"></a>3.2 RDD Cache</h4><p>对于上述的场景，肯定要执行优化，优化就是：rdd3如果不消失，那么rdd1-&gt;rdd2-&gt;rdd3这个链条就不会执行2次，或者更多次</p><p>RDD的缓存技术：Spark提供了缓存的API，可以让我们通过调用API，将指定的RDD的数据保留在内存或者硬盘上</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519230515665.png" alt="image-20230519230515665"><figcaption>image-20230519230515665</figcaption></figure></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519230657165.png" alt="image-20230519230657165"><figcaption>image-20230519230657165</figcaption></figure></p><h4 id="3-3-RDD-CheckPoint"><a href="#3-3-RDD-CheckPoint" class="headerlink" title="3.3 RDD CheckPoint"></a>3.3 RDD CheckPoint</h4><p>CheckPoint技术，也是将RDD的数据，保存起来但是它仅支持硬盘存储</p><p>并且：①它被设计认为是安全的；②不保留血缘关系。</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519230903213.png" alt="image-20230519230903213"><figcaption>image-20230519230903213</figcaption></figure></p><p>Cache（缓存）与CheckPoint的对比：</p><ul><li>​    CheckPoint不管分区数量多少，风险是一样的，缓存分区越多，风险越高</li><li>​    CheckPoint支持写入HDFS，缓存不行，HDFS是高可靠存储，CheckPoint被认为是安全的</li><li>​    CheckPoint不支持内存，缓存可以，缓存如果写内存，性能比CheckPoint要好一些</li><li>​    CheckPoint因为被设计认为是安全的，所以不保留血缘关系，而缓存因为设计上认为不安全，所以保留</li></ul><p>设置CheckPoint第一件事情，选择RDD的保存路径<br>如果是Local模式，可以支持本地文件系统，如果在集群运行，千万要用HDFS<br>Sc.setCheckpointDir(“hdfs:node1:9000&#x2F;out&#x2F;xxx”)<br>用的时候，直接调用checkpoint算子即可：<br>Rdd.checkpoint()</p><h3 id="第四章-Spark案例练习"><a href="#第四章-Spark案例练习" class="headerlink" title="第四章 Spark案例练习"></a>第四章 Spark案例练习</h3><h3 id="第五章-共享变量"><a href="#第五章-共享变量" class="headerlink" title="第五章 共享变量"></a>第五章 共享变量</h3><h4 id="5-1-广播变量"><a href="#5-1-广播变量" class="headerlink" title="5.1 广播变量"></a>5.1 广播变量</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519232155710.png" alt="image-20230519232155710"><figcaption>image-20230519232155710</figcaption></figure></p><p>!!!Executor是进程，进程内资源共享，一个Executor中有两个分区，那这两个分区就可以共享资源。Stu_info_list由Driver发送给每个分区，但是又可以共享，那么Executor中就有了两份相同的资源，造成内存的浪费和网络传输成本的增加。</p><p>解决方案：广播变量<br>使用方式：<br>#1、将本地list标记成广播变量即可<br>Broadcast &#x3D; sc.broadcast(stu_info_list)<br>#2、使用广播变量，从broadcast对象中取出本地list对象即可<br>Value &#x3D; broadcast.value</p><p>#也就是 先放进去broadcast内部，然后从broadcast内部取出来用，中间传输的是broadcast这个对象了<br>#只要中间传输的是broadcast对象，spark就会留意，只会给每个Ececutor发一份了，而不是傻傻的哪个分区要都给</p><h4 id="5-2-累加器"><a href="#5-2-累加器" class="headerlink" title="5.2 累加器"></a>5.2 累加器</h4><p>需求：想要对map算子计算中的数据，进行技术累加，得到全部数据计算完后的累加结果<br>因为我们初始化的count标志从Driver发送给Ececutor是发送的数值，不像指针地址那种</p><h3 id="第六章-Spark内核调度"><a href="#第六章-Spark内核调度" class="headerlink" title="第六章 Spark内核调度"></a>第六章 Spark内核调度</h3><h4 id="6-1-DAG"><a href="#6-1-DAG" class="headerlink" title="6.1 DAG"></a>6.1 DAG</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519232555194.png" alt="image-20230519232555194"><figcaption>image-20230519232555194</figcaption></figure></p><p>DAG：有向无环图。有方向没有形成闭环的一个执行流程图</p><p>一个Action会将前面一串的RDD依赖关系（Transformation）执行，也就是一个Action会产生一个DAG图<br>前面我们写的搜索日志分析案例，3个需求就是3个Action，就产生了3个DAG<br>一个Action会产生一个Job（一个应用程序内的子任务），每一个Job有各自的DAG图<br></p><p>结论：Job和Action<br>1个Action会产生1个DAG，如果代码中有3个Action就产生3个DAG<br>一个Action会产生一个DAG，会在程序运行中产生一个Job<br>所以：1个Action &#x3D; 1个DAG &#x3D; 1个Job<br>如果一个代码中，写了3个Action，那么这个代码运行起来产生3个Job，每个Job有自己的DAG<br></p><p>一个代码运行起来，在Spark中称之为：Application<br>层级关系：1个Application中，可以有多个Job，每一个Job内含一个DAG，同时每一个Job都是由一个Action产生的</p><p>DAG是Spark代码的逻辑执行图，这个DAG的最终作用是：为了构建物理上的Spark详细执行计划而生，所以，由于Spark是分布式（多分区）的，那么DAG和分区之间也是有关联的</p><h4 id="6-2-DAG的宽窄依赖和阶段划分"><a href="#6-2-DAG的宽窄依赖和阶段划分" class="headerlink" title="6.2 DAG的宽窄依赖和阶段划分"></a>6.2 DAG的宽窄依赖和阶段划分</h4><p>宽窄依赖：</p><p>在Spark RDD前后之间的关系，分为：窄依赖、 宽依赖<br>窄依赖：父RDD的一个分区，全部将数据发给子RDD的一个分区<br>宽依赖：父RDD的一个分区，将数据发给子RDD的多个分区<br>宽依赖还有一个别名：shuffle</p><p>阶段划分：</p><p>对于Spark来说，会根据DAG，按照宽窄依赖，划分不同的DAG阶段<br>划分依据：从后往前，遇到宽依赖，就分出一个阶段，称之为stage<br>在stage的内部，一定都是：窄依赖</p><h4 id="6-3-内存迭代运算"><a href="#6-3-内存迭代运算" class="headerlink" title="6.3 内存迭代运算"></a>6.3 内存迭代运算</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234250136.png" alt="image-20230519234250136"><figcaption>image-20230519234250136</figcaption></figure></p><p>![image-20230519233246978](.&#x2F;%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8ESpark%E7%9F%A5%E8%AF%86%E7%9A%84%E5%AD%A6%E4%B9%A0&#x2F;image-20230519233246978.png</p><p>​    <figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234238064.png" alt="image-20230519234238064"><figcaption>image-20230519234238064</figcaption></figure></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234346576.png" alt="image-20230519234346576"><figcaption>image-20230519234346576</figcaption></figure></p><h4 id="6-4-Spark并行度"><a href="#6-4-Spark并行度" class="headerlink" title="6.4 Spark并行度"></a>6.4 Spark并行度</h4><p>Spark的并行：在同一时间内，有多少个task在同时运行<br>并行度：并行能力的设置<br>比如设置并行度为6，其实就是要6个task并行在跑<br>在有了6个task并行的前提下，rdd的分区就被规划成6个分区了</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234535771.png" alt="image-20230519234535771"><figcaption>image-20230519234535771</figcaption></figure></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234559444.png" alt="image-20230519234559444"><figcaption>image-20230519234559444</figcaption></figure></p><h4 id="6-5-Spark任务调度"><a href="#6-5-Spark任务调度" class="headerlink" title="6.5 Spark任务调度"></a>6.5 Spark任务调度</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230519234628606.png" alt="image-20230519234628606"><figcaption>image-20230519234628606</figcaption></figure></p><p>DAG调度器：工作内容：将逻辑的DAG图进行处理，最终得到逻辑上的Task划分</p><p>Task调度器：工作内容：基于DAG调度器的产出，来规划这些逻辑的task，应该在哪些物理的executor上运行，以及监控管理它们的运行</p><p>Spark架构体系</p><p>StandAlone模式是spark⾃带的集群运⾏模式，不依赖其他的资源调度框架，部署起来简单。<br>StandAlone模式⼜分为client模式和cluster模式，本质区别是Driver运⾏在哪⾥，如果Driver运⾏在SparkSubmit进程中就是Client模式，如果Driver运⾏在集群中就是Cluster模式。</p><p>standalone client模式</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="/./%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8ESpark%E7%9F%A5%E8%AF%86%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20230519224936208.png" alt="image-20230519224936208"><figcaption>image-20230519224936208</figcaption></figure></p><p>standalone cluster模式</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="/./%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8ESpark%E7%9F%A5%E8%AF%86%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20230519225011812.png" alt="image-20230519225011812"><figcaption>image-20230519225011812</figcaption></figure></p><p>Spark On YARN cluster模式</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="/./%E5%85%B3%E4%BA%8E%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E5%BC%95%E6%93%8ESpark%E7%9F%A5%E8%AF%86%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20230519225828449.png" alt="image-20230519225828449"><figcaption>image-20230519225828449</figcaption></figure></p><ol><li>client向ResourceManager申请资源，返回一个application ID</li><li>client上传spark jars下面的jar包、自己写的jar包和配置</li><li>ResourceManager随机找一个资源充足的NodeManager</li><li>然后通过RPC让NodeManager从HDFS上下载jar包和配置，启动ApplicationMaster</li><li>ApplicationMaster向ResourceManager申请资源</li><li>ResourceManager中的调度器找到符合条件的NodeManager，将NodeManager的信息返回给ApplicationMaster</li><li>ApplicationMaster跟返回的NodeManager进行通信</li><li>NodeManager从HDFS下载依赖</li><li>NodeManager启动Executor</li><li>Executor启动之后反向向ApplicationMaster [Driver]注册</li></ol>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/spark/">spark</category>
      
      
      <comments>http://xuanyin02.github.io/2023/051959676.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Java面试常见问题</title>
      <link>http://xuanyin02.github.io/2023/05188946.html</link>
      <guid>http://xuanyin02.github.io/2023/05188946.html</guid>
      <pubDate>Thu, 18 May 2023 13:41:38 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;Java基础&quot;&gt;&lt;a href=&quot;#Java基础&quot; class=&quot;headerlink&quot; title=&quot;Java基础&quot;&gt;&lt;/a&gt;Java基础&lt;/h3&gt;&lt;h4 id=&quot;构造函数、成员变量、代码块初始化顺序&quot;&gt;&lt;a href=&quot;#构造函数、成员变量、代码块初始化顺序&quot;</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="Java基础"><a href="#Java基础" class="headerlink" title="Java基础"></a>Java基础</h3><h4 id="构造函数、成员变量、代码块初始化顺序"><a href="#构造函数、成员变量、代码块初始化顺序" class="headerlink" title="构造函数、成员变量、代码块初始化顺序"></a>构造函数、成员变量、代码块初始化顺序</h4><p>顺序：静态成员变量 -&gt; 静态代码块 -&gt; 非静态成员变量 -&gt; 非静态代码块 -&gt; 构造函数<br>详细顺序：父类静态变量、父类静态代码块、子类静态变量、子类静态代码块、父类非静态变量、父类非静态代码块、父类构造函数、子类非静态变量、子类非静态代码块、子类构造函数</p><h4 id="java三大特性"><a href="#java三大特性" class="headerlink" title="java三大特性"></a>java三大特性</h4><ul><li><p><strong>封装</strong>：利用抽象数据类型（类）将数据（属性）和基于数据的操作（基于属性的方法）绑定在一起，使其构成一个不可分隔的独立实体。</p></li><li><p><strong>继承</strong>：子类继承自父类，从而获得父类非private的属性和方法<br>继承应该遵循里氏替换原则<br>那么什么是里氏替换原则呢？：子类对象的行为应该与父类对象期望的行为一致（子类对象的行为造成的结果不能出现父类对象方法中未出现的结果）<br>父类引用指向子类对象 称为向上转型<br>父类 父类引用 &#x3D; new 子类()  –》 Animal animal &#x3D; new Cat()</p></li><li><p><strong>多态</strong>：分为编译时多态和运行时多态<br>编译时多态主要指方法的重载<br>运行时多态指程序中定义的对象引用所指向的具体类型在运行期间才确定<br>运行时多态有三个条件：继承、重写、向上转型<br>多态：多种不同形态<br>如：ArrayList是List的一种形态；LinkedList也是List的一种形态</p></li></ul><h4 id="简述内部类及其作用"><a href="#简述内部类及其作用" class="headerlink" title="简述内部类及其作用"></a>简述内部类及其作用</h4><ul><li>成员内部类：作为成员对象的内部类。可以访问 private 及以上外部类的属性和方法。外部类想要访问内部类属性或方法时，必须要创建一个内部类对象，然后通过该对象访问内部类的属性或方法。外部类也可访问 private 修饰的内部类属性</li><li>局部内部类：存在于方法中的内部类。访问权限类似局部变量，只能访问外部类的 final 变量</li><li>匿名内部类：只能使用一次，没有类名，只能访问外部类的 final 变量</li><li>静态内部类：类似类的静态成员变量</li></ul><h4 id="Java-语言中关键字-static-的作用是什么"><a href="#Java-语言中关键字-static-的作用是什么" class="headerlink" title="Java 语言中关键字 static 的作用是什么"></a>Java 语言中关键字 static 的作用是什么</h4><ul><li>为某种特定数据类型或对象 分配 与创建对象个数无关的单一的存储空间</li><li>使得某个方法或属性 与类而不是对象关联在一起，即在不创建对象的情况下可通过类直接调用方法或使用类的属性</li></ul><p>具体而言 static 又可分为 4 种使用方式：</p><ol><li>修饰成员变量。用 static 关键字修饰的静态变量在内存中只有一个副本。只要静态变量所在的类被加载，这个静态变量就会被分配空间，可以使用“类.静态变量”和“对象.静态变量”的方法使用</li><li>修饰成员方法。static 修饰的方法无需创建对象就可以被调用。static 方法中不能使用 this 和 super 关键字，不能调用非 static 方法，只能访问所属类的静态成员变量和静态成员方法</li><li>修饰代码块。JVM 在加载类的时候会执行 static 代码块。static 代码块常用于初始化静态变量。static 代码块只会被执行一次</li><li>修饰内部类。static 内部类可以不依赖外部类实例对象而被实例化。静态内部类不能与外部类有相同的名字，不能访问普通成员变量，只能访问外部类中的静态成员和静态成员方法</li></ol><h4 id="为什么String不可变"><a href="#为什么String不可变" class="headerlink" title="为什么String不可变"></a>为什么String不可变</h4><p>String内部使用char数组value[ ]存储数据，该数组被声明为final，这意味着value数组初始化之后，不能再引用其他数组，并且String内部没有改变value数组的方法，因此String不可变。</p><p>不可变的好处：</p><ul><li>String作为HashMap的key。不可变的特性可以使得hash值也不可变，因此只需要进行一次hash运算。</li><li>安全性，String作为参数，String不可变可以保证参数不变</li><li>线程安全</li></ul><h4 id="x3D-x3D-和-equals的区别"><a href="#x3D-x3D-和-equals的区别" class="headerlink" title="&#x3D;&#x3D; 和 equals的区别"></a>&#x3D;&#x3D; 和 equals的区别</h4><p>&#x3D;&#x3D;比较基本类型，比较的是值，&#x3D;&#x3D;比较引用类型，比较的是内存地址</p><p>equlas是Object类的方法，本质上与&#x3D;&#x3D;一样，但是有些类重写了equals方法，比如String的equals被重写后，比较的是内存地址，另外重写了equlas后，也必须重写hashcode()方法</p><h4 id="final、finally-和-finalize-的区别是什么"><a href="#final、finally-和-finalize-的区别是什么" class="headerlink" title="final、finally 和 finalize 的区别是什么"></a>final、finally 和 finalize 的区别是什么</h4><ul><li>final 用于声明属性、方法和类，分别表示属性不可变、方法不可覆盖、类不可继承</li><li>finally 作为异常处理的一部分，只能在 try&#x2F;catch 语句中使用，finally 附带一个语句块用来表示这个语句最终一定被执行，经常被用在需要释放资源的情况下</li><li>finalize 是 Object 类的一个方法，在垃圾收集器执行的时候会调用被回收对象的 finalize()方法。当垃圾回收器准备好释放对象占用空间时，首先会调用 finalize()方法，并在下一次垃圾回收动作发生时真正回收对象占用的内存</li></ul><h4 id="什么是反射机制"><a href="#什么是反射机制" class="headerlink" title="什么是反射机制"></a>什么是反射机制</h4><p>Java反射机制是在运行状态中，对于任意一个类，都能知道这个类的属性和方法；对于任意一个对象，都能调用它的任意一个方法和属性；这种动态获取信息以及动态调用对象的方法的功能称为Java的反射机制。</p><p>如何通过反射获得x类的class对象</p><ol><li>x.class</li><li>x.getClass()</li><li>Class.foeName(“类名”)</li></ol><p>序列化是什么</p><p>序列化是一种将对象转换成字节序列的过程，用于解决在对对象流进行读写操作时所引发的问题。序列化可以将对象的状态写在流里进行网络传输，或者保存到文件、<a class="link" href="https://cloud.tencent.com/solution/database?from=20065&from_column=20065">数据库 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>等系统里，并在需要的时候把该流读取出来重新构造成一个相同的对象</p><h4 id="concurrenthashmap的解析"><a href="#concurrenthashmap的解析" class="headerlink" title="concurrenthashmap的解析"></a>concurrenthashmap的解析</h4><h4 id="ArrayList-和-LinkedList-的区别"><a href="#ArrayList-和-LinkedList-的区别" class="headerlink" title="ArrayList 和 LinkedList 的区别"></a>ArrayList 和 LinkedList 的区别</h4><p>1、底层数据结构<br>ArrayList底层采用数组实现，因此支持随机访问，可以通过下标索引访问元素，时间复杂度为O(1)。但是在插入和删除操作时，需要移动后面的元素，时间复杂度为O(n)。<br>LinkedList底层采用双向链表实现，因此插入和删除操作只需要改变相邻节点的指针，时间复杂度为O(1)，但是随机访问需要遍历链表，时间复杂度为O(n)。</p><p>2、空间占用<br>ArrayList的内部是一个数组，当元素个数不足数组容量时，会浪费一部分内存空间。而LinkedList每个元素需要额外的空间来存储前后节点的指针，因此会占用更多的内存空间。</p><p>3、迭代器性能<br>在迭代操作时，ArrayList使用普通迭代器或增强for循环的性能比LinkedList更优。这是因为ArrayList的数据存储在连续的内存中，迭代时可以直接访问内存，而LinkedList需要通过遍历链表来访问每个元素。</p><p>4、使用场景<br>ArrayList适用于随机访问比较多，插入和删除操作较少的场景，例如缓存、排序、搜索等。而LinkedList适用于插入和删除操作较多，随机访问较少的场景，例如队列、栈等。</p><h3 id="Spring基础"><a href="#Spring基础" class="headerlink" title="Spring基础"></a>Spring基础</h3><ul><li><p>IoC（Inversion of Control）控制反转<br>使用对象时，由主动new产生对象转换为由外部提供对象，此过程中对象的创建控制权由程序转移到外部，这种思想称为控制反转<br>Spring技术对IOC思想进行了实现<br>Spring提供了一个容器，称为IoC容器（系统架构中的Core Container），用来充当IoC思想的“外部“<br>IoC容器负责对象的创建、初始化等一系列工作，被创建或被管理的对象在IoC容器中被称为Bean</p></li><li><p>DI (Dependency Injection) 依赖注入<br>在容器中建立Bena与Bean之间的依赖关系的整个过程,称为依赖注入</p></li><li><p>AOP（Aspect Oriented Programming）面向切面编程<br>AOP 的实现方式是通过动态代理或字节码操作，在代码运行期间动态地将切面织入到目标对象的方法执行过程中，从而实现对目标对象方法的增强。</p></li></ul><h3 id="数据库基础"><a href="#数据库基础" class="headerlink" title="数据库基础"></a>数据库基础</h3><h4 id="数据库事务四大特性"><a href="#数据库事务四大特性" class="headerlink" title="数据库事务四大特性"></a>数据库事务四大特性</h4><ol><li>原子性：事务中的所有操作要么全部完成，要么全部不完成，不会只完成其中的一部分操作。如果一个操作失败，整个事务将被回滚到事务开始前的状态，所有的操作都将被撤销。</li><li>一致性：事务执行前后，数据库的状态必须保持一致。如果事务执行后，数据库的状态不符合预期，事务将被回滚到事务开始前的状态，以保证数据库的一致性。</li><li>隔离性：事务执行的过程中，对其他事务是隔离的。即每个事务都认为自己是唯一在操作数据库的事务，不受其他事务的干扰。事务隔离级别包括读未提交、读已提交、可重复读和串行化，不同的隔离级别会影响并发性能和数据的一致性。</li><li>持久性：事务完成后，对数据库的修改必须永久保存在数据库中，即使出现了系统故障或断点等情况，也不能丢失，为了实现持久性，数据库通常使用日志来记录所有的事务操作，以便在系统故障恢复后恢复数据。</li></ol><h4 id="数据库隔离级别"><a href="#数据库隔离级别" class="headerlink" title="数据库隔离级别"></a>数据库隔离级别</h4><p>首先解释一下幻读和不可重复读</p><p><strong>幻读</strong>是指在同一个事务内，第一次查询某个范围内的数据时，没有查询到某些行，但是在该事务内后续再次查询同一范围内的数据时，却发现有新的数据行被查到，就像是出现了幻觉一样，因此称之为“幻读”。</p><p><strong>不可重复读</strong>是指在同一个事务内，多次读取同一个数据的结果不一样</p><ul><li>读未提交：最低级别，事务可以读取其他事务未提交的数据，也就是脏读。虽然可以提高并发性能，但是会导致数据的不一致，一般不建议使用。</li><li>读已提交：事务只能读取其他事务已提交的数据，避免了脏读，但是可能出现不可重复度和幻读的问题。</li><li>可重复读：在同一个事务内，多次读取同一个数据的结果都是一样的，不会出现不可重复读的问题。但是可能会出现幻读的问题，即在同一个事务内，一个范围内的数据记录被其他事务修改或删除。</li><li>串行化：最高级别，事务之间相互完全隔离，每个事务都像是在独立的系统中执行，避免了所有并发问题，但是对系统性能影响比较大。</li></ul><h3 id="TCP三次握手"><a href="#TCP三次握手" class="headerlink" title="TCP三次握手"></a>TCP三次握手</h3><p>TCP 是一种可靠的、面向连接的协议，它需要在客户端和服务器之间建立连接，保证数据能够准确、及时地传输。而 TCP 建立连接的方式就是通过三次握手（Three-way Handshake）来完成的。</p><ol><li>客户端向服务器发送 SYN 请求（SYN &#x3D; Synchronize Sequence Numbers），表示客户端想要建<br>立连接。这时，客户端会随机生成一个起始序列号（Sequence Number）x，并将 SYN 标志位置<br>为 1，同时等待服务器的响应。</li><li>服务器收到客户端的 SYN 请求后，会回复一个 SYN+ACK（ACK &#x3D; Acknowledgment）的响应，表<br>示服务器已经接受了客户端的请求，并且想要建立连接。这时，服务器会随机生成一个起始序<br>列号 y，并将 SYN 和 ACK 标志位置为 1，确认号（Acknowledgment Number）设置为 x+1，同时<br>等待客户端的响应。</li><li>客户端收到服务器的 SYN+ACK 响应后，会发送一个 ACK 确认响应，表示客户端已经接受了服务<br>器的请求，并且可以开始发送数据了。这时，客户端会将确认号设置为 y+1，表示客户端已经<br>收到了服务器的响应，连接建立成功。</li></ol><h3 id="常用的设计模式"><a href="#常用的设计模式" class="headerlink" title="常用的设计模式"></a>常用的设计模式</h3><ol><li>工厂模式：定义一个用于创建对象的接口，让子类决定实例化哪一个类。工厂模式通过对象的创建与使用分离，从而使代码更加灵活、可扩展，常用于创建对象复杂、种类繁多的场景。</li><li>单例模式：确保一个类只有一个实例，并提供全局访问点。单例模式通过限制一个类只能被实例化一次，从而避免了资源的浪费和竞争条件的产生，常用于需要全局唯一实例的场景。</li><li>观察者模式：定义对象之间一种一对多的依赖关系，当一个对象状态发生改变时，所有依赖它的对象都得到通知并自动更新。观察者模式通过松散耦合的方式，将观察者与被观察者分离，从而使对象之间的关系更加灵活、可扩展。</li><li>装饰器模式：动态地将责任附加到对象上，扩展对象的功能。装饰器模式通过递归组合的方式，使对象可以被无限层次地装饰，从而实现动态地添加、修改、删除对象的功能。</li><li>模板方法模式：定义一个操作中的算法骨架，而将一些步骤延迟到子类中。模板方法模式通过定义一个模板方法，将算法的骨架和具体的实现分离，从而使算法可以在不同的场景下得到重复利用。</li></ol>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E9%9D%A2%E8%AF%95/">面试</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/java/">java</category>
      
      
      <comments>http://xuanyin02.github.io/2023/05188946.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>数据仓库建模理论</title>
      <link>http://xuanyin02.github.io/2023/051846990.html</link>
      <guid>http://xuanyin02.github.io/2023/051846990.html</guid>
      <pubDate>Thu, 18 May 2023 01:56:02 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;第1章-数据仓库概述&quot;&gt;&lt;a href=&quot;#第1章-数据仓库概述&quot; class=&quot;headerlink&quot; title=&quot;第1章   数据仓库概述&quot;&gt;&lt;/a&gt;第1章   数据仓库概述&lt;/h3&gt;&lt;h4 id=&quot;1-1-数据仓库概念（Data-Warehouse）&quot;&gt;&lt;</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="第1章-数据仓库概述"><a href="#第1章-数据仓库概述" class="headerlink" title="第1章   数据仓库概述"></a>第1章   数据仓库概述</h3><h4 id="1-1-数据仓库概念（Data-Warehouse）"><a href="#1-1-数据仓库概念（Data-Warehouse）" class="headerlink" title="1.1 数据仓库概念（Data Warehouse）"></a>1.1 数据仓库概念（Data Warehouse）</h4><p>数据仓库是一个为数据分析而设计的企业级数据管理系统。<br>是面向主题的、集成的、稳定的和时变的数据集合。<br>输入数据：业务数据，用户行为数据、爬虫数据。<br>数据仓库，将各个异构的数据源数据库的数据统一管理起来，并且完成了质量较差的数据的剔除、格式转换等预处理操作，最终按照一种合理的建模方式来完成源数据组织形式的转变，为企业<strong>制定决策，提供数据支持</strong>。可以帮助企业<strong>改进业务流程、提高产品质量</strong>等。</p><h4 id="1-2-数据仓库核心架构"><a href="#1-2-数据仓库核心架构" class="headerlink" title="1.2 数据仓库核心架构"></a>1.2 数据仓库核心架构</h4><ul><li>原始数据层–ODS（ODSàOperation Data Store）：存放未经过处理的原始数据，结构上与源系统保持一致，是数据仓库的数据准备区。</li><li>明细数据层–DWD（DWDàData Warehouse Detail）：基于维度建模理论进行构建，存放维度模型中的事实表，保存各业务过程中最小粒度的操作记录。</li><li>公共维度层–DIM（DIMàDimension）：基于维度建模理论进行构建，存放维度模型中的维度表，保存一致性维度信息。</li><li>汇总数据层–DWS（DWSàData Warehouse Summary）：基于上层的指标需求，以分析的主题对象作为建模驱动，构建公共统计粒度的汇总表。</li><li>数据应用层–ADS（ADSàApplication Data Service）：存放各项统计指标结果。</li></ul><h4 id="1-3-数据仓库与数据库的区别"><a href="#1-3-数据仓库与数据库的区别" class="headerlink" title="1.3 数据仓库与数据库的区别"></a>1.3 数据仓库与数据库的区别</h4><ol><li>数据库是面向事务的设计，数据仓库是面向主题设计的</li><li>数据库一般存储业务数据，数据仓库存储的一般是历史数据</li><li>数据库设计是避免冗余，符合业务应用，但是不符合分析。数据仓库在设计是有意引入冗余，依照分析需求，分析维度、分析指标进行设计</li><li>数据库是为捕获数据而设计的，数据仓库是为分析数据而设计</li></ol><h3 id="第2章-数据仓库建模概述"><a href="#第2章-数据仓库建模概述" class="headerlink" title="第2章   数据仓库建模概述"></a>第2章   数据仓库建模概述</h3><h4 id="2-1-数据仓库建模意义"><a href="#2-1-数据仓库建模意义" class="headerlink" title="2.1 数据仓库建模意义"></a>2.1 数据仓库建模意义</h4><p>数据模型就是数据组织和存储的方法，它强调从业务、数据存取和使用角度合理存储数据。只有将数据有序的组织和存储起来之后，数据才能得到高性能、低成本、高效率、高质量的使用。</p><h4 id="2-2-数据仓库模型–ER模型"><a href="#2-2-数据仓库模型–ER模型" class="headerlink" title="2.2 数据仓库模型–ER模型"></a>2.2 数据仓库模型–ER模型</h4><p><strong>ER模型</strong>：用实体关系模型来描述企业业务，并用规范化（减少数据冗余，增强数据的一致性）的方式表示出来，在范式理论上符合3NF</p><ul><li>实体关系模型：实体关系模型将复杂的数据抽象为两个概念——实体和关系。实体表示一个对象，例如学生、班级，关系是指两个实体之间的关系，例如学生和班级之间的从属关系。</li><li>三范式：第一范式1NF：属性不可切割；第二范式2NF：不能存在“部分函数依赖”；第三范式3NF：不能存在传递函数依赖。</li></ul><blockquote><p>这种模型方法的出发点是整合数据，其目的是将整个企业的数据进行组合和合并，并进行规范处理，减少数据冗余性，保证数据的一致性（会产生很多表）。这种模型并不适合直接用于分析统计</p></blockquote><h4 id="2-3-数据仓库模型–维度模型"><a href="#2-3-数据仓库模型–维度模型" class="headerlink" title="2.3 数据仓库模型–维度模型"></a>2.3 数据仓库模型–维度模型</h4><p><strong>维度模型</strong>：将复杂的业务通过<strong>事实</strong>和<strong>维度</strong>两个概念进行呈现，事实通常对应业务过程，而维度通常对应业务过程发生时所处的环境。<br><strong>注</strong>：业务过程可以概括为一个个不可拆分的行为事件，例如电商交易中的下单，取消订单，付款，退单等，都是业务过程。</p><p>下图为一个典型的维度模型，其中<strong>位于中心的SalesOrder为事实表</strong>，其中保存的是下单这个业务过程的所有记录。位于周围每张表都是维度表，包括Date（日期），Customer（顾客），Product（产品），Location（地区）等，这些维度表就组成了每个订单发生时所处的环境，即何人、何时、在何地下单了何种产品。从图中可以看出，模型相对清晰、简洁。</p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230518104312602.png" alt="image-20230518104312602" style="zoom:50%;"><figcaption>image-20230518104312602</figcaption></figure><blockquote><p>维度建模以数据分析作为出发点，为数据分析服务，因此它关注的重点是用户如何更快的完成需求分析以及如何实现较好的大规模复杂查询的响应性能</p></blockquote><h3 id="第3章-维度建模理论之事实表"><a href="#第3章-维度建模理论之事实表" class="headerlink" title="第3章 维度建模理论之事实表"></a>第3章 维度建模理论之事实表</h3><p>事实表（“细长”：列比较少、行比较多，且行的增速快）作为数据仓库维度建模的核心，紧紧围绕着业务过程来设计。其包含与该业务过程的维度引用（维度表外键）以及该业务过程的度量（通常是可累加的数字类型字段）</p><h4 id="3-1-事务型事实表"><a href="#3-1-事务型事实表" class="headerlink" title="3.1 事务型事实表"></a>3.1 事务型事实表</h4><p><strong>概述</strong>：事务型事实表用来记录各业务过程，它保存的是各业务过程的原子操作事件，即最细粒度（粒度是指事实表中一行数据所表达的业务细节程度）的操作事件，可用于分析与各业务过程相关的各项统计指标</p><p><strong>设计流程</strong>：选择业务过程–&gt;声明粒度–&gt;确定维度–&gt;确定事实</p><ul><li><p>选择业务过程：挑选需要的业务，一个业务过程对应一张事务型事实表</p><p>可以确定有哪些事务型事实表</p></li><li><p>声明粒度：精确定义每张事务型事实表的每行数据表示什么</p><p>可以确定每张事务型事实表的每行数据是什么</p></li><li><p>确定维度：确定与每张事务型事实表相关的维度有哪些</p><p>可以确定每张事务型事实表的维度外键</p></li><li><p>确定事实：“事实”，指的是每个业务过程的度量值（通常是可累加的数字类型的值，例如：次数、个数、件数、金额等）</p><p>可以确定每张事务型事实表的度量值字段</p></li></ul><h4 id="3-2-周期型事实表"><a href="#3-2-周期型事实表" class="headerlink" title="3.2 周期型事实表"></a>3.2 周期型事实表</h4><p><strong>概述</strong>：周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，主要用于分析一些存量型（例如商品库存，账户余额）或者状态型（空气温度，行驶速度）指标。</p><h4 id="3-3-累计型事实表"><a href="#3-3-累计型事实表" class="headerlink" title="3.3 累计型事实表"></a>3.3 累计型事实表</h4><p><strong>概述</strong>：累计快照事实表是基于一个业务流程中的<strong>多个关键业务过程</strong>联合处理而构建的事实表，如交易流程中的下单、支付、发货、确认收货业务过程。</p>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/">数据仓库</category>
      
      
      <comments>http://xuanyin02.github.io/2023/051846990.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>Hadoop面试常见问题</title>
      <link>http://xuanyin02.github.io/2023/05169473.html</link>
      <guid>http://xuanyin02.github.io/2023/05169473.html</guid>
      <pubDate>Tue, 16 May 2023 08:26:48 GMT</pubDate>
      
        
        
      <description>&lt;h4 id=&quot;一-什么是Hadoop？&quot;&gt;&lt;a href=&quot;#一-什么是Hadoop？&quot; class=&quot;headerlink&quot; title=&quot;一 什么是Hadoop？&quot;&gt;&lt;/a&gt;一 什么是Hadoop？&lt;/h4&gt;&lt;p&gt;Hadoop是一个开源的分布式计算平台，可以处理大规模数据集</description>
        
      
      
      
      <content:encoded><![CDATA[<h4 id="一-什么是Hadoop？"><a href="#一-什么是Hadoop？" class="headerlink" title="一 什么是Hadoop？"></a>一 什么是Hadoop？</h4><p>Hadoop是一个开源的分布式计算平台，可以处理大规模数据集。它由两个核心组件组成：HDFS文件系统 和 MapReduce计算框架。</p><h4 id="二-Hadoop的优点是什么？"><a href="#二-Hadoop的优点是什么？" class="headerlink" title="二 Hadoop的优点是什么？"></a>二 Hadoop的优点是什么？</h4><p>  可以处理大规模数据集，支持PB级别的数据存储和处理</p><p>  可以在廉价的硬件上运行，降低了成本</p><p>  可以通过数据冗余和自动故障转移来提高可靠性</p><p>  可以通过水平扩展来提高性能</p><p>  可以通过MapReduce计算框架来处理复杂的数据分析任务</p><h4 id="三-Hadoop的工作流程是什么？"><a href="#三-Hadoop的工作流程是什么？" class="headerlink" title="三 Hadoop的工作流程是什么？"></a>三 Hadoop的工作流程是什么？</h4><p>  数据被分割成块并存储在HDFS中</p><p>  MapReduce框架将任务分配给集群中的节点</p><p>  每个节点运行Map任务来处理数据块</p><p>  Map任务输出键值对，这些键值对被shuffle后传递给Reduce任务</p><p>  Reduce任务对键值对进行聚合和处理，并将结果写回HDFS</p><h4 id="四-Hadoop的数据复制是如何实现的？–涉及HDFS数据写入流程"><a href="#四-Hadoop的数据复制是如何实现的？–涉及HDFS数据写入流程" class="headerlink" title="四 Hadoop的数据复制是如何实现的？–涉及HDFS数据写入流程"></a>四 Hadoop的数据复制是如何实现的？–涉及HDFS数据写入流程</h4><p>  Hadoop的数据复制是通过HDFS实现的，当数据被写入HDFS时，它被分成块并复制到多个节点上。默认情况下，每个块会被复制到三个节点上，以提高数据的可靠性。如果一个节点失效，HDFS会自动将数据块复制到其他节点上，以保证数据的可用性。</p><h4 id="五-Hadoop是数据压缩是如何实现的？"><a href="#五-Hadoop是数据压缩是如何实现的？" class="headerlink" title="五 Hadoop是数据压缩是如何实现的？"></a>五 Hadoop是数据压缩是如何实现的？</h4><p>  Hadoop是数据压缩是通过MapReduce框架中的压缩器实现的。压缩器可以在Map和Reduce任务之间压缩和解压缩数据。Hadoop支持多种压缩算法，包括Gzip、Bzip2和Snappy等。</p><h4 id="六-Hadoop的数据安全是如何实现的？"><a href="#六-Hadoop的数据安全是如何实现的？" class="headerlink" title="六 Hadoop的数据安全是如何实现的？"></a>六 Hadoop的数据安全是如何实现的？</h4><p>  Hadoop的数据安全是通过HDFS中的访问控制列表（ACL）和MapReduce框架中的用户身份验证实现的。ACL可以控制对HDFS中文件和目录的访问权限。MapReduce框架可以通过用户身份验证来确保只有授权用户才能运行作业。</p><h4 id="七-Hadoop的调优技巧有哪些？"><a href="#七-Hadoop的调优技巧有哪些？" class="headerlink" title="七 Hadoop的调优技巧有哪些？"></a>七 Hadoop的调优技巧有哪些？</h4><p>  增加节点数：通过增加节点数来提高性能</p><p>  调整块大小：通过调整块大小来提高性能</p><p>  使用压缩：通过使用压缩来减少数据传输量和磁盘空间</p><p>  调整内存和CPU：通过调整内存和CPU来提高性能</p><p>  使用本地磁盘：通过使用本地磁盘来提高性能</p><p>  避免数据倾斜：通过避免数据倾斜来提高性能</p><h4 id="八-Hadoop是常见问题有哪些？"><a href="#八-Hadoop是常见问题有哪些？" class="headerlink" title="八 Hadoop是常见问题有哪些？"></a>八 Hadoop是常见问题有哪些？</h4><p>  数据倾斜：当数据分布不均匀时，会导致某些节点的负载过重，从而影响性能</p><p>  网络带宽限制：当集群中的节点之间的网络带宽不足时，会影响性能</p><p>  硬件故障：当节点硬件故障时，会影响数据可靠性和性能</p><p>  数据丢失：当数据丢失时，会影响数据可靠性和性能</p><p>  安全问题：当Hadoop集群存在安全漏洞时，会影响数据安全性和性能</p><h4 id="九-如何解决Hadoop是常见问题？"><a href="#九-如何解决Hadoop是常见问题？" class="headerlink" title="九 如何解决Hadoop是常见问题？"></a>九 如何解决Hadoop是常见问题？</h4><p>  数据倾斜：通过数据预处理、数据分区和数据复制等方法来避免数据倾斜</p><p>  网络带宽限制：通过增加网络带宽、优化数据传输和调整数据块大小等方法来解决网络带宽限制问题</p><p>  硬件故障：通过使用冗余节点、自动故障转移和备份等方法来提高数据可靠性</p><p>  数据丢失：通过使用数据备份、数据恢复和数据冗余等方法来提高数据可靠性</p><p>  安全问题：通过使用访问控制列表、用户身份验证和加密等方法来提高数据安全性</p><h4 id="十-请说下HDFS读写流程"><a href="#十-请说下HDFS读写流程" class="headerlink" title="十 请说下HDFS读写流程"></a>十 请说下HDFS读写流程</h4><p>HDFS写流程：</p><ol><li>Client发送上传请求，通过RPC与NameNode建立通信，NameNode检查用户是否有上传权限，以及上传的文件是否在HDFS对应的目录下不重名，如果这两者有任意一个不满足，则直接报错，如果两者都满足，则返回给客户端一个可以上传的信息；</li><li>Client根据文件的大小进行切分，默认128M一块，切分完成之后给NameNode发送请求第一个block块上传到哪些服务器上；</li><li>NameNode收到请求之后，根据网络拓扑和机架感知以及副本机制进行文件分配，返回可用的DataNode的地址；</li><li>Cilent收到地址之后与服务器地址列表中的一个节点如A进行通信，本质上就是RPC调用，建立pipeline，A收到请求后会继续调用B，B再调用C，将整个pipeline建立完成，逐级返回Client；</li><li>Cilent开始向A上发送第一个block（先从磁盘读取数据然后放到本地缓存），以packet（数据包，64kb）为单位，A收到一个packet就会发送给B，然后B发送给C，A每传完一个packet就会放入一个应答队列等待应答；</li><li>数据被分隔成一个个的packet数据包在pipeline上依次传输，在pipeline反向传输中，逐个发送ack（命令正确应答），最终由pipeline中第一个DataNode节点A将pipeline ack发送给Cilent；</li><li>当一个block传输完成之后，Client再次请求NameNode上传第二个block，NameNode重新选择三台DataNode给Client。</li></ol><p>HDFS读流程：</p><ol><li>Client向NameNode发送RPC请求，请求block的位置；</li><li>NameNode收到请求之后会检查用户权限以及是否有这个文件，如果都符合，则会视情况返回部分或全部的block列表，对于每个block，NameNode都会返回含有该block副本的DataNode地址；这些返回的DataNode地址，会按照集群拓扑结构得出DataNode与客户端的距离，然后进行排序，排序两个规则：网络拓扑结构中距离Client近的排靠前；心跳机制中超时汇报的DataNode状态为STALE，这样的排靠后；</li><li>Client选取排序靠前的DataNode来读取block，如果Client本身就是DataNode，那么将从本地直接获取数据（短路读取特性）；</li><li>底层本质上是建立Socket Stream（FSDataInputStream），重复的调用父类DataInputStream的read方法，知道这个块上的数据读取完毕；</li><li>当读完列表的block后，若文件读取还没有结束，Client会继续向NameNode获取下一批的block列表；</li><li>读取完一个block都会进行checksum验证，如果读取DataNode时出现错误，Client会通知NameNode，然后再从下一个拥有该block副本的DataNode继续读；</li><li>read方法是并行的读取block信息，不是一块一块的读取；NameNode只是返回Client请求包含块的DataNode地址，并不是返回请求块是数据；</li><li>最终读取来所有的block会合并成一个完整的最终文件。</li></ol><h4 id="十一-什么是HDFS的安全模式？"><a href="#十一-什么是HDFS的安全模式？" class="headerlink" title="十一 什么是HDFS的安全模式？"></a>十一 什么是HDFS的安全模式？</h4><p>HDFS的安全模式，即HDFS safe mode，是HDFS文件系统的一种特殊状态，在该状态下，HDFS文件系统只接收数据请求，而不接收删除、修改等变更操作，也不能复制底层的block及其副本</p><h4 id="十二-进入安全模式的两种方式"><a href="#十二-进入安全模式的两种方式" class="headerlink" title="十二 进入安全模式的两种方式"></a>十二 进入安全模式的两种方式</h4><p><strong>被动进入</strong>：使用命令—&gt; hdfs dfsadmin -safemode enter</p><p><strong>主动进入</strong>：为了保证整个文件系统的数据一致性&#x2F;整个文件系统不丢失数据</p><ul><li>​    根本原因：</li></ul><ol><li><p>与namenode保持定期心跳的datanode的个数没有达到指定阈值（阈值通过dfs.namenode.safemode.min.datanodes指定）</p></li><li><p>没有足够的block拥有指定的副本数（也就是没达到阈值，最小副本数通过参数dfs.namenode.replication.min指定，阈值通过dfs.namenode.safemode.threshold-pct指定，默认是0.999f）</p></li></ol><blockquote><p>正常情况下，HDFS启动过程中，会主动进入安全模式一段时间，这是HDFS的分布式架构决定的。因为namenode启动成功后，需要等待datanode启动成功并通过心跳汇报datanode上存储的block信息，有足够的block拥有指定的副本数之后，并等待特定时间后（通过参数dfs.namenode.safemode.extensions 30000控制），主动退出安全模式。</p></blockquote><ul><li>直接原因：</li></ul><ol><li>部分datanode启动失败或者因为网络原因与namenode心跳连接失败</li><li>部分datanode节点存储hdfs数据的磁盘卷有损坏，导致存储在该磁盘卷中的数据无法提取</li><li>部分datanode节点存储hdfs数据的磁盘分区空间满，导致存储在该磁盘卷中的数据无法正常读取</li></ol><h4 id="十三-修复问题–修复数据"><a href="#十三-修复问题–修复数据" class="headerlink" title="十三 修复问题–修复数据"></a>十三 修复问题–修复数据</h4><p>比如如果有datanode未成功启动，则尝试修复并启动对应的datanode<br>比如如果有datanode存储hdfs数据的磁盘分区空间满，则尝试扩展磁盘分区空间<br>比如如果有datanode存在存储卷故障，则尝试修复存储卷，如果无法修复则需要替换存储卷（会丢失存储卷上的数据）</p><p>需要注意的是，如果出现了某些datanode彻底损坏无法启动，或某些datanode节点磁盘卷故障彻底无法修复的情况，则这些数据对应的block及block上层的hdfs文件，就被丢失了，就需要补数据（从上游重新拉取数据，或重新运行作业生成数据），也可能无法补。</p><h4 id="十四-HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办？"><a href="#十四-HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办？" class="headerlink" title="十四 HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办？"></a>十四 HDFS在读取文件的时候，如果其中一个块突然损坏了怎么办？</h4><p>  Cilent读取完DataNode上的块之后会进行checksum验证，也就是把客户端读取到本地的块与HDFS上的原始块进行校验，如果发现校验结果不一致，客户端会通知NameNode，然后再从下一个拥有该block副本的DataNode继续读。</p><h4 id="十五-HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办？"><a href="#十五-HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办？" class="headerlink" title="十五 HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办？"></a>十五 HDFS在上传文件的时候，如果其中一个DataNode突然挂掉了怎么办？</h4><p>  客户端上传文件时与DataNode建立pipeline管道，管道的正方向是客户端向DataNode发送的数据包，管道反方向是DataNode向客户端发送ack确认，也就是正确接收到数据包之后发送一个已确认接收到的应答。</p><p>  当DataNode突然挂掉了，客户端接收不到这个DataNode发送的ack确认，客户端会通知NameNode，NameNode检查该块的副本与规定的不符，NameNode会通知DataNode去复制副本，并将挂掉的DataNode做下线处理，不再让它参与文件上传与下载。</p><h4 id="十六-NameNode在启动的时候会做哪些操作？"><a href="#十六-NameNode在启动的时候会做哪些操作？" class="headerlink" title="十六 NameNode在启动的时候会做哪些操作？"></a>十六 NameNode在启动的时候会做哪些操作？</h4><p>NameNode数据存储在内存和本地磁盘，本地磁盘数据存储在fsimage镜像文件和edits编辑日志文件。</p><p>首次启动NameNode：</p><ol><li><p>格式化文件系统，为了生成fsimage镜像文件</p></li><li><p>启动NameNode：</p><ol><li>读取fsimage文件，将文件内容加载进内容</li><li>等待DataNode注册与发送block report</li></ol></li><li><p>启动DataNode：</p><ol><li>向NameNode注册</li><li>发送block report</li><li>检查fsimage中记录的块的数量和block report中的块是总数是否相同</li></ol></li><li><p>对文件系统进行操作（创建目录、上传文件、删除文件等）：</p><p>此时内存中已经有文件系统改变的信息，但是磁盘中没有文件系统改变的信息，此时会将这些改变信息写入edits中，edits文件中存储的是文件系统元数据改变的信息</p></li></ol><p>第二次启动NameNode：</p><ol><li>读取fsimage和edits文件；</li><li>将fsimage和edits文件合并成新的fsimage文件；</li><li>创建新的edits文件，内容开始为空；</li><li>启动DataNode</li></ol><h4 id="十七-谈谈对Secondary-NameNode的了解，它的工作机制是怎样的？"><a href="#十七-谈谈对Secondary-NameNode的了解，它的工作机制是怎样的？" class="headerlink" title="十七 谈谈对Secondary NameNode的了解，它的工作机制是怎样的？"></a>十七 谈谈对Secondary NameNode的了解，它的工作机制是怎样的？</h4><p>Secondary NameNode是合并NameNode的edits logs到fsimage文件中。</p><p>它的具体工作机制：</p><ol><li>Secondary NameNode询问NameNode是否需要checkpoint。直接带回结果；</li><li>Secondary NameNode请求执行checkpoint；</li><li>NameNode滚动正在写的edits日志；</li><li>将滚动前的edits日志和fsimage镜像文件拷贝到Secondary NameNode；</li><li>Secondary NameNode加载edits日志和fsimage镜像文件到内存，并合并；</li><li>生成新的镜像文件fsimage.checkpoint；</li><li>拷贝到fsimage.checkpoint到NameNode；</li><li>NameNode将fsimage.checkpoint重新命名成fsimage；</li></ol><p>所以如果NameNode中的元数据丢失，是可以从Secondary NameNode恢复一部分元数据信息的，但不是全部，因为NaneNode正在写的edits日志还没有拷贝到Secondary NameNode，这部分恢复不了。</p><h4 id="十八-Secondary-NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全？"><a href="#十八-Secondary-NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全？" class="headerlink" title="十八 Secondary NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全？"></a>十八 Secondary NameNode不能恢复NameNode的全部数据，那如何保证NameNode数据存储安全？</h4><p>这个问题就要说NameNode的高可用了，即NameNode HA。</p><p>一个NameNode有单点故障问题，那就配置两个NameNode，配置有两个关键点，一是必须要保证这两个NameNode的元数据信息必须要同步的，二是一个NameNode挂掉之后另一个要立马补上。</p><ol><li>元数据信息同步在HA方案中采用的是“共享存储”。每次写文件时，需要将日志同步写入共享存储，这个步骤成功才能认定写文件成功。然后备份节点定期从共享存储同步日志，以便进行主备切换。</li><li>监控NameNode状态采用zookeeper，两个NameNode节点的状态存放在zookeeper中，另外，两个NameNode节点分别有一个进程监控程序–ZKFC，实时读取zookeeper中NameNode的状态，来判断当前的NameNode是不是已经宕机。如果Standby的NameNode节点的ZKFC发现主节点已经挂掉，那么就会强制给原本的Active NameNode节点发送强制关闭请求，之后将Standby的NameNode设置为Active。</li></ol><h4 id="十九-在NameNode-HA中，会出现脑裂问题吗？怎么解决脑裂？"><a href="#十九-在NameNode-HA中，会出现脑裂问题吗？怎么解决脑裂？" class="headerlink" title="十九 在NameNode HA中，会出现脑裂问题吗？怎么解决脑裂？"></a>十九 在NameNode HA中，会出现脑裂问题吗？怎么解决脑裂？</h4><p>会出现脑裂问题：假设 NameNode1 当前为 Active 状态，NameNode2 当前为 Standby 状态。如果某一时刻 NameNode1 发生了“假死”现象（在进行垃圾回收时 和 网络延迟大时 可能发生），那么 Zookeeper 服务端会认为 NameNode1 挂掉了，根据前面的主备切换逻辑，NameNode2 会替代 NameNode1 进入 Active 状态。但是此时 NameNode1 可能仍然处于 Active 状态正常运行，这样 NameNode1 和 NameNode2 都处于 Active 状态，都可以对外提供服务。这种情况称为脑裂。</p><p>脑裂对于NameNode这类对数据一致性要求非常高的系统来说是灾难性的，数据会发生错乱且无法恢复。zookeeper社区对这种问题的解决方法叫做fencing（隔离），也就是想办法把旧是Active NameNode隔离起来，使它不能正常对外提供服务。</p><p>在进行fencing的时候，会执行以下操作：</p><ol><li>首先尝试调用这个旧Active NameNode的HAServiceProtocol RPC接口的transition ToStandby方法，看能不能把它转换为Standby状态。</li><li>如果失败，那么就执行Hadoop配置文件之中预定义的隔离措施，Hadoop目前主要提供两种隔离措施，通常会选择sshfence：<ol><li>sshfence：通过SSH登录到目标机器上，执行命令fuser将对应的进程杀死；</li><li>shellfence：执行一个用户自定义的shell脚本来将对应的进程隔离。</li></ol></li></ol><h4 id="二十-小文件过多会有什么危害，如何避免？"><a href="#二十-小文件过多会有什么危害，如何避免？" class="headerlink" title="二十 小文件过多会有什么危害，如何避免？"></a>二十 小文件过多会有什么危害，如何避免？</h4><p>Hadoop上大量HDFS元数据信息存储在NameNode内存中，因此过多的小文件必定会压垮NameNode的内存。</p><p>每个元数据对象约占50byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间。</p><p>显而易见的解决这个问腿的方法就是合并小文件，可以选择在客户端上传时执行一定的策略先合并，或者是使用Hadoop的CombineFileInputFormat&lt;K,V&gt;实现小文件的合并。</p><h4 id="二十一-请说下HDFS的组织架构"><a href="#二十一-请说下HDFS的组织架构" class="headerlink" title="二十一 请说下HDFS的组织架构"></a>二十一 请说下HDFS的组织架构</h4><ol><li>Client：客户端<ol><li>切分文件。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理HDFS，比如启动关闭HDFS、访问HDFS目录及内容（hdfs dfs -ls &#x2F;）等</li></ol></li><li>NameNode：名称节点，也称主节点，存储数据的元数据信息，不存储具体的数据<ol><li>管理HDFS的名称空间</li><li>管理数据块映射信息</li><li>配置副本策略</li><li>处理客户端读写请求</li></ol></li><li>DataNode：数据节点，也称从节点。NameNode下达命令，DataNode执行实际的操作<ol><li>存储实际的数据块</li><li>执行数据块的读&#x2F;写操作</li></ol></li><li>Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务<ol><li>辅助NameNode，分担其工作量</li><li>定期合并Fsimage和Edits，并推送给NameNode</li><li>在紧急情况下，可辅助恢复NameNode</li></ol></li></ol><h4 id="二十二-请说下MR中的Map-Task的工作机制"><a href="#二十二-请说下MR中的Map-Task的工作机制" class="headerlink" title="二十二 请说下MR中的Map Task的工作机制"></a>二十二 请说下MR中的Map Task的工作机制</h4><p>简单概述：</p><p>inputFile通过split被切割为多个split文件，通过Record按行读取内容给map（自己写的处理逻辑的方法），数据被map处理完之后，交给OutputCollect收集器，对其结果key进行分区（默认使用的hashPartitioner），然后写入buffer，每个map task都有一个内存缓冲区（环形缓冲区），存放着map的输出结果，当缓冲区快满的时候需要将缓冲区的数据以一个临时文件的方式溢写到磁盘，当整个map task结束后再对磁盘中这个map task产生的所有临时文件做合并，生成最终的正式输出文件，然后等待reduce task的拉取。</p><p>详细步骤：</p><ol><li>读取数据组件 InputFormat（默认 TextInputFormat）会通过 getSplits 方法对输入目录中的文件进行逻辑切片规划得到InputSplit，有多少个InputSplit就对应启动多少个map task</li><li>将输入文件切分为InputSplit之后，由RecordReader对象（默认是LineRecordReader）进行读取，以 \n 作为分隔符，读取一行数据，返回&lt;key,value&gt;，key表示每行首字符偏移量，value表示这一行文本内容</li><li>读取InputSplit返回&lt;key,value&gt;，进入用户自己继承的Mapper类中，执行用户重写的map函数，RecordReader读取一行这里调用一次</li><li>Mapper逻辑结束之后，将Mapper的每条结果通过 context.write进行collect数据收集。在collect中，会先对其进行分区处理，默认使用HashPartitioner</li><li>接下来，会将数据写入内存，内存中这片区域叫做环形缓冲区（默认100M），缓冲区的作用是批量收集Mapper结果，减少磁盘IO的影响。我们的key&#x2F;value键值对以及Partition的结果都会被写入缓冲区。当然，写入之前，key与value值都会被序列化成字节数组</li><li>当环形缓冲区的数据达到溢写比例（默认0.8），也就是80M时，溢写线程启动，需要对这80M空间内的数据做排序和合并。排序是MR模型默认的行为，这里的排序也是对序列化的字节做的排序</li><li>合并溢写文件，每次溢写会在磁盘上生成一个临时溢出文件（写之前判断是否有Combiner），如果Mapper的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个临时溢出文件存在。当整个数据处理结束之后开始对磁盘中的所有临时文件进行Merge归并、排序，因为最终的文件只有一个写入磁盘，并且为这个文件提供了一个索引文件，以记录每个reduce对应数据的偏移量</li></ol><h4 id="二十三-请说下MR中Reduce-Task的工作机制"><a href="#二十三-请说下MR中Reduce-Task的工作机制" class="headerlink" title="二十三 请说下MR中Reduce Task的工作机制"></a>二十三 请说下MR中Reduce Task的工作机制</h4><p>简单描述：</p><p>Reduce大致分为copy、sort、reduce三个阶段，重点在前两个阶段</p><p>copy阶段包含一个eventFetcher来获取已完成的map列表，由Fetcher线程去copy数据，在此过程中启动两个merge线程，分别为inMemoryMerger和onDiskMerger，分别将内存中的数据merge到磁盘和磁盘中是数据进行merge。待数据copy完成之后，copy阶段就完成了。</p><p>开始sort阶段，sort阶段主要是执行finalMerge操作，纯粹的sort阶段，完成之后就是reduce阶段，调用用户定义的reduce函数进行处理</p><p>详细步骤：</p><ol><li><p>Copy阶段：简单地拉取数据。Reduce进程启动一些数据copy线程（Fetcher），通过HTTP方式请求map task获取属于自己的文件（map task的分区会标识每个map task属于哪个reduce task，默认reduce task的标识从0开始）</p></li><li><p>Merge阶段：在远程拷贝数据的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘文件过多</p><p>merge有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的数据量到达一定阈值，就直接启动内存到磁盘的merge。与map端类似，这也是溢写的过程，这个过程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。内存到磁盘的merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge方式生成最终的文件</p></li><li><p>合并排序：把分散的数据合并成一个大的数据后，还会再对合并后的数据排序</p></li><li><p>对排序后的键值对调用reduce方法：键相等的键值对调用一次reduce方法，每次调用会产生零个或者多个键值对，最后把这些输出的键值对写入到HDFS文件中</p></li></ol><h4 id="二十四-请说下MR中的shuffle阶段"><a href="#二十四-请说下MR中的shuffle阶段" class="headerlink" title="二十四 请说下MR中的shuffle阶段"></a>二十四 请说下MR中的shuffle阶段</h4><p>shuffle阶段分为四个步骤：分区、排序、规约、分组，其中前三个步骤在map阶段完成，最后一个步骤在reduce阶段完成</p><p>shuffle是MR的核心，它分布在MR的map阶段和reduce阶段。一般把从map产生输出开始到reduce取得数据作为输入之前的过程称作shuffle</p><ol><li>Collect阶段：将Map Task的结果输出到默认为100M的环形缓冲区，保存的是key&#x2F;value，Partition分区信息等</li><li>Spill阶段：当内存中的数据量达到一定的阈值的时候，就会将数据写入本地磁盘，在将数据写入磁盘之前需要对数据进行一次排序的操作，如果配置了Combiner，还会将有相同分区号和key的数据进行排序</li><li>Map Task阶段的Merge：把所有溢出的临时文件进行一次合并操作，以确保一个Map Task最终只产生一个中间数据文件</li><li>Copy阶段：Reduce Task启动Fetcher线程到已完成Map Task的节点上复制一份属于自己的数据，这些数据默认会保存在内存的缓冲区中，当内存的缓冲区达到一定阈值的时候，就会将数据写到磁盘上</li><li>Reduce Task阶段的Merge：在Reduce Task远程复制数据的同时，会在后台开启两个线程对内存到本地的数据文件进行合并操作</li><li>Sort阶段：在对数据进行合并的同时，会进行排序操作，由于Map Task阶段已经对数据进行了局部的排序，Reduce Task只需保证copy的数据的最终整体有效性即可</li></ol><p>Shuffle中的缓冲区大小会影响到MR程序的执行效率，原则上说，缓冲区越大，磁盘io的次数越少，执行速度越快。缓冲区的大小可以通过参数调整，参数：mapreduce.task.io.sort.mb 默认100M</p><h4 id="二十五-shuffle阶段的数据压缩机制了解吗？"><a href="#二十五-shuffle阶段的数据压缩机制了解吗？" class="headerlink" title="二十五 shuffle阶段的数据压缩机制了解吗？"></a>二十五 shuffle阶段的数据压缩机制了解吗？</h4><p>在shuffle阶段，可以看到数据通过大量拷贝，从map阶段输出的数据，都要通过网络拷贝，发送到reduce阶段，这一过程中，涉及到大量的网络IO，如果数据能够进行压缩，那么数据的发送量就会少很多</p><p>hadoop当中支持的压缩算法：gzip、bzip2、LZO、LZ4、Snappy，这几种压缩算法综合压缩和解压缩的速率，谷歌的Snappy是最优的，一般都选择Snappy压缩。</p><h4 id="二十六-在写MR时，什么情况下可以使用规约？"><a href="#二十六-在写MR时，什么情况下可以使用规约？" class="headerlink" title="二十六 在写MR时，什么情况下可以使用规约？"></a>二十六 在写MR时，什么情况下可以使用规约？</h4><p>规约（Combiner）是不能够影响任务的运行结果的局部汇总，适用于求和类，不适用于求平均值，如果reduce的输入参数类型和输出参数的类型是一样的，则规约的类可以使用reduce类，只需要在驱动类中指明规约的类即可</p><h4 id="二十七-YARN集群的架构和工作原理知道多少？"><a href="#二十七-YARN集群的架构和工作原理知道多少？" class="headerlink" title="二十七 YARN集群的架构和工作原理知道多少？"></a>二十七 YARN集群的架构和工作原理知道多少？</h4><p>YARN的基本设计思想是将MapReduce V1中的JobTracker拆分为两个独立的服务：ResourceManager和ApplicationMaster</p><p>ResourceManager负责整个系统的资源管理和分配，ApplicationMaster负责单个应用程序的管理</p><ol><li><p>ResourceManager：RM是一个全局的资源管理器，负责整个系统的资源管理和分配，它由两个部分组成：调度器（Scheduler）和应用程序管理器（Application Manager）</p><p>调度器根据容量、队列等限制条件，将系统中的资源分配给正在运行的应用程序，在保证容量、公平性和服务等级的前提下，优化集群资源利用率，让所有的资源都被充分利用。应用程序管理器负责管理整个系统中的所有的应用程序，包括应用程序的提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重启它。</p></li><li><p>ApplicationMaster：用户提交的一个应用程序会ApplicationMaster，它的主要功能有：</p><ol><li>与RM调度器协商以获得资源，资源以Container表示</li><li>将得到的任务进一步分配给内部的任务</li><li>与NM通信与启动&#x2F;停止任务</li><li>监控所有的内部任务状态，并在任务运行失败的时候重新为任务申请资源以重启任务</li></ol></li><li><p>NodeManager：NodeManager是每个节点上的资源和任务管理器，一方面，它会定期地向RM汇报本节点上的资源使用情况和各个Container的运行状态；另一方面，他接收并处理来自AM的Container启动和停止使用请求</p></li><li><p>Container：Container是YARN中的资源抽象，封装了各种资源。一个应用程序会分配一个Container，这个应用程序只能使用Container中描述的资源。不同于MapReduce V1中槽位solt的资源封装，Container是一个动态资源的划分单位，更能充分利用资源。</p></li></ol><blockquote><p>可以把YARN中的ResourceManager比作一个工程公司，其中调度器比作财务部，负责财务（资源）的管理；应用程序管理器比作研发部，负责管理研发任务；ApplicationMaster比作研发部门的小组，但是不是事先存在的，有任务才产生；NodeManager比作研发小组中的组员；Container比作给的钱。<br>当一个研发任务被客户提交给公司，研发部跟财务部协商，打第一份钱来组成研发小组，研发小组成立，如果还有足够余钱（因为组成这个小组会花钱，比如配电脑什么的），就直接分配任务和钱给组员。否则就向财务部申请money，再分配给组员。在这个过程中，组员会向小组汇报任务进程，小组会向研发部汇报进程。</p></blockquote><h4 id="二十八-YARN的任务提交流程是怎样的？"><a href="#二十八-YARN的任务提交流程是怎样的？" class="headerlink" title="二十八 YARN的任务提交流程是怎样的？"></a>二十八 YARN的任务提交流程是怎样的？</h4><p>当JobClient向YARN提交一个应用程序后，YARN将分两个阶段运行这个应用程序：一是启动ApplicationMaster；二是由ApplicationMaster创建应用程序，为它申请资源，监控运行直到结束。具体步骤如下：</p><ol><li>用户向YARN提交一个应用程序，并指定ApplicationMaster程序、启动ApplicationMaster、用户程序</li><li>RM为这个应用程序分配第一个Container，并与之对应的NM通讯，要求他在这个Container中启动应用程序ApplicationMaster</li><li>ApplicationMaster向RM注册，然后拆分为内部各个子任务，为各个内部任务申请资源，并监控这些任务的运行，直到结束</li><li>AM采用轮询的方式向RM申请和领取资源</li><li>RM为AM分配资源，以Container形式返回</li><li>AM申请到资源后，便与之对应的NM通讯，要求NM启动任务</li><li>NodeManager为任务设置好运行环境，将任务启动命令写到一个脚本中，并通过运行这个脚本启动任务</li><li>各个任务向AM汇报自己的状态和进度，以便当任务失败时可以重启任务</li><li>应用程序完成后，ApplicationMaster向ResourceManager注销并关闭自己</li></ol><h4 id="二十九-YARN的资源调度三种模型了解吗？"><a href="#二十九-YARN的资源调度三种模型了解吗？" class="headerlink" title="二十九 YARN的资源调度三种模型了解吗？"></a>二十九 YARN的资源调度三种模型了解吗？</h4><p>在YARN中有三种调度器可以选择：FIFO Scheduler、Capacity Scheduler、Fair Scheduler</p><p>Apache版本的hadoop默认使用的是Capacity Scheduler调度方式。CDH版本的默认使用的是Fair Scheduler调度方式</p><p>FIFO Scheduler（先来先服务）：</p><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用分配资源，待最头上的应用需求满足后再给下一个分配</p><p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用可能会占用所有集群资源，这就导致其他应用被阻塞。</p><p>Capacity Scheduler（能力调度器）：</p><p>对于Capacity调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO Scheduler调度器时的时间</p><p>Fair Scheduler（公平调度器）：</p><p>在Fair调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有的job动态的调整系统资源</p><p>比如：当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小job提交后，Fair调度器会分配一半资源给这个小job，让这两个任务公平的共享集群资源</p><p>需要注意的是，在Fair调度器中，第二个任务从提交到获得资源会有一定延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后，也会释放自己占用的资源，大人物又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E9%9D%A2%E8%AF%95/">面试</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/hadoop/">hadoop</category>
      
      
      <comments>http://xuanyin02.github.io/2023/05169473.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于数据仓库hive知识的学习(2)</title>
      <link>http://xuanyin02.github.io/2023/05142227.html</link>
      <guid>http://xuanyin02.github.io/2023/05142227.html</guid>
      <pubDate>Sun, 14 May 2023 14:56:31 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;第6章-SQL快速掌握&quot;&gt;&lt;a href=&quot;#第6章-SQL快速掌握&quot; class=&quot;headerlink&quot; title=&quot;第6章 SQL快速掌握&quot;&gt;&lt;/a&gt;第6章 SQL快速掌握&lt;/h3&gt;&lt;h4 id=&quot;6-1-sql的运算模型-–-逐行运算模型&quot;&gt;&lt;a hre</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="第6章-SQL快速掌握"><a href="#第6章-SQL快速掌握" class="headerlink" title="第6章 SQL快速掌握"></a>第6章 SQL快速掌握</h3><h4 id="6-1-sql的运算模型-–-逐行运算模型"><a href="#6-1-sql的运算模型-–-逐行运算模型" class="headerlink" title="6.1 sql的运算模型 – 逐行运算模型"></a>6.1 sql的运算模型 – 逐行运算模型</h4><p>逐行运算：select后的运算表达式，是对每一行独立运算</p><p>表：一个数据的集合（集合中每一行就是一条数据：记录）</p><pre><code>select：    对一条数据的运算逻辑    -- 常量 :&quot;ok&quot; ,10 ,8.9    -- 变量 : id , name ,  age,    -- 运算符表达式：  age+10,  id+8 , id&gt;8  , id&gt;2 and id&lt;9    -- 函数表达式：  upper(name) , greatest(s1,s2,s3)     -- 复合表达式：  greatest(s1,s2,s3)&gt;2 , greatest(s1,s2,s3)+10 , lower(substr(upper(name),0,3))</code></pre><pre><code>from ：选择一个运算的数据集 </code></pre><pre><code>join：准备数据集（它可以将多个数据集拼成一个数据集，拼的时候可以带条件）join的拼接有多种方式：t1 join t2  --&gt; 笛卡尔积,结果的总行数的t1的行数*t2的行数；t1 join t2 on t1.id=t2.id  --&gt;内连接，满足拼接条件的才拼接..t1 left join t2 on t1.id=t2.id --&gt; 左（外）连接 left outer join ;左表所有行都保留，连接不上的右表字段为nullt1 right join t2 on t1.id=t2.id --&gt;右（外）连接 right outer join;右表的所有行都保留，连接不上的左表字段为nullt1 full join t2  on t1.id=t2.id --&gt;全（外）连接 full outer join;左、右表的行都保留，拼不上的字段为null-- hive不支持不等值join-- hive中有一种特别的join    left semi join  --&gt;左半连接，是sql中in子句的一个变种实现！    hive1.x中不支持in，现在的新版本hive支持in子句</code></pre><pre><code>where：逐行过滤将要运算的数据集,where执行在select之前   -- where id&gt;2   -- where (id+10)&gt;2   -- where upper(name) = &#39;ZHANGSAN&#39;   -- where id in (select id from t_x)</code></pre><pre><code>group by：函数：类似java代码中的方法，接收变量，返回结果，也就是一个表达式having：紧跟group by之后，是对分组后的数据进行按组过滤，将不满足条件表达式的组去除；</code></pre><h4 id="6-2-sql运算模型–分组聚合运算"><a href="#6-2-sql运算模型–分组聚合运算" class="headerlink" title="6.2 sql运算模型–分组聚合运算"></a>6.2 sql运算模型–分组聚合运算</h4><p>– 分组运算模型中，select后面的表达式只能有如下情形：</p><ol><li>常量</li><li>分组key</li><li>聚合函数</li></ol><p><strong>select语句中非分组函数的字段必须声明在GORUP BY中反之，GROUP BY中声明的字段可以不出现在select语句中</strong></p><p><strong>分组key可以有多个，分组key越多，分出来的组也会越多</strong></p><blockquote><p><em><strong>分组聚合还可以采用 partition by</strong></em></p></blockquote><h4 id="6-3-sql运算模型–开窗运算（窗口分析运算模型）"><a href="#6-3-sql运算模型–开窗运算（窗口分析运算模型）" class="headerlink" title="6.3 sql运算模型–开窗运算（窗口分析运算模型）"></a>6.3 sql运算模型–开窗运算（窗口分析运算模型）</h4><p>– 可以用窗口分析函数：row_number() over() 来实现</p><p>row_number() over(partition by sex order by salary desc) as rn</p><h3 id="第7章-HQL查询语法详解"><a href="#第7章-HQL查询语法详解" class="headerlink" title="第7章 HQL查询语法详解"></a>第7章 HQL查询语法详解</h3><pre class=" language-sql"><code class="language-sql"><span class="token punctuation">[</span><span class="token keyword">WITH</span> CommonTableExpression <span class="token punctuation">(</span><span class="token punctuation">,</span> CommonTableExpression<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">]</span>    <span class="token punctuation">(</span>Note: Only available starting <span class="token keyword">with</span> Hive <span class="token number">0.13</span><span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">SELECT</span> <span class="token punctuation">[</span><span class="token keyword">ALL</span> <span class="token operator">|</span> <span class="token keyword">DISTINCT</span><span class="token punctuation">]</span> select_expr<span class="token punctuation">,</span> select_expr<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>  <span class="token keyword">FROM</span> table_reference  <span class="token punctuation">[</span><span class="token keyword">WHERE</span> where_condition<span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token keyword">GROUP</span> <span class="token keyword">BY</span> col_list<span class="token punctuation">]</span>  <span class="token punctuation">[</span><span class="token keyword">ORDER</span> <span class="token keyword">BY</span> col_list<span class="token punctuation">]</span>  <span class="token punctuation">[</span>CLUSTER <span class="token keyword">BY</span> col_list    <span class="token operator">|</span> <span class="token punctuation">[</span>DISTRIBUTE <span class="token keyword">BY</span> col_list<span class="token punctuation">]</span> <span class="token punctuation">[</span>SORT <span class="token keyword">BY</span> col_list<span class="token punctuation">]</span>  <span class="token punctuation">]</span></code></pre><h4 id="7-1-with…as…临时表语法"><a href="#7-1-with…as…临时表语法" class="headerlink" title="7.1 with…as…临时表语法"></a>7.1 with…as…临时表语法</h4><blockquote><p>示例：</p></blockquote><pre class=" language-sql"><code class="language-sql"><span class="token keyword">with</span> o <span class="token keyword">as</span> <span class="token punctuation">(</span><span class="token keyword">select</span>id<span class="token punctuation">,</span>m<span class="token punctuation">,</span><span class="token function">sum</span><span class="token punctuation">(</span>sale<span class="token punctuation">)</span> <span class="token keyword">as</span> amt<span class="token keyword">from</span> t_sale<span class="token keyword">group</span> <span class="token keyword">by</span> id<span class="token punctuation">,</span>m<span class="token punctuation">)</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> o <span class="token keyword">where</span> o<span class="token punctuation">.</span>amt<span class="token operator">></span><span class="token number">80</span><span class="token punctuation">;</span></code></pre><h4 id="7-2-排序"><a href="#7-2-排序" class="headerlink" title="7.2 排序"></a>7.2 排序</h4><h5 id="7-2-1-全局排序（Order-By）"><a href="#7-2-1-全局排序（Order-By）" class="headerlink" title="7.2.1 全局排序（Order By）"></a>7.2.1 全局排序（Order By）</h5><p>全局排序，强制只有一个reduce</p><h5 id="7-2-2-Sort-By"><a href="#7-2-2-Sort-By" class="headerlink" title="7.2.2 Sort By"></a>7.2.2 Sort By</h5><p>每个task内部进行排序，对全局结果集来说不是排序</p><p>要设置reduce个数：set mapreduce.job.reduces&#x3D;3;</p><p>当设置的reduce为1时，结果与Order By一样</p><h5 id="7-2-3-分桶排序1（Distribute-By-Sort-By）"><a href="#7-2-3-分桶排序1（Distribute-By-Sort-By）" class="headerlink" title="7.2.3 分桶排序1（Distribute By + Sort By）"></a>7.2.3 分桶排序1（Distribute By + Sort By）</h5><p>Distribute By：类似MR中partition，进行分区，可以结合sort by使用。<br>注意，Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。<br>对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。</p><p>示例：先按照部门编号分区，再按照员工编号降序排序</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>job<span class="token punctuation">.</span>reduces<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">;</span><span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> emp distribute <span class="token keyword">by</span> 部门编号 sort <span class="token keyword">by</span> 员工编号 <span class="token keyword">desc</span><span class="token punctuation">;</span></code></pre><h5 id="7-2-4-分桶排序2（Cluster-By）"><a href="#7-2-4-分桶排序2（Cluster-By）" class="headerlink" title="7.2.4 分桶排序2（Cluster By）"></a>7.2.4 分桶排序2（Cluster By）</h5><p>当distribute by和sorts by字段相同时，可以使用cluster by代替</p><p>cluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是升序排序，不能指定排序规则为ASC或者DESC。</p>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/hive/">hive</category>
      
      
      <comments>http://xuanyin02.github.io/2023/05142227.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于多功能引擎ES知识的学习</title>
      <link>http://xuanyin02.github.io/2023/051139179.html</link>
      <guid>http://xuanyin02.github.io/2023/051139179.html</guid>
      <pubDate>Thu, 11 May 2023 01:57:47 GMT</pubDate>
      
      <description>介绍了分布式全文搜索引擎ES的知识</description>
      
      
      
      <content:encoded><![CDATA[<p>想必大家都知道ES，一款非常好用的工具，但是我还是想详细解释一下它的概念：ES全称Elasticsearch，是一个<strong>开源的高扩展的分布式全文搜索引擎</strong>，是整个Elastic Stack技术栈的核心。它可以近乎实时的存储、检索数据；本身扩展性很好，可以扩展到上百台服务器，处理PB级别的数据。可以看出ES功能非常强大，所以现在国内外很多公司都在使用。</p><p>看到这里又会有人说了：传统数据库不行吗？还真不行（也不是不行，就是效果不好），哈哈哈哈。传统数据库实现全文检索的话很鸡肋，因为一般不会用传统数据库存非结构化的像文本字段等数据。进行全文检索需要扫描整个表，如果数据量大的话即使对SQL的语法优化，也收效甚微。建立了索引，但是维护起来也很麻烦，对于insert和update操作都会重新构建索引。基于以上原因可以分析得出，在一些生产环境中，使用常规的搜索方式，性能是非常差的：</p><ul><li>搜索的数据对象是大量的非结构化的文本数据</li><li>文件记录量达到数十万或数百万个甚至更多</li><li>支持大量基于交互式文本的查询</li><li>需求非常灵活的全文搜索查询</li><li>对高度相关的搜索结果的有特殊需求，但是没有可用的关系数据库可以满足</li><li>对不同记录类型、非文本数据操作或安全事务处理的需求相对较少的情况</li></ul><p>为了解决这些问题，我们就需要<strong>全文搜索引擎</strong>，它的工作原理是计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户。这个过程类似于通过字典中的检索字表查字的过程。</p><h3 id="首先我们来讲讲-ES-入门知识"><a href="#首先我们来讲讲-ES-入门知识" class="headerlink" title="首先我们来讲讲 ES 入门知识"></a>首先我们来讲讲 ES 入门知识</h3><h4 id="数据格式"><a href="#数据格式" class="headerlink" title="数据格式"></a>数据格式</h4><p>Elasticsearch 是面向文档型数据库，一条数据在这里就是一个文档。将 Elasticsearch 里存储文档数据和关系型数据库 MySQL 存储数据的概念进行一个类比</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521151302269.png" alt="image-20230521151302269"><figcaption>image-20230521151302269</figcaption></figure></p><blockquote><p>Elasticsearch 7.X 中, Type 的概念已经被删除</p></blockquote><h4 id="HTTP操作"><a href="#HTTP操作" class="headerlink" title="HTTP操作"></a>HTTP操作</h4><h5 id="索引操作"><a href="#索引操作" class="headerlink" title="索引操作"></a>索引操作</h5><h6 id="1）创建索引"><a href="#1）创建索引" class="headerlink" title="1）创建索引"></a>1）创建索引</h6><p>向 ES 服务器发 <strong>PUT</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping">http://127.0.0.1:9200/shopping <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521151518186.png" alt="image-20230521151518186"><figcaption>image-20230521151518186</figcaption></figure></p><h6 id="2）查看所有索引"><a href="#2）查看所有索引" class="headerlink" title="2）查看所有索引"></a>2）查看所有索引</h6><p>向 ES 服务器发 <strong>GET</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/_cat/indices?v">http://127.0.0.1:9200/_cat/indices?v <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521151602723.png" alt="image-20230521151602723"><figcaption>image-20230521151602723</figcaption></figure></p><h6 id="3）查看单个索引"><a href="#3）查看单个索引" class="headerlink" title="3）查看单个索引"></a>3）查看单个索引</h6><p>向 ES 服务器发 <strong>GET</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping">http://127.0.0.1:9200/shopping <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521151654728.png" alt="image-20230521151654728"><figcaption>image-20230521151654728</figcaption></figure></p><h6 id="4）删除索引"><a href="#4）删除索引" class="headerlink" title="4）删除索引"></a>4）删除索引</h6><p>向 ES 服务器发 <strong>DELETE</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping">http://127.0.0.1:9200/shopping <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521151727176.png" alt="image-20230521151727176"><figcaption>image-20230521151727176</figcaption></figure></p><h5 id="文档操作"><a href="#文档操作" class="headerlink" title="文档操作"></a>文档操作</h5><h6 id="1）创建文档"><a href="#1）创建文档" class="headerlink" title="1）创建文档"></a>1）创建文档</h6><p>向 ES 服务器发 <strong>POST</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_doc">http://127.0.0.1:9200/shopping/_doc <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为:</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"title"</span><span class="token operator">:</span><span class="token string">"小米手机"</span><span class="token punctuation">,</span><span class="token property">"category"</span><span class="token operator">:</span><span class="token string">"小米"</span><span class="token punctuation">,</span><span class="token property">"images"</span><span class="token operator">:</span><span class="token string">"http://www.gulixueyuan.com/xm.jpg"</span><span class="token punctuation">,</span><span class="token property">"price"</span><span class="token operator">:</span><span class="token number">3999.00</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521152238668.png" alt="image-20230521152238668"><figcaption>image-20230521152238668</figcaption></figure></p><p>上面的数据创建后，由于没有指定数据唯一性标识（ID），默认情况下，ES 服务器会随机生成一个。</p><p>如果想要自定义唯一性标识，需要在创建时指定：<a class="link" href="http://127.0.0.1:9200/shopping/_doc/1">http://127.0.0.1:9200/shopping/_doc/1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h6 id="2）查看文档"><a href="#2）查看文档" class="headerlink" title="2）查看文档"></a>2）查看文档</h6><p>向 ES 服务器发 <strong>GET</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_doc/1">http://127.0.0.1:9200/shopping/_doc/1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h6 id="3）修改文档"><a href="#3）修改文档" class="headerlink" title="3）修改文档"></a>3）修改文档</h6><p>向 ES 服务器发 <strong>POST</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_doc/1">http://127.0.0.1:9200/shopping/_doc/1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"title"</span><span class="token operator">:</span><span class="token string">"华为手机"</span><span class="token punctuation">,</span><span class="token property">"category"</span><span class="token operator">:</span><span class="token string">"华为"</span><span class="token punctuation">,</span><span class="token property">"images"</span><span class="token operator">:</span><span class="token string">"http://www.gulixueyuan.com/hw.jpg"</span><span class="token punctuation">,</span><span class="token property">"price"</span><span class="token operator">:</span><span class="token number">4999.00</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h6 id="4）修改字段"><a href="#4）修改字段" class="headerlink" title="4）修改字段"></a>4）修改字段</h6><p>向 ES 服务器发 <strong>POST</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_update/1">http://127.0.0.1:9200/shopping/_update/1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为:</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"doc"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"price"</span><span class="token operator">:</span><span class="token number">3000.00</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h6 id="5）删除文档"><a href="#5）删除文档" class="headerlink" title="5）删除文档"></a>5）删除文档</h6><blockquote><p>删除一个文档不会立即从磁盘上移除，它只是被标记成已删除（逻辑删除）</p></blockquote><p>向 ES 服务器发 <strong>DELETE</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_doc/1">http://127.0.0.1:9200/shopping/_doc/1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h6 id="6）条件删除文档"><a href="#6）条件删除文档" class="headerlink" title="6）条件删除文档"></a>6）条件删除文档</h6><p>向 ES 服务器发 <strong>POST</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/shopping/_delete_by_query">http://127.0.0.1:9200/shopping/_delete_by_query <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为:</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span><span class="token property">"query"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"match"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"price"</span><span class="token operator">:</span><span class="token number">4000.00</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h5 id="映射操作"><a href="#映射操作" class="headerlink" title="映射操作"></a>映射操作</h5><p>有了索引库，等于有了数据库中的 database。接下来就需要创建索引库(index)中的映射了，类似于数据库(database)中的表结构(table)。创建数据库表需要设置字段名称，类型，长度，约束等；索引库也一样，需要知道这个类型下有哪些字段，每个字段有哪些约束信息，这就叫做映射(mapping)。</p><h6 id="1）创建映射"><a href="#1）创建映射" class="headerlink" title="1）创建映射"></a>1）创建映射</h6><p>向 ES 服务器发 <strong>PUT</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/student/_mapping">http://127.0.0.1:9200/student/_mapping <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"properties"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"name"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>            <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">true</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>        <span class="token property">"sex"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>            <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">false</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>        <span class="token property">"age"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"long"</span><span class="token punctuation">,</span>            <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">false</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><p>映射数据说明：</p><ul><li><p>字段名：任意填写，下面指定许多属性，例如：title、subtitle、images、price</p><p>​type：类型，Elasticsearch 中支持的数据类型非常丰富，说几个关键的：</p><p>​String 类型，又分两种：</p><p>​text：可分词<br>keyword：不可分词，数据会作为完整字段进行匹配</p><p>​Numerical：数值类型，分两类</p><p>​基本数据类型：long、integer、short、byte、double、float、half_float</p><p>​浮点数的高精度类型：scaled_float</p><p>​Date：日期类型</p><p>​Array：数组类型</p><p>​Object：对象</p></li><li><p>index：是否索引，默认为 true，也就是说你不进行任何配置，所有字段都会被索引。</p><p>​true：字段会被索引，则可以用来进行搜索</p><p>​false：字段不会被索引，不能用来搜索</p></li><li><p>store：是否将数据进行独立存储，默认为 false</p><p>​原始的文本会存储在_source 里面，默认情况下其他提取出来的字段都不是独立存储的，是从_source 里面提取出来的。当然你也可以独立的存储某个字段，只要设置”store”: true 即可，获取独立存储的字段要比从_source 中解析快得多，但是也会占用更多的空间，所以要根据实际业务需求来设置。</p></li><li><p>analyzer：分词器，这里的 ik_max_word 即使用 ik 分词器,后面会有专门的章节学习</p></li></ul><h6 id="2）查看映射"><a href="#2）查看映射" class="headerlink" title="2）查看映射"></a>2）查看映射</h6><p>向 ES 服务器发 <strong>GET</strong> 请求 ：<a class="link" href="http://127.0.0.1:9200/student/_mapping">http://127.0.0.1:9200/student/_mapping <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><h6 id="3）索引映射关联"><a href="#3）索引映射关联" class="headerlink" title="3）索引映射关联"></a>3）索引映射关联</h6><p>向 ES 服务器发 PUT 请求 ：<a class="link" href="http://127.0.0.1:9200/student1">http://127.0.0.1:9200/student1 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p><p>请求体内容为：</p><pre class=" language-json"><code class="language-json">&amp;#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token property">"settings"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>    <span class="token property">"mappings"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token property">"properties"</span><span class="token operator">:</span> &amp;#<span class="token number">123</span><span class="token punctuation">;</span>            <span class="token property">"name"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>                <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">true</span>            &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>            <span class="token property">"sex"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"text"</span><span class="token punctuation">,</span>                <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">false</span>            &amp;#<span class="token number">125</span><span class="token punctuation">;</span><span class="token punctuation">,</span>            <span class="token property">"age"</span><span class="token operator">:</span>&amp;#<span class="token number">123</span><span class="token punctuation">;</span>                <span class="token property">"type"</span><span class="token operator">:</span> <span class="token string">"long"</span><span class="token punctuation">,</span>                <span class="token property">"index"</span><span class="token operator">:</span> <span class="token boolean">false</span>            &amp;#<span class="token number">125</span><span class="token punctuation">;</span>        &amp;#<span class="token number">125</span><span class="token punctuation">;</span>    &amp;#<span class="token number">125</span><span class="token punctuation">;</span>&amp;#<span class="token number">125</span><span class="token punctuation">;</span></code></pre><h4 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h4><h3 id="然后再说说-ES-进阶知识"><a href="#然后再说说-ES-进阶知识" class="headerlink" title="然后再说说 ES 进阶知识"></a>然后再说说 ES 进阶知识</h3><h4 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h4><ul><li><p>索引（Index）</p><p>一个索引就是一个拥有几份相似特征的文档的集合。一个索引由一个名字来标识（<strong>必须全部是小写字母</strong>），并且当我们要对这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。在一个集群中，可以定义任意多的索引。</p><p>能搜索的数据必须索引，这样的好处是可以提高查询速度，比如：新华字典前面的目录就是索引的意思，目录可以提高查询速度。</p><blockquote><p><strong>Elasticsearch  索引的精髓：一切设计都是为了提高搜索的性能</strong></p></blockquote></li><li><p>类型（Type）</p><p>一个类型是你的索引的一个逻辑上的分类&#x2F;分区。ElasticSearch 7.x 默认不再支持自定义索引类型（默认类型为：_doc）。</p></li><li><p>文档（Document）</p><p>一个文档是一个可被索引的基础信息单元，也就是一条数据。文档以 JSON 格式来表示，而 JSON 是一个<br>到处存在的互联网数据交互格式。</p></li><li><p>字段（Field）</p><p>相当于是数据表的字段，对文档数据根据不同属性进行的分类标识。</p></li><li><p>映射（Mapping）</p></li><li><p>分片（Shards）</p><p>被混淆的概念是，一个 Lucene  索引  我们在 Elasticsearch  称作  分片  。  一个 Elasticsearch  索引  是分片的集合。  当 Elasticsearch  在索引中搜索的时候，  他发送查询到每一个属于索引的分片(Lucene  索引) ，然后合并每个分片的结果到一个全局的结果集</p></li><li><p>副本（Replicas）</p></li><li><p>分配（Allocation）</p><p>将分片分配给某个节点的过程，包括分配主分片或者副本。如果是副本，还包含从主分片复制数据的过程。这个过程是由 master 节点完成的。</p></li></ul><h4 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521163302271.png" alt="image-20230521163302271"><figcaption>image-20230521163302271</figcaption></figure></p><p>当一个节点被选举成为主节点时， 它将负责管理集群范围内的所有变更，例如增加、删除索引，或者增加、删除节点等。 而主节点并不需要涉及到文档级别的变更和搜索等操作，所以当集群只拥有一个主节点的情况下，即使流量的增加它也不会成为瓶颈。</p><p>作为用户，我们可以将请求发送到集群中的任何节点 ，包括主节点。 <strong>每个节点都知道任意文档所处的位置，并且能够将我们的请求直接转发到存储我们所需文档的节点</strong>。 无论我们将请求发送到哪个节点，它都能负责从各个包含我们所需文档的节点收集回数据，并将最终结果返回給客户端。 Elasticsearch 对这一切的管理都是透明的。</p><h4 id="路由计算"><a href="#路由计算" class="headerlink" title="路由计算"></a>路由计算</h4><p>当索引一个文档的时候，文档会被存储到一个主分片中。Elasticsearch 如何知道一个文档应该存放到哪个分片中呢？当我们创建文档时，它如何决定这个文档应当被存储在分片 1 还是分片 2 中呢？</p><p>通过路由计算我们就能知道以上答案</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521163737067.png" alt="image-20230521163737067"><figcaption>image-20230521163737067</figcaption></figure></p><blockquote><p><strong>routing 默认是文档的_id，也可以设置成自定义值。number_of_primary_shards 是主分片的数量</strong></p></blockquote><h4 id="分片控制"><a href="#分片控制" class="headerlink" title="分片控制"></a>分片控制</h4><p>我们可以发送请求到集群中的任一节点。 每个节点都有能力处理任意请求。 每个节点都知道集群中任一文档位置，所以可以直接将请求转发到需要的节点上。 在下面的例子中，将所有的请求发送到 Node1，我们将其称为<strong>协调节点</strong>(coordinating node) 。</p><blockquote><p>当发送请求的时候，为了扩展负载，更好的做法是轮询集群中所有的节点</p></blockquote><h5 id="写流程"><a href="#写流程" class="headerlink" title="写流程"></a>写流程</h5><p>新建、索引和删除请求都是<strong>写</strong>操作， <strong>必须在主分片上面完成之后才能被复制到相关的副本分片</strong></p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521164742537.png" alt="image-20230521164742537"><figcaption>image-20230521164742537</figcaption></figure></p><ol><li>客户端向Node1发送新建、索引或者删除请求</li><li>节点使用文档的 _id 确定文档属于分片 0 。请求会被转发到 Node3，因为分片 0 的主分片目前被分配在 Node3 上</li><li>Node3 在主分片上面执行请求。如果成功了，它将请求并行转发到 Node1 和 Node2的副本分片上。一旦所有的副本分片都报告成功, Node3 将向协调节点报告成功，协调节点向客户端报告成功</li></ol><h5 id="读流程"><a href="#读流程" class="headerlink" title="读流程"></a>读流程</h5><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521164935969.png" alt="image-20230521164935969"><figcaption>image-20230521164935969</figcaption></figure></p><ol><li>客户端向 Node1 发送获取请求</li><li>节点使用文档的 _id 来确定文档属于分片 0 。分片 0 的副本分片存在于所有的三个节点上。 在这种情况下，它将请求转发到 Node2 </li><li>Node2 将文档返回给 Node1，然后将文档返回给客户端</li></ol><h5 id="更新流程"><a href="#更新流程" class="headerlink" title="更新流程"></a>更新流程</h5><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521165313434.png" alt="image-20230521165313434"><figcaption>image-20230521165313434</figcaption></figure></p><ol><li>客户端向 Node1 发送更新请求</li><li>它将请求转发到主分片所在的 Node3</li><li>Node3 从主分片检索文档，修改 _source 字段中的 JSON ，并且尝试重新索引主分片的文档。 如果文档已经被另一个进程修改，它会重试步骤 3 ，超过 retry_on_conflict 次后放弃</li><li>如果 Node3 成功地更新文档，它将<strong>完整的新版本的文档</strong>并行转发到 Node1 和 Node2 上的副本分片，重新建立索引。一旦所有副本分片都返回成功， Node3 向协调节点也返回成功，协调节点向客户端返回成功</li></ol><h4 id="分片原理"><a href="#分片原理" class="headerlink" title="分片原理"></a>分片原理</h4><p>分片是 Elasticsearch 最小的工作单元。但是究竟什么是一个分片，它是如何工作的？</p><p>文本字段中的每个单词需要被搜索，对数据库意味着需要单个字段有索引多值的能力。最好的支持是一个字段多个值<br>需求的数据结构是<strong>倒排索引</strong></p><h5 id="倒排索引"><a href="#倒排索引" class="headerlink" title="倒排索引"></a>倒排索引</h5><p>首先来看看什么是正向索引</p><p><strong>正向索引</strong>，就是搜索引擎会将待搜索的文件都对应一个文件 ID，搜索时将这个ID 和搜索关键字进行对应，形成 K-V 对，然后对关键字进行统计计数</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521175100274.png" alt="image-20230521175100274"><figcaption>image-20230521175100274</figcaption></figure></p><p>但是搜索引擎中的文档的数目是个天文数字，这样的索引结构根本无法满足实时返回结果要求，所以搜索引擎会将正向索引重新构建为倒排索引</p><p><strong>倒排索引</strong>，即把文件ID对应到关键词的映射转换为关键词到文件ID的映射，每个关键词都对应着一系列的文件，这些文件中都出现这个关键词</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521175237157.png" alt="image-20230521175237157"><figcaption>image-20230521175237157</figcaption></figure></p><p>分词和标准化的过程称为<strong>分析</strong></p><blockquote><p>倒排索引被写入磁盘后是不可改变的:它永远不会修改</p></blockquote><p>不变性有重要的价值：</p><ul><li>不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题</li><li>一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升</li><li>其它缓存(像 filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化</li><li>写入单个大的倒排索引允许数据被压缩，减少磁盘 I&#x2F;O 和 需要被缓存到内存的索引的使用量</li></ul><h5 id="动态更新索引"><a href="#动态更新索引" class="headerlink" title="动态更新索引"></a>动态更新索引</h5><p>如何在保留不变性的前提下实现倒排索引的更新？</p><p><strong>更新方案</strong>：用更多的索引。通过增加新的补充索引来反映新近的修改，而不是直接重写整个倒排索引。每一个倒排索引都会被轮流查询到，从最早的开始查询完后再对结果进行合并。</p><h5 id="按段搜索"><a href="#按段搜索" class="headerlink" title="按段搜索"></a>按段搜索</h5><p>按段搜索会以如下流程执行：</p><ol><li><p>新文档被收集到内存索引缓存</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521184530401.png" alt="image-20230521184530401"><figcaption>image-20230521184530401</figcaption></figure></p></li><li><p>不时地, 缓存被提交</p><p>(1) 一个新的段–一个追加的倒排索引—被写入磁盘。<br>(2) 一个新的包含新段名字的 提交点 被写入磁盘<br>(3) 磁盘进行 同步 — 所有在文件系统缓存中等待的写入都刷新到磁盘，以确保它们被写入物理文件</p></li><li><p>新的段被开启，让它包含的文档可见以被搜索</p></li><li><p>内存缓存被清空，等待接收新的文档</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230521184659247.png" alt="image-20230521184659247"><figcaption>image-20230521184659247</figcaption></figure></p></li></ol><h3 id="最后聊一下如何对-ES-进行优化"><a href="#最后聊一下如何对-ES-进行优化" class="headerlink" title="最后聊一下如何对 ES 进行优化"></a>最后聊一下如何对 ES 进行优化</h3><h4 id="硬件选择"><a href="#硬件选择" class="headerlink" title="硬件选择"></a>硬件选择</h4><ul><li>使用SSD。固态硬盘比机械硬盘优秀很多</li><li>使用 RAID 0。条带化 RAID 会提高磁盘 I&#x2F;O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能</li><li>另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面</li><li>不要使用远程挂载的存储，比如 NFS 或者 SMB&#x2F;CIFS。这个引入的延迟对性能来说完全是背道而驰的</li></ul><h4 id="分片策略"><a href="#分片策略" class="headerlink" title="分片策略"></a>分片策略</h4>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/es/">es</category>
      
      
      <comments>http://xuanyin02.github.io/2023/051139179.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于列式数据库ClickHouse知识的学习</title>
      <link>http://xuanyin02.github.io/2023/051164496.html</link>
      <guid>http://xuanyin02.github.io/2023/051164496.html</guid>
      <pubDate>Thu, 11 May 2023 01:56:02 GMT</pubDate>
      
      <description>介绍了一项非常好用的技术--Clickhouse</description>
      
      
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/clickhouse/">clickhouse</category>
      
      
      <comments>http://xuanyin02.github.io/2023/051164496.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于日志采集系统Flume知识的学习</title>
      <link>http://xuanyin02.github.io/2023/050462077.html</link>
      <guid>http://xuanyin02.github.io/2023/050462077.html</guid>
      <pubDate>Thu, 04 May 2023 01:58:43 GMT</pubDate>
      
        
        
      <description>&lt;h3 id=&quot;第1章-Flume概述&quot;&gt;&lt;a href=&quot;#第1章-Flume概述&quot; class=&quot;headerlink&quot; title=&quot;第1章 Flume概述&quot;&gt;&lt;/a&gt;第1章 Flume概述&lt;/h3&gt;&lt;h4 id=&quot;1-1-Flume定义&quot;&gt;&lt;a href=&quot;#1-1-Fl</description>
        
      
      
      
      <content:encoded><![CDATA[<h3 id="第1章-Flume概述"><a href="#第1章-Flume概述" class="headerlink" title="第1章 Flume概述"></a>第1章 Flume概述</h3><h4 id="1-1-Flume定义"><a href="#1-1-Flume定义" class="headerlink" title="1.1 Flume定义"></a>1.1 Flume定义</h4><p>Flume是一个<strong>高可用，高可靠的，分布式的海量日志采集、聚合和传输的系统</strong><br>Flume基于流式架构，灵活简单<br>Flume最主要的作用就是，<strong>实时读取服务器本地磁盘的数据，将数据写入到HDFS</strong></p><h4 id="1-2-Flume基础架构"><a href="#1-2-Flume基础架构" class="headerlink" title="1.2 Flume基础架构"></a>1.2 Flume基础架构</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230504100830651.png" alt="image-20230504100830651"><figcaption>image-20230504100830651</figcaption></figure></p><h5 id="1-2-1-Agent"><a href="#1-2-1-Agent" class="headerlink" title="1.2.1 Agent"></a>1.2.1 Agent</h5><p>Agent是一个JVM进程，它以事件的形式将数据从源头送至目的地<br>Agent主要有3个部分组成，Source、Channel、Sink</p><h5 id="1-2-2-Source"><a href="#1-2-2-Source" class="headerlink" title="1.2.2 Source"></a>1.2.2 Source</h5><p>Source是负责接收数据到Flume Agent组件。Source组件可以处理各种类型、各种格式的日志数据</p><h5 id="1-2-3-Sink"><a href="#1-2-3-Sink" class="headerlink" title="1.2.3 Sink"></a>1.2.3 Sink</h5><p>Sink不断轮询Channel中的事件且批量移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另外一个Flume Agent</p><h5 id="1-2-4-Channel"><a href="#1-2-4-Channel" class="headerlink" title="1.2.4 Channel"></a>1.2.4 Channel</h5><p>Channel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作<br>Flume自带两种Channel：Memory Channel 和 File Channel<br>Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失<br>File Channel将所有事件写入磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据</p><h5 id="1-2-5-Event"><a href="#1-2-5-Event" class="headerlink" title="1.2.5 Event"></a>1.2.5 Event</h5><p>传输单元，Flume数据传输的基本单元，以Event的形式将数据从源头送至目的地。Event由Header和Body两部分组成，Header用来存放该Event的一些属性，为K-V结构，Body用来存放该条数据，形式为字节数组</p><h3 id="第2章-Flume入门"><a href="#第2章-Flume入门" class="headerlink" title="第2章 Flume入门"></a>第2章 Flume入门</h3><h4 id="2-1-Flume入门案例–监控端口数据"><a href="#2-1-Flume入门案例–监控端口数据" class="headerlink" title="2.1 Flume入门案例–监控端口数据"></a>2.1 Flume入门案例–监控端口数据</h4><p>使用Flume监听一个端口，收集该端口的数据，并打印到控制台</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230504111054631.png" alt="image-20230504111054631"><figcaption>image-20230504111054631</figcaption></figure></p><p>实现步骤：</p><ol><li><p>在 flume 目录下创建 job 文件夹并进入 job 文件夹</p></li><li><p>在 job 文件夹下创建 Flume Agent 配置文件 flume-netcat-logger.conf</p></li><li><p>在 flume-netcat-logger.conf 文件中添加如下内容</p><pre><code># Name the components on this agenta1：表示agent的名称a1.sources = r1r1：表示a1的source的名称a1.sinks = k1k1：表示a1的sink名称a1.channels = c1c1：表示a1的channel名称# Describe/configure the sourcea1.sources.r1.type = netcat表示a1是输入源类型为netcat端口类型a1.sources.r1.bind = localhost表示a1的监听的主机a1.sources.r1.port = 44444表示a1的监听的端口号# Describe the sinka1.sinks.k1.type = logger表示a1的输出目的地是控制台logger类型# Use a channel which buffers events in memorya1.channels.c1.type = memory表示a1的channel类型是memory内存型a1.channels.c1.capacity = 1000表示a1的channel总容量是1000个eventa1.channels.c1.transactionCapacity = 100表示a1的channel传输时收集到了100条event以后再去提交事务# Bind the source and sink to the channela1.sources.r1.channels = c1表示r1和c1连接起来a1.sinks.k1.channel = c1表示k1和c1连接起来</code></pre></li><li><p>开启Flume监听端口<br>第1种写法：bin&#x2F;flume-ng agent –conf conf&#x2F; –name a1 –conf-file job&#x2F;flume-netcat-logger.conf -<br>Dflume.root.logger&#x3D;INFO,console<br>第2种写法：bin&#x2F;flume-ng agent -c conf&#x2F; -n a1 -f job&#x2F;flume-netcat-logger.conf -Dflume.root.logger&#x3D;INFO,console<br>参数说明：</p><ul><li>–conf&#x2F;-c：表示配置文件存储在 conf&#x2F;目录</li><li>–name&#x2F;-n：表示给 agent 起名为 a1</li><li>–conf-file&#x2F;-f：flume 本次启动读取的配置文件是在 job 文件夹下的 flume-telnet.conf<br>文件</li><li>-Dflume.root.logger&#x3D;INFO,console ：-D 表示 flume 运行时动态修改 flume.root.logger<br>参数属性值，并将控制台日志打印级别设置为 INFO 级别。日志级别包括:log、info、warn、<br>error</li></ul></li><li><p>使用 netcat 工具向本机的 44444 端口发送内容</p><pre><code>nc localhost 44444</code></pre></li></ol><h4 id="2-2-Flume入门案例–实时监控单个追加文件"><a href="#2-2-Flume入门案例–实时监控单个追加文件" class="headerlink" title="2.2 Flume入门案例–实时监控单个追加文件"></a>2.2 Flume入门案例–实时监控单个追加文件</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="/./%E5%85%B3%E4%BA%8E%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E7%B3%BB%E7%BB%9FFlume%E7%9F%A5%E8%AF%86%E7%9A%84%E5%AD%A6%E4%B9%A0/image-20230505173625217.png" alt="image-20230505173625217"><figcaption>image-20230505173625217</figcaption></figure></p><p>这里同上一个案例的区别主要是Flume Agent 配置文件不同</p><p>创建配置文件 flume-file-hdfs.conf</p><pre><code># Name the components on this agenta2.sources = r2a2.sinks = k2a2.channels = c2# Describe/configure the sourcea2.sources.r2.type = exec# hive的日志a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log# Describe the sinka2.sinks.k2.type = hdfsa2.sinks.k2.hdfs.path = hdfs://hadoop102:8200/flume/%Y%m%d/%H#上传文件的前缀a2.sinks.k2.hdfs.filePrefix = logs-#是否按照时间滚动文件夹a2.sinks.k2.hdfs.round = true#多少时间单位创建一个新的文件夹a2.sinks.k2.hdfs.roundValue = 1#重新定义时间单位a2.sinks.k2.hdfs.roundUnit = hour#是否使用本地时间戳a2.sinks.k2.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a2.sinks.k2.hdfs.batchSize = 100#设置文件类型，可支持压缩a2.sinks.k2.hdfs.fileType = DataStream#多久生成一个新的文件a2.sinks.k2.hdfs.rollInterval = 60#设置每个文件的滚动大小a2.sinks.k2.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a2.sinks.k2.hdfs.rollCount = 0# Use a channel which buffers events in memorya2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100# Bind the source and sink to the channela2.sources.r2.channels = c2a2.sinks.k2.channel = c2</code></pre><p><strong>注意：对于所有与时间相关的转义序列，Event Header 中必须存在以 “timestamp”的key（除非 hdfs.useLocalTimeStamp 设置为 true，此方法会使用 TimestampInterceptor 自动添加 timestamp）</strong></p><h4 id="2-3-Flume入门案例–实时监控目录下多个新文件"><a href="#2-3-Flume入门案例–实时监控目录下多个新文件" class="headerlink" title="2.3 Flume入门案例–实时监控目录下多个新文件"></a>2.3 Flume入门案例–实时监控目录下多个新文件</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230505175242130.png" alt="image-20230505175242130"><figcaption>image-20230505175242130</figcaption></figure></p><p><strong>说明：在使用Spooling Directory Source时，不要在监控目录中创建目录并持续修改文件；上传完成的文件会以.COMPLETED结尾；被监控的文件每500毫秒扫描一次文件变动</strong></p><p>创建配置文件 flume-dir-hdfs.conf</p><pre><code>a3.sources = r3a3.sinks = k3a3.channels = c3# Describe/configure the sourcea3.sources.r3.type = spooldira3.sources.r3.spoolDir = /opt/module/flume/uploada3.sources.r3.fileSuffix = .COMPLETEDa3.sources.r3.fileHeader = true#忽略所有以.tmp 结尾的文件，不上传a3.sources.r3.ignorePattern = ([^ ]*\.tmp)# Describe the sinka3.sinks.k3.type = hdfsa3.sinks.k3.hdfs.path =hdfs://hadoop102:9820/flume/upload/%Y%m%d/%H#上传文件的前缀a3.sinks.k3.hdfs.filePrefix = upload-#是否按照时间滚动文件夹a3.sinks.k3.hdfs.round = true#多少时间单位创建一个新的文件夹a3.sinks.k3.hdfs.roundValue = 1#重新定义时间单位a3.sinks.k3.hdfs.roundUnit = hour#是否使用本地时间戳a3.sinks.k3.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a3.sinks.k3.hdfs.batchSize = 100#设置文件类型，可支持压缩a3.sinks.k3.hdfs.fileType = DataStream#多久生成一个新的文件a3.sinks.k3.hdfs.rollInterval = 60#设置每个文件的滚动大小大概是 128Ma3.sinks.k3.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a3.sinks.k3.hdfs.rollCount = 0# Use a channel which buffers events in memorya3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r3.channels = c3a3.sinks.k3.channel = c3</code></pre><p>2.4 Flume入门案例–实时监控目录下多个追加文件</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230505175422818.png" alt="image-20230505175422818"><figcaption>image-20230505175422818</figcaption></figure></p><p><strong>Exec Source适用于监控一个实时追加的文件，不能实现断点续传；Spooling Source适用于同步新文件，但不适合对实时追加日志的文件进行监听并同步；而Taildir Source适合用于监听多个实时追加的文件，并且能够实现断点续传</strong></p><p>创建配置文件 flume-taildir-hdfs.conf</p><pre><code>a3.sources = r3a3.sinks = k3a3.channels = c3# Describe/configure the sourcea3.sources.r3.type = TAILDIRa3.sources.r3.positionFile = /opt/module/flume/tail_dir.jsona3.sources.r3.filegroups = f1 f2a3.sources.r3.filegroups.f1 = /opt/module/flume/files/.*file.*a3.sources.r3.filegroups.f2 = /opt/module/flume/files2/.*log.*# Describe the sinka3.sinks.k3.type = hdfsa3.sinks.k3.hdfs.path =hdfs://hadoop102:9820/flume/upload2/%Y%m%d/%H#上传文件的前缀a3.sinks.k3.hdfs.filePrefix = upload-#是否按照时间滚动文件夹a3.sinks.k3.hdfs.round = true#多少时间单位创建一个新的文件夹a3.sinks.k3.hdfs.roundValue = 1#重新定义时间单位a3.sinks.k3.hdfs.roundUnit = hour#是否使用本地时间戳a3.sinks.k3.hdfs.useLocalTimeStamp = true#积攒多少个 Event 才 flush 到 HDFS 一次a3.sinks.k3.hdfs.batchSize = 100#设置文件类型，可支持压缩a3.sinks.k3.hdfs.fileType = DataStream#多久生成一个新的文件a3.sinks.k3.hdfs.rollInterval = 60#设置每个文件的滚动大小大概是 128Ma3.sinks.k3.hdfs.rollSize = 134217700#文件的滚动与 Event 数量无关a3.sinks.k3.hdfs.rollCount = 0# Use a channel which buffers events in memorya3.channels.c3.type = memorya3.channels.c3.capacity = 1000a3.channels.c3.transactionCapacity = 100# Bind the source and sink to the channela3.sources.r3.channels = c3a3.sinks.k3.channel = c3</code></pre><p><strong>Taildir 说明：Taildir Source维护了一个json格式的position file，其会定期的往position file中更新每个文件读取到的最新的位置，因此能够实现断点续传</strong></p>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/flume/">flume</category>
      
      
      <comments>http://xuanyin02.github.io/2023/050462077.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于实时计算框架flink知识的学习</title>
      <link>http://xuanyin02.github.io/2023/042717719.html</link>
      <guid>http://xuanyin02.github.io/2023/042717719.html</guid>
      <pubDate>Thu, 27 Apr 2023 08:27:07 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;这一篇文章我要分享的是flink实时计算框架，我先解释一下flink的基本概念：一个分布式的、有状态的实时流式处理系统（编程框架）。那么就会有人问：之前不是学习过spark streaming吗，也能做实时流式处理，为什么还要学习flink？其实答案很简单：flink比sp</description>
        
      
      
      
      <content:encoded><![CDATA[<p>这一篇文章我要分享的是flink实时计算框架，我先解释一下flink的基本概念：一个分布式的、有状态的实时流式处理系统（编程框架）。那么就会有人问：之前不是学习过spark streaming吗，也能做实时流式处理，为什么还要学习flink？其实答案很简单：flink比spark streaming更<strong>实时</strong>！spark streaming的流式处理是基于微批处理的思想，需要隔一点时间才会去处理，flink采用了基于操作符的连续模型，可以做到微秒级别的延迟</p><p>下面就是我分享的知识：</p><h3 id="1-快速认识flink"><a href="#1-快速认识flink" class="headerlink" title="1 快速认识flink"></a>1 快速认识flink</h3><h4 id="1-1-flink基本概念"><a href="#1-1-flink基本概念" class="headerlink" title="1.1 flink基本概念"></a>1.1 flink基本概念</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230505164747393.png" alt="image-20230505164747393"><figcaption>image-20230505164747393</figcaption></figure></p><p><strong>flink，以流处理方式作为基础的世界观，并通过引入有界流来实现批计算，从而实现流批一体，可以说是非常np</strong></p><h4 id="1-2-flink的运行架构"><a href="#1-2-flink的运行架构" class="headerlink" title="1.2 flink的运行架构"></a>1.2 flink的运行架构</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230505165006278.png" alt="image-20230505165006278"><figcaption>image-20230505165006278</figcaption></figure></p><p>flink集群采用 Master - Slave 架构：</p><ul><li>Master的角色为JobManager，负责集群和作业管理</li><li>Slave的角色是TaskManager，负责执行计算任务</li><li>客户端Client负责集群和提交任务，JobManager 和 TaskManager是集群的进程</li></ul><p>各角色主要职责说明：</p><ol><li>Client：是flink提供的CLI命令行工具，用来提交flink作业到flink集群，在客户端中负责 Stream Graph（流图）和 Job Graph（作业图）的构建</li><li>JobManager：根据并行度将flink客户端提交的flink应用分解为子任务，从资源管理器ResourceManager申请所需的计算资源，资源具备之后，开始分发到TaskManager执行Task，并负责应用容错，跟踪作业的执行状态，发现异常则恢复作业等</li><li>TaskManager：接收JobManager分发的子任务，根据自身的资源情况，管理子任务的启动、停止、销毁、异常恢复等生命周期阶段。flink程序中必须有一个TaskManager</li></ol><h4 id="1-3-flink的特性"><a href="#1-3-flink的特性" class="headerlink" title="1.3 flink的特性"></a>1.3 flink的特性</h4><ol><li><p>适用于几乎所有的流式数据处理管道</p><ul><li>事件驱动型应用</li><li>流、批数据分析</li><li>数据管道及ETL</li></ul></li><li><p>自带状态管理机制</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230505172208248.png" alt="image-20230505172208248"><figcaption>image-20230505172208248</figcaption></figure></p></li><li><p>强大的准确性保证</p><ul><li>exactly-once 状态一致性</li><li>事件时间处理</li><li>专业的迟到数据处理</li></ul></li><li><p>灵活丰富的多层api</p><ul><li>流、批数据之上的SQL查询</li><li>流、批数据之上的TableApi</li><li>datastream流处理算子api、dataset批处理算子api</li><li>精细可控的processFunction</li></ul></li><li><p>规模弹性扩展</p><ul><li>可扩展的分布式架构（集群级别的资源规模灵活配置，算子粒度的独立并行度灵活配置）</li><li>支持超大状态管理</li><li>增量checkpoint机制</li></ul></li><li><p>强大的运维能力</p><ul><li>弹性实施部署机制</li><li>高可用机制</li><li>保存点恢复机制</li></ul></li><li><p>优秀的性能</p><ul><li>低延迟</li><li>高吞吐</li><li>内存计算</li></ul></li></ol><h3 id="2-flink编程基础"><a href="#2-flink编程基础" class="headerlink" title="2 flink编程基础"></a>2 flink编程基础</h3><h4 id="2-1-flink的DataStream抽象"><a href="#2-1-flink的DataStream抽象" class="headerlink" title="2.1 flink的DataStream抽象"></a>2.1 flink的DataStream抽象</h4><ul><li>DataStream 代表一个数据流，<strong>它可以是无界的，也可以是有界的</strong></li><li>DataStream 类似于Spark的rdd，它是不可变的</li><li>无法对一个DataStream进行自由的添加或删除或修改元素</li><li>只能通过算子对DataStream中的数据进行转换，将一个DataStream转成另一个DataStream</li><li>DataStream可以通过source算子加载、映射外部数据而来；或者从已存在的DataStream转换而来</li></ul><h4 id="2-2-flink编程模板"><a href="#2-2-flink编程模板" class="headerlink" title="2.2 flink编程模板"></a>2.2 flink编程模板</h4><ol><li>获取一个编程、执行入口环境env</li><li>通过数据源组件、加载、创建DataStream</li><li>对DataStream调用各种处理算子表达计算逻辑</li><li>通过sink算子指定计算结果的输出方式</li><li>在env上触发程序提交运行</li></ol><h4 id="2-3-flink程序的并行概念"><a href="#2-3-flink程序的并行概念" class="headerlink" title="2.3 flink程序的并行概念"></a>2.3 flink程序的并行概念</h4><ul><li><p>flink程序中，每一个算子都可以成为一个独立任务（task）</p></li><li><p>flink程序中，视上下游算子间数据分发规划、并行度、共享槽位设置，可组成算子链成为一个task</p></li><li><p>每个任务在运行时都可拥有多个并行是运行实例（subtask）</p></li><li><p>且每个算子任务的并行度都可以在代码中显式设置</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230514180529758.png" alt="image-20230514180529758"><figcaption>image-20230514180529758</figcaption></figure></p></li></ul><h4 id="2-4-flink编程入口"><a href="#2-4-flink编程入口" class="headerlink" title="2.4 flink编程入口"></a>2.4 flink编程入口</h4><p>批处理入口</p><pre class=" language-java"><code class="language-java">ExecutionEnvironment env <span class="token operator">=</span> ExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnviroment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>流处理入口</p><pre class=" language-java"><code class="language-java">StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnviroment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>流批一体处理入口</p><pre class=" language-java"><code class="language-java">StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnviroment</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//为env设置环境参数</span>ExecutionConfig config <span class="token operator">=</span> env<span class="token punctuation">.</span><span class="token function">getConfig</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//设置为批处理模式</span>config<span class="token punctuation">.</span><span class="token function">setExecutionMode</span><span class="token punctuation">(</span>ExecutionMode<span class="token punctuation">.</span>BATCH<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>开启webui的本地运行环境处理入口</p><pre class=" language-java"><code class="language-java">Configuration conf <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Configuration</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>conf<span class="token punctuation">.</span><span class="token function">setInteger</span><span class="token punctuation">(</span><span class="token string">"rest.port"</span><span class="token punctuation">,</span><span class="token number">8081</span><span class="token punctuation">)</span><span class="token punctuation">;</span>StreamExecutionEnvironment env <span class="token operator">=</span> StreamExecutionEnvironment<span class="token punctuation">.</span><span class="token function">getExecutionEnviroment</span><span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><blockquote><p>要开启上述webui功能，需添加依赖：flink-runtime-web_2.12</p></blockquote><h4 id="2-5-基本source算子"><a href="#2-5-基本source算子" class="headerlink" title="2.5 基本source算子"></a>2.5 基本source算子</h4><p>source是用来获取外部数据的算子，按照获取数据的方式，可以分为：</p><ul><li>基于集合的Source</li><li>基于Socket网络端口的Source</li><li>基于文件的Source</li><li>第三方Connector Source</li><li>自定义Source</li></ul><p>在此我仅介绍 第三方Connector Source 和 自定义Source</p><p>在实际生产环境中，为了保证flink可以高效读取数据源中的数据，通常是跟一些分布式消息中间件结合使用，例如Kafka。Flink和Kafka整合可以高效的读取数据，并且可以保证Exactly Once（精确一次性语义）</p><h5 id="第三方Connector-Source"><a href="#第三方Connector-Source" class="headerlink" title="第三方Connector Source"></a>第三方Connector Source</h5><p>以 Kafka Source为例：首先引入依赖：flink-connector-kafka_2.12</p><blockquote><p>代码示例：</p></blockquote><pre class=" language-java"><code class="language-java">KafkaSource<span class="token operator">&lt;</span>String<span class="token operator">></span> kafkaSource <span class="token operator">=</span> KafkaSource<span class="token punctuation">.</span>&lt;String<span class="token operator">></span><span class="token function">builder</span><span class="token punctuation">(</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">setTopics</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置主题</span>                <span class="token punctuation">.</span><span class="token function">setGroupId</span><span class="token punctuation">(</span><span class="token string">"01"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置消费组ID</span>                <span class="token punctuation">.</span><span class="token function">setBootstrapServers</span><span class="token punctuation">(</span><span class="token string">"node1:9092"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置kafka连接地址</span>                <span class="token comment" spellcheck="true">//OffsetsInitializer.committedOffsets(OffsetResetStrategy.LATEST)   消费起始位移选择之前提交的偏移量（如果没有，则重置为LATEST）</span>                <span class="token comment" spellcheck="true">//OffsetsInitializer.earliest() 消费起始位移直接选择为“最早”</span>                <span class="token comment" spellcheck="true">//OffsetsInitializer.latest() 消费起始位移直接选择为“最新”</span>                <span class="token comment" spellcheck="true">//OffsetsInitializer.offsets(Map&lt; TopicPartition,Long>) 消费起始位移为：方法所传入的每个分区和对应的起始偏移量</span>                <span class="token punctuation">.</span><span class="token function">setStartingOffsets</span><span class="token punctuation">(</span>OffsetsInitializer<span class="token punctuation">.</span><span class="token function">committedOffsets</span><span class="token punctuation">(</span>OffsetResetStrategy<span class="token punctuation">.</span>LATEST<span class="token punctuation">)</span><span class="token punctuation">)</span>                <span class="token punctuation">.</span><span class="token function">setValueOnlyDeserializer</span><span class="token punctuation">(</span><span class="token keyword">new</span> <span class="token class-name">SimpleStringSchema</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">//设置反序列化Schema，SimpleStringSchema指的是读取kafka中的数据反序列化成String格式</span>                <span class="token comment" spellcheck="true">//开启了Kafka底层消费者的自动位移提交机制，它会把最新的消费位移提交到Kafka的consumer_offsets中</span>                <span class="token comment" spellcheck="true">//但是就算把自动位移提交机制开启，KafkaSource依然不依赖自动提交机制（宕机重启时，优先从flink算子自己的状态去获取偏移量&lt;更可靠>）</span>                <span class="token punctuation">.</span><span class="token function">setProperty</span><span class="token punctuation">(</span><span class="token string">"auto.offset.commit"</span><span class="token punctuation">,</span> <span class="token string">"true"</span><span class="token punctuation">)</span>                <span class="token comment" spellcheck="true">//把本source算子设置成 BOUNDED（有界流），将来本source去读取数据的时候，读到指定的位置，就停止读取并退出</span>                <span class="token comment" spellcheck="true">//常用来补数或者重跑一段历史数据</span>                <span class="token comment" spellcheck="true">//.setBounded(OffsetsInitializer.committedOffsets());</span>                <span class="token comment" spellcheck="true">//把本source算子设置成 UNBOUNDED（无界流），但是并不会一直读取数据，而是达到指定位置就停止读取，但程序不退出</span>                <span class="token comment" spellcheck="true">//主要应用场景：   需要从Kafka中读取某一段固定长度的数据，然后拿着这段数据去跟真正的无界流联合处理</span>                <span class="token comment" spellcheck="true">//.setUnbounded(OffsetsInitializer.latest())</span>                <span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>flink会把kafka消费者的消费位移记录在算子状态中，这样就实现了消费位移状态的容错，从而可以支持端到端的exactly-once</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230514185841763.png" alt="image-20230514185841763"><figcaption>image-20230514185841763</figcaption></figure></p><h5 id="自定义Source"><a href="#自定义Source" class="headerlink" title="自定义Source"></a>自定义Source</h5><p>本质上就是定义一个类，实现SourceFunction或继承RichParallelSourceFunction，实现run方法和cancel方法</p><blockquote><p>代码示例：</p></blockquote><pre class=" language-java"><code class="language-java"><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">MyParallelSource</span> <span class="token keyword">extends</span> <span class="token class-name">RichParallelSourceFunction</span><span class="token operator">&lt;</span>String<span class="token operator">></span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token keyword">private</span> <span class="token keyword">int</span> i <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//定义一个 int 类型的变量，从 1 开始</span>    <span class="token keyword">private</span> <span class="token keyword">boolean</span> flag <span class="token operator">=</span> <span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//定义一个 flag 标标志</span>    <span class="token comment" spellcheck="true">//run 方法就是用来读取外部的数据或产生数据的逻辑</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">run</span><span class="token punctuation">(</span>SourceContext<span class="token operator">&lt;</span>String<span class="token operator">></span> ctx<span class="token punctuation">)</span> <span class="token keyword">throws</span> Exception <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//满足 while 循环的条件，就将数据通过 SourceContext 收集起来</span>    <span class="token keyword">while</span> <span class="token punctuation">(</span>i <span class="token operator">&lt;=</span> <span class="token number">10</span> <span class="token operator">&amp;&amp;</span> flag<span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        Thread<span class="token punctuation">.</span><span class="token function">sleep</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//为避免太快，睡眠 1 秒</span>        ctx<span class="token punctuation">.</span><span class="token function">collect</span><span class="token punctuation">(</span><span class="token string">"data："</span> <span class="token operator">+</span> i<span class="token operator">++</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//将数据通过 SourceContext 收集起来</span>        <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">//cancel 方法就是让 Source 停止</span>    <span class="token annotation punctuation">@Override</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">cancel</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&amp;</span>#<span class="token number">123</span><span class="token punctuation">;</span>        <span class="token comment" spellcheck="true">//将 flag 设置成 false，即停止 Source</span>        flag <span class="token operator">=</span> <span class="token boolean">false</span><span class="token punctuation">;</span>    <span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span><span class="token operator">&amp;</span>#<span class="token number">125</span><span class="token punctuation">;</span></code></pre>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/flink/">flink</category>
      
      
      <comments>http://xuanyin02.github.io/2023/042717719.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>关于数据仓库hive知识的学习(1)</title>
      <link>http://xuanyin02.github.io/2023/042713923.html</link>
      <guid>http://xuanyin02.github.io/2023/042713923.html</guid>
      <pubDate>Thu, 27 Apr 2023 08:24:33 GMT</pubDate>
      
        
        
      <description>&lt;p&gt;hive在离线数据仓库中十分常用，那么hive是什么呢？有什么用？它是怎么工作的？怎么使用它？下面这篇文章将一一解答你的问题！&lt;/p&gt;
&lt;h3 id=&quot;第1章-hive入门&quot;&gt;&lt;a href=&quot;#第1章-hive入门&quot; class=&quot;headerlink&quot; title=&quot;第</description>
        
      
      
      
      <content:encoded><![CDATA[<p>hive在离线数据仓库中十分常用，那么hive是什么呢？有什么用？它是怎么工作的？怎么使用它？下面这篇文章将一一解答你的问题！</p><h3 id="第1章-hive入门"><a href="#第1章-hive入门" class="headerlink" title="第1章 hive入门"></a>第1章 hive入门</h3><h4 id="1-1-什么是hive"><a href="#1-1-什么是hive" class="headerlink" title="1.1 什么是hive"></a>1.1 什么是hive</h4><p>hive是基于hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能（HQL）。</p><p>1)hive处理的数据存储在HDFS<br>2)hive分析数据底层的实现是Mapreduce&#x2F;Spark（分布式运行框架）<br>3)执行程序运行在yarn上</p><h4 id="1-2-hive的优缺点"><a href="#1-2-hive的优缺点" class="headerlink" title="1.2 hive的优缺点"></a>1.2 hive的优缺点</h4><h5 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h5><ol><li>操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</li><li>避免了去写Mapreduce，减少开发人员的学习成本</li><li>hive的执行<strong>延迟比较高</strong>，因此hive常用于数据分析，对实时性要求不高的场合（历史数据分析等）</li><li>hive优势在于处理大数据，对于处理小数据没有优势，因为hive的执行延迟比较高</li><li>hive支持用户<strong>自定义函数</strong>，用户可以根据自己的需求来实现自己的函数</li></ol><h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ol><li><p>hive的HQL表达能力有限</p><p>（1）迭代式算法无法表达</p><p>（2）数据挖掘方面不擅长</p></li><li><p>hive的效率比较低</p><p>（1）hive自动生成的Mapreduce作业，通常情况下不够智能化</p><p>（2）hive调优比较困难，粒度较粗（快）</p></li></ol><h4 id="1-3-hive的应用场景"><a href="#1-3-hive的应用场景" class="headerlink" title="1.3 hive的应用场景"></a>1.3 hive的应用场景</h4><p>①hive不适合需要低延迟的应用<br>②hive不是为联机事务处理而设计的，hive不提供实时的查询和基于行级的数据更新操作<br>③hive的最佳使用场合是大数据集的批处理作业，如网络日志分析</p><h4 id="1-4-hive架构及工作原理"><a href="#1-4-hive架构及工作原理" class="headerlink" title="1.4 hive架构及工作原理"></a>1.4 hive架构及工作原理</h4><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/hive%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86.png" alt="image-20230427164622923"><figcaption>image-20230427164622923</figcaption></figure></p><ol><li><p>用户接口：Client</p><p>CLI（hive shell）、JDBC（java访问hive）、WEB UI（浏览器访问hive）</p></li><li><p>元数据：Metastore</p><p>元数据包括：表名、列名、分区、表属性等信息称为hive元数据</p><p>默认存储在内嵌的derby数据库中，推荐使用MySQL存储Metastore。为什么呢？<strong>因为derby只能允许一个会话连接，而MySQL支持多用户会话</strong></p><p>那为什么不存储在hdfs中呢？<strong>因为hive元数据可能面临不断更新、修改和读取，不适合使用hdfs进行存储</strong></p></li><li><p>Hadoop</p><p>使用hdfs进行存储，使用Mapreduce进行计算</p></li><li><p>驱动器：Driver</p><ul><li>解析器(SQLParser): 将HQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行 语法 分析，比如表是否存在、字段是否存在、SQL语义是否有误</li><li>编译器(Compiler)： 对hql语句进行词法、语法、语义的编译(需要跟元数据关联)，编译完成后会生成一个执行计划。 hive上就是编译成mapreduce的job</li><li>优化器(Optimizer)： 将执行计划进行优化，减少不必要的列、使用分区、使用索引等。优化job</li><li>执行器(Executer): 将优化后的执行计划提交给hadoop的yarn上执行。提交job</li></ul></li></ol><h5 id="hive工作原理"><a href="#hive工作原理" class="headerlink" title="hive工作原理"></a>hive工作原理</h5><p>①由hive驱动模块中的编译器对用户输入的HQL语言进行词法和语法解析，将HQL语句转换成抽象语法树AST<br>②由于抽象语法树的结构仍很复杂，因此，把抽象语法树转化为查询块<br>③把查询块转换为逻辑查询计划，里面包含了许多逻辑操作符<br>④重写逻辑查询计划，进行优化，合并多余操作，减少Mapreduce任务数量<br>⑤将逻辑操作符转换为需要执行的具体的Mapreduce任务<br>⑥对生成的Mapreduce任务进行优化，生成最终的Mapreduce任务执行计划<br>⑦由hive驱动模块中的执行器对最终的Mapreduce任务进行执行输出</p><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230427170924636.png" alt="image-20230427170924636"><figcaption>image-20230427170924636</figcaption></figure></p><h4 id="1-5-hive和传统数据库比较"><a href="#1-5-hive和传统数据库比较" class="headerlink" title="1.5 hive和传统数据库比较"></a>1.5 hive和传统数据库比较</h4><ul><li>数据存储格式不同：hive没有专门的数据存储格式，只需要在创建表时指定数据的列分隔符和行分隔符，hive就可以解析数据；传统数据库的数据存储格式由系统预先定义</li><li>数据验证不同：hive在数据加载过程中不进行数据验证，而是在数据查询时才进行验证；传统数据库在数据加载时进行验证，因此hive加载数据比传统数据库快</li><li>DML操作不同：hive不支持数据更新操作，支持批量数据导入；传统数据库支持各种DML操作，支持数据更新、单条或批量数据导入</li><li>延迟性不同：hive操作延迟性高，不适合低延迟操作；传统数据库延迟性低，适合低延迟操作</li><li>数据规模不同：hive存储在hdfs中，利用Mapreduce进行并行计算，适合大规模数据操作；传统数据库主要采用本地文件系统存储数据，存在容量上限，在本地运行，数据处理能力有限</li></ul><h3 id="第2章-hive安装部署"><a href="#第2章-hive安装部署" class="headerlink" title="第2章 hive安装部署"></a>第2章 hive安装部署</h3><p>这里附上一篇链接，我就不进行赘述了</p><p>[参考文章]: <a class="link" href="https://blog.csdn.net/W_chuanqi/article/details/130242723">https://blog.csdn.net/W_chuanqi/article/details/130242723 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>“HIve安装配置（超详细）”</p><h3 id="第3章-hive数据类型"><a href="#第3章-hive数据类型" class="headerlink" title="第3章 hive数据类型"></a>第3章 hive数据类型</h3><h4 id="3-1-数据类型概述"><a href="#3-1-数据类型概述" class="headerlink" title="3.1 数据类型概述"></a>3.1 数据类型概述</h4><h5 id="3-1-1-基本数据类型"><a href="#3-1-1-基本数据类型" class="headerlink" title="3.1.1 基本数据类型"></a>3.1.1 基本数据类型</h5><p>TINYINT、SMALLINT、INT、BIGINT、BOOLEAN、FLOAT、DOUBLE、STRING、TIMESTAM-时间类型、BINARY-字节数组</p><h5 id="3-1-2-集合数据类型"><a href="#3-1-2-集合数据类型" class="headerlink" title="3.1.2 集合数据类型"></a>3.1.2 集合数据类型</h5><ul><li><p>STRUCT</p><p>和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING}，那么第1个元素可以通过字段’ .first ‘来引用</p></li><li><p>MAP</p><p>MAP是一组键-值对元组集合，使用数组标识法可以访问数据。例如，如果某个列的数据类型是MAP，其中键-&gt;值对是’ first ‘ -&gt;’ John ‘ 和 ‘ last ‘-&gt;’ Doe ‘，那么可以通过字段名[‘ last ‘]获取最后一个元素</p></li><li><p>ARRAY</p><p>数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘ John ‘,’ Doe ‘]，那么第2个元素可以通过数组名[1]进行引用</p></li></ul><h4 id="3-2-数据类型详解"><a href="#3-2-数据类型详解" class="headerlink" title="3.2 数据类型详解"></a>3.2 数据类型详解</h4><h5 id="3-2-1-数字类型"><a href="#3-2-1-数字类型" class="headerlink" title="3.2.1 数字类型"></a>3.2.1 数字类型</h5><p>TINYINT(1字节整数)<br>SMALLINT (2字节整数)<br>INT&#x2F;INTEGER (4字节整数)<br>BIGINT (8字节整数)<br>FLOAT (4字节浮点数)<br>DOUBLE (8字节双精度浮点数)</p><p>示例：<br><strong>create table t_test(a string ,b int,c bigint,d float,e double,f tinyint,g smallint)</strong></p><h5 id="3-2-2-时间类型"><a href="#3-2-2-时间类型" class="headerlink" title="3.2.2 时间类型"></a>3.2.2 时间类型</h5><p>TIMESTAMP（时间戳）–包含年月日时分毫秒的一种封装<br>DATE（日期）–只包含年月日<br></p><p>示例，假如有以下数据：<br>1,zhangsan,1985-06-30<br>2,lisi,1986-07-10<br>3,wangwu,1985-08-09<br>那么，就可以建立一个表来对数据进行映射<br><strong>create table t_customer(id int, name string, birthday date)<br>row format delimited fields terminated by ‘,’;</strong><br>然后导入数据<br><strong>load data local inpath ‘&#x2F;root&#x2F;customer.dat’ into table t_customer;</strong></p><h5 id="3-2-3-字符串类型"><a href="#3-2-3-字符串类型" class="headerlink" title="3.2.3 字符串类型"></a>3.2.3 字符串类型</h5><p>STRING<br>VARCHAR(字符串1-65355长度，超长截断)<br>CHAR (字符串，最大长度255)</p><h5 id="3-2-4-其他类型"><a href="#3-2-4-其他类型" class="headerlink" title="3.2.4 其他类型"></a>3.2.4 其他类型</h5><p>BOOLEAN（布尔类型）：true false<br>BINARY (二进制数组)</p><h5 id="3-2-5-集合类型"><a href="#3-2-5-集合类型" class="headerlink" title="3.2.5 集合类型"></a>3.2.5 集合类型</h5><h6 id="3-2-5-1-STRUCT举例"><a href="#3-2-5-1-STRUCT举例" class="headerlink" title="3.2.5.1 STRUCT举例"></a>3.2.5.1 STRUCT举例</h6><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230427182958022.png" alt="image-20230427182958022"><figcaption>image-20230427182958022</figcaption></figure></p><h6 id="3-2-5-2-MAP举例"><a href="#3-2-5-2-MAP举例" class="headerlink" title="3.2.5.2 MAP举例"></a>3.2.5.2 MAP举例</h6><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230427182914489.png" alt="image-20230427182914489"><figcaption>image-20230427182914489</figcaption></figure></p><h6 id="3-2-5-3-ARRAY举例"><a href="#3-2-5-3-ARRAY举例" class="headerlink" title="3.2.5.3 ARRAY举例"></a>3.2.5.3 ARRAY举例</h6><p><figure class="image-caption"><img lazyload src="/images/loading.svg" data-src="https://cdn.jsdelivr.net/gh/xuanyin02/ImgHosting/article_img/image-20230427183049077.png" alt="image-20230427183049077"><figcaption>image-20230427183049077</figcaption></figure></p><h4 id="3-3-类型转换"><a href="#3-3-类型转换" class="headerlink" title="3.3 类型转换"></a>3.3 类型转换</h4><h5 id="3-3-1-隐式转换"><a href="#3-3-1-隐式转换" class="headerlink" title="3.3.1 隐式转换"></a>3.3.1 隐式转换</h5><p>粗粒度–&gt;细粒度</p><h5 id="3-3-2-使用CAST操作"><a href="#3-3-2-使用CAST操作" class="headerlink" title="3.3.2 使用CAST操作"></a>3.3.2 使用CAST操作</h5><p>例如 CAST(‘1’ AS INT)会把字符串’1’转换成整数1；如果强制类型转换失败，如执行 CAST(‘X’ AS INT)，表达式返回空值NULL</p><h3 id="第4章-DDL语言"><a href="#第4章-DDL语言" class="headerlink" title="第4章 DDL语言"></a>第4章 DDL语言</h3><h4 id="4-1-创建数据库"><a href="#4-1-创建数据库" class="headerlink" title="4.1 创建数据库"></a>4.1 创建数据库</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">database</span> <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token keyword">exists</span> db_hive location <span class="token keyword">on</span> <span class="token string">'在hdfs上的位置'</span><span class="token punctuation">;</span></code></pre><h4 id="4-2-查询数据库"><a href="#4-2-查询数据库" class="headerlink" title="4.2 查询数据库"></a>4.2 查询数据库</h4><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">//显示数据库</span><span class="token keyword">show</span> <span class="token keyword">databases</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//查询通配符匹配的数据库</span><span class="token keyword">show</span> <span class="token keyword">databases</span> <span class="token operator">like</span> <span class="token string">'db_hive*'</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//显示数据库详细信息</span><span class="token keyword">desc</span> <span class="token keyword">database</span> <span class="token punctuation">[</span>extend<span class="token punctuation">]</span> db_hive<span class="token punctuation">;</span></code></pre><h4 id="4-3-修改数据库"><a href="#4-3-修改数据库" class="headerlink" title="4.3 修改数据库"></a>4.3 修改数据库</h4><p>用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息，不可以修改数据库元数据信息</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">alter</span> <span class="token keyword">database</span> db_hive <span class="token keyword">set</span> dbproperties<span class="token punctuation">(</span><span class="token string">'createtime'</span><span class="token operator">=</span><span class="token string">'20170830'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h4 id="4-4-删除数据库"><a href="#4-4-删除数据库" class="headerlink" title="4.4 删除数据库"></a>4.4 删除数据库</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">drop</span> <span class="token keyword">databases</span> <span class="token keyword">if</span> <span class="token keyword">exists</span> db_hive<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">//如果数据库不为空，可以采用cascade命令，级联删除</span><span class="token keyword">drop</span> <span class="token keyword">database</span> db_hive <span class="token keyword">cascade</span><span class="token punctuation">;</span></code></pre><h4 id="4-5-创建表"><a href="#4-5-创建表" class="headerlink" title="4.5 创建表"></a>4.5 创建表</h4><pre class=" language-sql"><code class="language-sql"><span class="token keyword">CREATE</span> <span class="token punctuation">[</span>EXTERNAL<span class="token punctuation">]</span> <span class="token keyword">TABLE</span> <span class="token punctuation">[</span><span class="token keyword">IF</span> <span class="token operator">NOT</span> <span class="token keyword">EXISTS</span><span class="token punctuation">]</span> table_name <span class="token comment" spellcheck="true">//①EXTERNAL关键字可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</span><span class="token punctuation">[</span><span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> table_comment<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//COMMENT：为表和列添加注释。</span><span class="token punctuation">[</span>PARTITIONED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name data_type <span class="token punctuation">[</span><span class="token keyword">COMMENT</span> col_comment<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">//PARTITIONED BY创建分区表</span><span class="token punctuation">[</span><span class="token keyword">CLUSTERED</span> <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name<span class="token punctuation">,</span> col_name<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">//CLUSTERED BY创建分桶表</span><span class="token punctuation">[</span>SORTED <span class="token keyword">BY</span> <span class="token punctuation">(</span>col_name <span class="token punctuation">[</span><span class="token keyword">ASC</span><span class="token operator">|</span><span class="token keyword">DESC</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token keyword">INTO</span> num_buckets BUCKETS<span class="token punctuation">]</span> <span class="token comment" spellcheck="true">//SORTED BY不常用</span><span class="token punctuation">[</span><span class="token keyword">ROW</span> FORMAT row_format<span class="token punctuation">]</span>   <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> “分隔符”<span class="token punctuation">[</span>STORED <span class="token keyword">AS</span> file_format<span class="token punctuation">]</span><span class="token comment" spellcheck="true">//STORED AS指定存储文件类型。常用的存储文件类型：SEQUENCEFILE（hadoop_kv序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）、PARQUETFILE(列式存储文件）。如果文件数据是纯文本，可以使用STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。</span><span class="token punctuation">[</span>LOCATION hdfs_path<span class="token punctuation">]</span><span class="token comment" spellcheck="true">//LOCATION ：指定表在HDFS上的存储位置。</span><span class="token punctuation">[</span><span class="token operator">LIKE</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">//LIKE允许用户复制现有的表结构，但是不复制数据.</span></code></pre><h5 id="4-5-1-内部表（也称“管理表”）"><a href="#4-5-1-内部表（也称“管理表”）" class="headerlink" title="4.5.1 内部表（也称“管理表”）"></a>4.5.1 内部表（也称“管理表”）</h5><p>在上面代码块中的第①个解释中已说明</p><h5 id="4-5-2-外部表"><a href="#4-5-2-外部表" class="headerlink" title="4.5.2 外部表"></a>4.5.2 外部表</h5><p>同上</p><h5 id="4-5-3-内部表与外部表的互相转换"><a href="#4-5-3-内部表与外部表的互相转换" class="headerlink" title="4.5.3 内部表与外部表的互相转换"></a>4.5.3 内部表与外部表的互相转换</h5><pre class=" language-sql"><code class="language-sql"><span class="token comment" spellcheck="true">//查询student1、student2表的类型</span><span class="token keyword">desc</span> formatted student1<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--内部表</span><span class="token keyword">desc</span> formatted student2<span class="token punctuation">;</span><span class="token comment" spellcheck="true">--外部表</span><span class="token comment" spellcheck="true">//修改内部表student1为外部表</span><span class="token keyword">alter</span> <span class="token keyword">table</span> student <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'TRUE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//修改外部表student2为内部表</span><span class="token keyword">alter</span> <span class="token keyword">table</span> student2 <span class="token keyword">set</span> tblproperties<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'FALSE'</span><span class="token punctuation">)</span><span class="token punctuation">;</span>注意：<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'TRUE'</span><span class="token punctuation">)</span>和<span class="token punctuation">(</span><span class="token string">'EXTERNAL'</span><span class="token operator">=</span><span class="token string">'FALSE'</span><span class="token punctuation">)</span>为固定写法，区分大小写！</code></pre><h4 id="4-6-分区表"><a href="#4-6-分区表" class="headerlink" title="4.6 分区表"></a>4.6 分区表</h4><p>hive中的分区就是分目录</p><h5 id="4-6-1-静态分区"><a href="#4-6-1-静态分区" class="headerlink" title="4.6.1 静态分区"></a>4.6.1 静态分区</h5><p>创建静态分区表语法</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> 表名 <span class="token keyword">partition</span><span class="token punctuation">(</span>字段名 字段类型<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> partitioned <span class="token keyword">by</span><span class="token punctuation">(</span>分区字段名 分区字段类型<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>加载数据到分区表中</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">load</span> <span class="token keyword">data</span> <span class="token keyword">local</span> inpath <span class="token string">'本地文件路径'</span> <span class="token keyword">into</span> <span class="token keyword">table</span> 数据库名<span class="token punctuation">.</span>表名 <span class="token keyword">partition</span><span class="token punctuation">(</span>分区字段名<span class="token operator">=</span><span class="token string">'分区字段值'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><p>增加分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">alter</span> <span class="token keyword">table</span> 表名 <span class="token keyword">add</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>分区字段名<span class="token operator">=</span><span class="token string">'分区字段值1'</span><span class="token punctuation">)</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>分区字段名<span class="token operator">=</span><span class="token string">'分区字段值2'</span><span class="token punctuation">)</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span></code></pre><p>删除分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">alter</span> <span class="token keyword">table</span> 表名 <span class="token keyword">drop</span> <span class="token keyword">partition</span><span class="token punctuation">(</span>分区字段名<span class="token operator">=</span><span class="token string">'分区字段值1'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token keyword">partition</span><span class="token punctuation">(</span>分区字段名<span class="token operator">=</span><span class="token string">'分区字段值2'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">;</span></code></pre><p>查看分区表分区</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">show</span> partitions 表名<span class="token punctuation">;</span></code></pre><p>多级分区：partitioned by 多个分区字段</p><p>把数据导入分区的三种方式：<br>(1)方式1：上传数据后修复<br>(2)方式2：上传数据后添加分区<br>(3)方式3：创建文件夹后load数据到分区</p><h5 id="4-6-2-动态分区"><a href="#4-6-2-动态分区" class="headerlink" title="4.6.2 动态分区"></a>4.6.2 动态分区</h5><p>按照某个字段的不同值自动将数据加载到不同分区中</p><p>创建动态分区表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">database</span> 表名<span class="token punctuation">(</span>字段名 字段类型<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> partitioned <span class="token keyword">by</span><span class="token punctuation">(</span>分区字段名 分区字段类型<span class="token punctuation">)</span><span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span><span class="token punctuation">;</span></code></pre><p>设置开启动态分区参数</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token operator">=</span><span class="token boolean">true</span> <span class="token comment" spellcheck="true">//使用动态分区</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span><span class="token keyword">partition</span><span class="token punctuation">.</span>mode<span class="token operator">=</span>nonstrick<span class="token punctuation">;</span><span class="token comment" spellcheck="true">//无限制模式，如果模式是strict，则必须有一个静态分区且放在最前面</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token punctuation">.</span>pernode<span class="token operator">=</span><span class="token number">10000</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//每个节点生成动态分区的最大个数</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>dynamic<span class="token punctuation">.</span>partitions<span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//生成动态分区的最大个数</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">exec</span><span class="token punctuation">.</span>max<span class="token punctuation">.</span>created<span class="token punctuation">.</span>files<span class="token operator">=</span><span class="token number">150000</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//一个任务最多可以创建的文件数目</span><span class="token keyword">set</span> dfs<span class="token punctuation">.</span>datanode<span class="token punctuation">.</span>max<span class="token punctuation">.</span>xcievers<span class="token operator">=</span><span class="token number">8192</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">//限定一次最多打开的文件数</span><span class="token keyword">set</span> hive<span class="token punctuation">.</span><span class="token keyword">merge</span><span class="token punctuation">.</span>mapfiles<span class="token operator">=</span><span class="token boolean">true</span><span class="token punctuation">;</span> <span class="token comment" spellcheck="true">// map端的结果进行合并</span><span class="token keyword">set</span> mapreduce<span class="token punctuation">.</span>reduce<span class="token punctuation">.</span>tasks <span class="token operator">=</span><span class="token number">20000</span><span class="token punctuation">;</span>  <span class="token comment" spellcheck="true">//设置reduce task个数  增加reduce阶段的并行度</span></code></pre><p>加载数据</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span>  <span class="token keyword">table</span> demo2 <span class="token keyword">partition</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">--此处的分区变量x应该跟demo2中的分区变量名一致</span><span class="token keyword">select</span> id<span class="token punctuation">,</span> cost<span class="token punctuation">,</span> birthday <span class="token keyword">from</span> demo<span class="token punctuation">;</span> <span class="token comment" spellcheck="true">--select中的最后一个表达式birthday，会作为分区变量x的动态值，注意顺序</span></code></pre><p>4.8 SerDe组件</p><p>SerDe是Serialize&#x2F;Deserilize的简称，目的是用于序列化和反序列化。<br>用户在建表时可以用自定义的SerDe或使用Hive自带的SerDe，SerDe能为表切分、解析列，且对列指定相应的数据。</p><p>创建表</p><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> 表名<span class="token punctuation">(</span>字段名 字段类型<span class="token punctuation">,</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token keyword">row</span> format serde <span class="token string">'org.apache.hadoop.hive.serde2.RegexSerDe'</span> <span class="token keyword">with</span> serdeproperties <span class="token punctuation">(</span><span class="token string">"input.regex"</span> <span class="token operator">=</span> <span class="token string">"id=(.*),name=(.*)"</span><span class="token punctuation">)</span> stored <span class="token keyword">as</span> textfile<span class="token punctuation">;</span></code></pre><h3 id="第5章-数据导入导出操作"><a href="#第5章-数据导入导出操作" class="headerlink" title="第5章 数据导入导出操作"></a>第5章 数据导入导出操作</h3><h4 id="5-1-数据导入"><a href="#5-1-数据导入" class="headerlink" title="5.1 数据导入"></a>5.1 数据导入</h4><h5 id="5-1-1-使用load向表中装载数据"><a href="#5-1-1-使用load向表中装载数据" class="headerlink" title="5.1.1 使用load向表中装载数据"></a>5.1.1 使用load向表中装载数据</h5><pre class=" language-SQL"><code class="language-SQL">load data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];</code></pre><ol><li>load data：表示加载数据</li><li>local：表示从本地加载数据到hive表；否则从HDFS加载数据到hive表</li><li>inpath：表示加载数据的路径</li><li>overwrite：表示覆盖表中已有数据，否则表示追加</li><li>into table：表示加载到哪张表</li><li>student：表示具体的表</li><li>partition：表示上传到指定分区</li></ol><h5 id="5-1-2-使用insert…values向表中插入数据"><a href="#5-1-2-使用insert…values向表中插入数据" class="headerlink" title="5.1.2 使用insert…values向表中插入数据"></a>5.1.2 使用insert…values向表中插入数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span>  student1 <span class="token keyword">partition</span><span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201709'</span><span class="token punctuation">)</span> <span class="token keyword">values</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token string">'wangwu'</span><span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h5 id="5-1-3-使用insert…select向表中插入数据"><a href="#5-1-3-使用insert…select向表中插入数据" class="headerlink" title="5.1.3 使用insert…select向表中插入数据"></a>5.1.3 使用insert…select向表中插入数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token keyword">table</span> student2 <span class="token keyword">partition</span><span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201708'</span><span class="token punctuation">)</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span> name <span class="token keyword">from</span> student1 <span class="token keyword">where</span> month<span class="token operator">=</span><span class="token string">'201709'</span><span class="token punctuation">;</span></code></pre><h5 id="5-1-4-用create-as创建表并加载数据"><a href="#5-1-4-用create-as创建表并加载数据" class="headerlink" title="5.1.4 用create as创建表并加载数据"></a>5.1.4 用create as创建表并加载数据</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token keyword">exists</span> student3<span class="token keyword">as</span> <span class="token keyword">select</span> id<span class="token punctuation">,</span> name <span class="token keyword">from</span> student<span class="token punctuation">;</span></code></pre><h5 id="5-1-5-创建表时通过指定location加载数据路径"><a href="#5-1-5-创建表时通过指定location加载数据路径" class="headerlink" title="5.1.5 创建表时通过指定location加载数据路径"></a>5.1.5 创建表时通过指定location加载数据路径</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> <span class="token keyword">if</span> <span class="token operator">not</span> <span class="token keyword">exists</span> student4<span class="token punctuation">(</span>id <span class="token keyword">int</span><span class="token punctuation">,</span> name string<span class="token punctuation">)</span> <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated by</span> <span class="token string">'\t'</span> location <span class="token string">'/user/hive/warehouse/student4'</span><span class="token punctuation">;</span></code></pre><h5 id="5-1-6-import数据到指定Hive表中"><a href="#5-1-6-import数据到指定Hive表中" class="headerlink" title="5.1.6 import数据到指定Hive表中"></a>5.1.6 import数据到指定Hive表中</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">import</span> <span class="token keyword">table</span> student2 <span class="token keyword">partition</span><span class="token punctuation">(</span>month<span class="token operator">=</span><span class="token string">'201709'</span><span class="token punctuation">)</span> <span class="token keyword">from</span> <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span></code></pre><blockquote><p>注意：先用export导出后，再将数据导入</p></blockquote><h4 id="5-2-数据导出"><a href="#5-2-数据导出" class="headerlink" title="5.2 数据导出"></a>5.2 数据导出</h4><h5 id="5-2-1-insert导出"><a href="#5-2-1-insert导出" class="headerlink" title="5.2.1 insert导出"></a>5.2.1 insert导出</h5><pre class=" language-sql"><code class="language-sql"><span class="token keyword">insert</span> overwrite <span class="token punctuation">[</span><span class="token keyword">local</span><span class="token punctuation">]</span> directory <span class="token string">'/opt/module/datas/export/student1'</span><span class="token keyword">ROW</span> FORMAT DELIMITED <span class="token keyword">FIELDS</span> <span class="token keyword">TERMINATED BY</span> <span class="token string">'\t'</span>             <span class="token keyword">select</span> <span class="token operator">*</span> <span class="token keyword">from</span> student<span class="token punctuation">;</span></code></pre><h5 id="5-2-2-export导出到HDFS上"><a href="#5-2-2-export导出到HDFS上" class="headerlink" title="5.2.2 export导出到HDFS上"></a>5.2.2 export导出到HDFS上</h5><pre class=" language-sql"><code class="language-sql">export <span class="token keyword">table</span> <span class="token keyword">default</span><span class="token punctuation">.</span>student <span class="token keyword">to</span> <span class="token string">'/user/hive/warehouse/export/student'</span><span class="token punctuation">;</span></code></pre><h4 id="5-3-清除表中数据（Truncate）"><a href="#5-3-清除表中数据（Truncate）" class="headerlink" title="5.3 清除表中数据（Truncate）"></a>5.3 清除表中数据（Truncate）</h4><pre class=" language-sql"><code class="language-sql"> <span class="token keyword">truncate</span> <span class="token keyword">table</span> student<span class="token punctuation">;</span></code></pre><blockquote><p>注意：Truncate只能删除管理表（内部表），不能删除外部表中数据</p></blockquote>]]></content:encoded>
      
      
      <category domain="http://xuanyin02.github.io/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</category>
      
      
      <category domain="http://xuanyin02.github.io/tags/hive/">hive</category>
      
      
      <comments>http://xuanyin02.github.io/2023/042713923.html#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
